@article{1000,
 abstract = {Due to license restrictions and installation issues, it is often not feasible to experiment with software without making substantial investments. Especially in the case of legacy tools, it turns out that even free software is often too costly (i.e., time-consuming) to be installed for evaluating the quality of a research contribution. After organizing a series of events related to software modeling, we have constructed (and started to use) SHARE, a system for sharing practically any type of software artifact to reviewers and to other participants who have very limited time available. The system relies on cloud-computing technologies to provide online access to interactive environments containing all the tools, documentation, input and output models to reproduce alleged research results. The system also enables one to clone such an environment and add additional models or tools in order to extend a contribution or pinpoint a problem. In retrospect, we observe that the approach is not limited to software modeling and SHARE is in fact gaining acceptance in other fields already.},
 duplicado = {false},
 inserir = {false},
 title = {Supporting the internet-based evaluation of research software with cloud infrastructure},
 year = {2012}
}

@article{1001,
 abstract = {Abstract:
Testing web services impose many challenges to existing testing methods, techniques, and tools; especially those available to traditional applications. Composed web services increase these challenges by requiring additional validation and verification efforts. Structural-based testing approaches have been thoroughly researched for traditional applications; however, they have not yet been examined, as a methodology, for testing composed web services. In this work, we introduce a formal model for an abstract-based workflow framework that can be used to capture a composed web service under test. We then define a set of applicable structural-based testing criteria to the framework. Finally we outline a promising line of testing criteria that can be applied to this framework.},
 duplicado = {false},
 inserir = {false},
 title = {An Abstract Workflow-Based Framework for Testing Composed Web Services},
 year = {2007}
}

@article{1003,
 abstract = {Software process establishment, evaluation and improvement are key research areas in the software engineering field today. Extensive research has been carried out and many different kinds of approaches exist to improve the software process and even more efforts are underway. Verification & validation process, which is part of the broader software process activities, plays a vital role in quality and profitability of the developed product but is believed to consume major portion of the development expenses and resources. Probably, research towards improving the verification & validation process has not been as actively directed as compared to software process improvement research. This paper identifies several potential future research directions towards improving verification and validation process.},
 duplicado = {false},
 inserir = {false},
 title = {Research directions in verification & validation process improvement},
 year = {2007}
}

@article{1007,
 abstract = {- Parallel to the considerable growth in applications of web-based systems, there are increasing demands for methods and tools to assure their quality. Testing these systems, due to their inherent complexities and special characteristics, is complex, time-consuming and challenging. In this paper a novel multi-agent framework for automated testing of web-based systems is presented. The main design goals have been to develop an effective and flexible framework that supports different types of tests and utilize different sources of information about the system under test to automate the test process. A prototype of the proposed framework has been implemented and is used to perform some experiments. The results are promising and prove the overall design of the framework.},
 duplicado = {false},
 inserir = {false},
 title = {An Agent-Based Framework for Automated Testing of Web-Based Systems},
 year = {2011}
}

@article{1008,
 abstract = {Abstract- Testing Web applications is still a
challenging work which can greatly benefit from
test automation techniques. In this paper, we focus
on using ontologies as a means of test automation.
Current works that use ontologies for software
testing are discussed. Further a theoretical
roadmap is presented, with some examples, on
ontology-based web application testing.},
 duplicado = {false},
 inserir = {true},
 title = {Ontology-Based Web Application Testing},
 year = {2010}
}

@article{1009,
 abstract = {Runtime testing cost caused by service invocations is considered as one of the major limitations in Service-centric System Testing (ScST). Unfortunately, most of the existing work cannot achieve cost reduction at runtime as they perform offline testing. In this paper, we introduce a novel cost-aware pareto optimal test suite minimisation approach for ScST aimed at reducing runtime testing cost. In experimental analysis, the proposed approach achieved reductions between 69% and 98.6% in monetary cost of service invocations while retaining test suite coverage. The results also provided evidence for the effectiveness of the selected algorithm HNSGA-II over the two commonly used algorithms: Greedy and NSGA-II.},
 duplicado = {false},
 inserir = {false},
 title = {Cost-aware pareto optimal test suite minimisation for service-centric systems},
 year = {2013}
}

@article{1010,
 abstract = {Cloud Computing makes it possible for users to access a wide range of web services in the public domain
and to embed these global services in their local applications. This promises to save a significant
amount of individual development cost. The biggest obstacle to using this technology is the problem of
trust. To gain trust in the services offered they have to be extensively tested, either by the user himself
or by a trusted agent. This chapter deals with the testing of web services in the cloud. There are many
similarities to testing web services in a local service-oriented architecture, but there are also significant
differences. In a company specific SOA, testers can gain access to the source. This is not true of the
cloud. There is no possibility of accessing the source. Therefore, testers must rely solely on the specification
contained in the service level agreement SLA and the web service interface definition WSDL
or REST to base their test upon. Testing in the cloud is strictly a black-box test. The goal of a cloud
service test is also not to find errors but to assess the suitability of the service to the purpose of the user.
It may be necessary to test several services in order to find that one best suited to the requirements of the
user. To judge suitability it is necessary to define an ideal usage profile, including performance, security
and other non-functional criteria, and to compare that with the actual profile of each potential service.
For this both static and dynamic analysis methods must be applied. The chapter presents an automated
approach to assessing cloud services and selecting that one most suitable to the user s application.},
 duplicado = {false},
 inserir = {false},
 title = {Testing Web Services in the Cloud},
 year = {2013}
}

@article{1012,
 abstract = {Abstract:
Web development frameworks (WDFs) that incorporate both Workflow paradigms and the Model View Controller (MVC) are known as WMVC-WFDs. Such frameworks guide the developer in generating the flow of control between the various components. Despite the pressing need for a visual environment to help developers manage the overwhelming number workflows during development, we have not come across any work that addresses this legitimate concern. In this work we describe, in the context of Ruby on Rail (ROR), an abstract model for the visual generation and management of workflows in WMVC-based web application.},
 duplicado = {false},
 inserir = {false},
 title = {Visual management of workflows in web development frameworks},
 year = {2011}
}

@article{1015,
 abstract = {Abstract:
Client centers have considerable influence to the success of a business. Companies encourage people to visit their client centers, in the hope of engaging potential clients. However, this could give rise to large financial costs. With the recent advances in the technology of computer graphics and virtual reality, this paper presents an approach to construct 3D virtual client centers. To visit a virtual client center, the client no longer needs to travel from place to place, but just takes a virtual tour while staying before an internet-enabled computer. The contributions of this paper are as following: 1) we present an approach to construct a 3D virtual client center in a relatively convenient way, 2) we model the virtual client center as a collection of services, to further facilitate the development work.},
 duplicado = {false},
 inserir = {false},
 title = {3D Virtual Client Center and its Service Oriented Modeling},
 year = {2011}
}

@article{1016,
 abstract = {Abstract- Now days, making use of web based applications becomes crucial for the success of businesses worldwide. But as they are open and
built on Internet, this kind of applications is imposing the new challenges to the developers and researchers such as such as dynamic behaviors,
heterogeneous representations, novel control flow and data flow mechanisms, etc. In the previous studies, the agent based approach provided for
web application testing in order to reduce the complexity of such applications. A four-level data flow test approach can be employed to perform
structure testing on them. In this approach, data flow analysis will be performed as Function Level Testing, Function Cluster Level Testing, Object
Level Testing, and Web Application Level Testing, from low abstract level to high abstract level. But that approach was limited because only the
basic features of such framew ork are implemented.
Therefore, in this research thesis we are further extending that framew ork w ith more specific features implement like specific test agents for each
particular type of Web document or object. Moreover, integrating more testing approaches, such as navigation testing, object state testing,
statistical testing, etc., is still necessary for a systematic testing approach for Web applications},
 duplicado = {false},
 inserir = {false},
 title = {Efficient Agent Based Testing Framework for Web Applications},
 year = {2012}
}

@article{1017,
 abstract = {Abstract:
Open API platform is a trend for many leading social networks and e-business internet enterprises to publish services. The third-party developers can build their own applications interacting with Open platform via the Open API. The APIs have rules on the acceptable value of parameters. However, these rules are often not documented formally or explicitly. Developers may not realize these rules until error is exposed in the runtime. The application will experience robustness problem due to these unhandled errors. Developers using Open API can only handle these errors in a trial-and-error manner when API invocation returning error messages. In this paper, we present an approach to generate Open API usage rules from the error description. It can generate useful API usage rules related to the parameters if the Open API platforms have detailed error descriptions in their documentations. These rules help developers to be aware of the potential error-leading API usage in the development stage of the Open API based applications.},
 duplicado = {false},
 inserir = {false},
 title = {Generating Open API Usage Rule from Error Descriptions},
 year = {2013}
}

@article{1020,
 abstract = {ABSTRACT
Testing the correctness of service integration is a step toward assurance of the quality of applications.
These applications however may bind dynamically to their supportive services using the SOA pattern that
share the same service interface, yet the latter services may behave differently. In addition, a service may
implement a business strategy, such as best pricing, relative to the behaviors of its competitors and the
dynamic market conditions. As such, defining a test oracle that specifies the absolute expected outcomes
for individual test cases is hard. Many existing works ignore these issues to address the problem of
identifying failures from test results. This chapter studies an approach to online testing. Service testing is
divided into two steps. In the spirit of metamorphic testing, the offline step determines a set of successful
test cases to construct their corresponding follow-up test cases for the online step. These test cases will be
executed by metamorphic services that encapsulate the services as well as applicable metamorphic
relations. Thus, any failure revealed by the approach will be a failure of the service under test. },
 duplicado = {false},
 inserir = {false},
 title = {A Metamorphic Testing Methodology for Online SOA Application Testing},
 year = {2010}
}

@article{1021,
 abstract = {Even though semantic service provisioning is still a hot research topic, many projects and collaborations already gained a tremendous amount of experiences. This chapter sketches the prototype experiences from one of these projects, the European integration project ASG, which was also the starting point for this book. The chapter further discusses possible next steps for advanced service provisioning platforms.},
 duplicado = {false},
 inserir = {false},
 title = {Application and Outlook},
 year = {2016}
}

@article{1022,
 abstract = {Abstract. The utilisation of web services in building e-commerce applications
and business-to-business solutions is becoming increasingly
widespread. The nature of these applications dictates a strong need for
robust and fault-tolerant infrastructures. To this end, the establishment
of sound approaches to testing web services is essential. Existing literature
on the subject typically focuses on theoretical classifications of possible
tests, with limited consideration being given to practical issues. In
this paper we describe the approach taken to testing web services within
the (iterative) development of a large-scale service-based infrastructure.
The system in question is being developed to support a variety of distributed
applications pertaining to healthcare delivery and research. We
also report upon our experiences, which have given rise to a collection of
generic principles for the testing of systems built on web services.},
 duplicado = {false},
 inserir = {false},
 title = {Practical Experiences of Testing Web Services},
 year = {2007}
}

@article{1024,
 abstract = {Recent years have seen a rapid growth in the development of web services technology.
BPEL (Business Process Execution Language) as a de-facto standard for web service
orchestration has drawn particularly attention from researchers and industries. BPEL is
a semi-formal flow language with complex features, so it is essential to apply automated
validation tools in finding the interaction inconsistencies of BPEL processes. In addition
to validating the model properties by verification, it is desirable to test the correctness
with respect to the functional requirements. To test a model thoroughly, we need to
cover different execution scenarios. As is well known, it is tedious, time-consuming, and
error prone to design test cases manually, especially for complex modelling languages.
Hence, it is desirable to apply existing model-based-testing techniques in the domain
of web services.
This thesis proposes a web service automaton as the operational semantics for
BPEL, and presents an automatic test framework to verify and test BPEL processes.
From the testing point of view, we show the suitability of using web service automaton
formalism for BPEL by modelling various BPEL features. Based on the web service
automata, we provide a model checking based test framework to verify the general
properties and generate test cases for BPEL processes. The framework supports both
control-flow and data-flow testing of BPEL. Two levels of test cases can be generated to
check the behavioural and interface conformance for web services. To our knowledge,
none of the prior research studies the verificatioll and testing for BPEL control and
data flows in a unified approach.
The formal work in this thesis underpins the development of an automated test case
generation and execution tool that has been integrated into the DBE Studio that was
developed under the EU funded Digital Business Ecosystems Integrated Programme. },
 duplicado = {false},
 inserir = {false},
 title = {An Automatic Test Framework for BPEL-based Web Services},
 year = {2007}
}

@article{1025,
 abstract = {Testing in Web services and SOA environment can be far more distributed in comparison with testing stand-alone or traditional applications. This is because such systems are composed of several hybrid components. These include Web servers and their related components, server side applications, communication services, and client side Web services. In this chapter, the authors focus on challenges and opportunities for software testing in SOA environment. They divide testing activities based on three classifications: testing activities that are going to be similar to those in traditional software development environments, testing activities that will be less usable or popular in SOA, and testing activities that will evolve significantly to adapt to the new environment. The authors believe that most generic testing activities are going to stay in any new software development environment. However, their importance, significance, challenges, and difficulties are going to be dependent on the subject environment. Some tasks will be easier to implement and test and others will either be un-applicable or difficult to test and implement in comparison with testing in traditional software development environments.},
 duplicado = {false},
 inserir = {false},
 title = {The Distribution of Testing Activities in Web Services and SOA Environment},
 year = {2013}
}

@article{1026,
 abstract = {Abstract:
In this paper, we present a service-oriented architecture (SOA) based system to meet the on-demand VMI requirements. SOA is used to create flexible and efficient loosely-coupled systems . We utilize technologies including web services, SOAP, HTTP, XML and WSDL to develop a VMI system. Web services can support the integration of information and services across platforms and operating systems. In addition, we implement web service coordination (WS-Coordination) to enable coordinated transactions across platforms. WS-Coordination is a specification for providing protocols that coordinate the actions of distributed applications. Also, we present workflow of VMI operations from two aspects: seller's market and buyer's market. Finally, we use the business process execution language (BPEL) for web services to describe business processes of VMI. Service components are deployed flexibly in web environment using the Apache Axis2 web services engine.},
 duplicado = {false},
 inserir = {false},
 title = {A Service-Oriented Architecture Based Vendor Managed Inventory System},
 year = {2008}
}

@article{1027,
 abstract = {Software testing is a difficult task for web based applications due to their special features like multi-tier structure and emergence of new technologies (e.g. Ajax). In recent years, automatic testing of web based applications has been emerged as a promising technique to tackle the difficulties of testing these types of applications and several frameworks have been proposed for this purpose. But the most important problem of these frameworks is the lack of generality for different types of tests and programming environments. In this paper, we proposed a general framework for automatic testing of web based applications, that covers all aspects of different types of testing in an arbitrary web based application.},
 duplicado = {false},
 inserir = {false},
 title = {A General Framework for Testing Web-Based Applications},
 year = {2010}
}

@article{1028,
 abstract = {An experiment with a group of teachers from different levels of education (Early Childhood; Primary;
Secondary and University Education) is described. Participants took part in a cooperative learning
process on a digital platform in a project organized by the Madre Coraje NGO to share experiences
in the design and testing of Education for Development (ED) teaching proposals. The participants'
levels of professional development in this virtual learning environment ranged widely; some were
voluntary collaborators with the NGO with some previous experience in ED, whilst others were
recruited by the teacher trainers who coordinated the project. The format was based on a non face-toface
asynchronous online network with instructors setting three-phase assignments: the design of an
assignment-based classroom activity, its testing and its evaluation. Online work was complemented
with classroom work through a number of face-to-face meetings in the form of seminars. The unique
process and the participants' heterogeneity have provided some interesting findings as to how they
interacted and shared their experiences},
 duplicado = {false},
 inserir = {false},
 title = {MADRE CORAJE AND EDUCATION FOR DEVELOPMENT (ED): A COLLABORATIVE EXPERIENCE IN A VIRTUAL LEARNING ENVIRONMENT},
 year = {2014}
}

@article{1030,
 abstract = {Software testing is done to check the quality of the product or service under test. Test techniques include the process of executing a
program or application and finding software bugs (errors or other defects). Software testers face great challenge in testing Web Services
(WS) especially when integrating to services owned by other vendors. They must deal with the diversity of implementation techniques used
by the other services. However, testers are in lack of software artifacts, the means of control over test executions and observation on the
internal behavior of the other services. An automated testing technique must be developed to be capable of testing nonintrusively and no
disruptively. Addressing these problems, this paper proposes a collaborative testing tool called COWST in which test tasks are completed
through the collaboration of various test services that are registered, discovered, and invoked at runtime using the ontology of software
testing. The ontology can be extended and updated so that it can support a wide open range of test activities, methods, techniques, and
types of software artifacts. Experimental evaluation of the framework has also demonstrated using a Travel Agency Service.},
 duplicado = {false},
 inserir = {true},
 title = {COWST: Collaborative Web Service Tester},
 year = {2014}
}

@article{1031,
 abstract = {Oracle and Sun Microsystems are adopting this new approach to developing software and
systems. SOA advocates run-time system integration of loosely coupled services across
heterogeneous platforms in a distributed environment and also improves the flexibility of
system development. However, trustworthiness becomes a serious problem and
appropriate tradeoffs have to be made.
This research introduces a verification framework for SOA. An effective verification
framework will greatly reduce the effort for rapid and adaptive service composition and
evaluation of applications based on SOA. The proposed framework consists of two
aspects: a testing infrastructure for static verification of services, and a policy-based
dynamic verification mechanism for service collaboration and composition. The proposed
verification framework provides the following advantages:
It offers a CV&V mechanism to verify services.
It provides the function of integration testing and functional (black-box) testing.
It provides the capability of test case profiling, test case ranking, service ranking,
static service profiling, and dynamic service profiling.
It has the capability of dynamic verification over services.
WebStrar (Web Services Testing, Ranking, and test case Ranking) is the component in
the verification framework to perform static verification. It assures the trustworthiness
and reduces the vulnerability of WS by rigorous positive and negative testing, reliability
assessing, and ranking. A policy specification language PSML-P (Process Specification
and Modeling Language for Policy) and a policy enforcement framework "Pi4SOA"
(Policy Infrastructure for Service-Oriented Architecture) are proposed to verify and
control the collaboration process of SOA during service runtime. },
 duplicado = {false},
 inserir = {false},
 title = {A verification framework for Service-Oriented Architecture applications},
 year = {2008}
}

@article{1033,
 abstract = {Objectives: To study on software testing pertaining to the trends, techniques, problems and
challengers from 2006 to 2009 using systematic literature reviews (SLRs) approach.
Method: We used the standard systematic literature review method employing a manual online
search of 41 journals, Conference, Article and Books.
Conclusions: Currently, the topic areas covered are trends, techniques, problems and challengers
on software testing from 2006 to 2009. },
 duplicado = {false},
 inserir = {false},
 title = {Systematic literature reviews pertaining to the trends, techniques, problems, and challenges in software testing (2006-2009) A systematic literature review},
 year = {2015}
}

@article{1034,
 abstract = {The majority of current researches on SOA (Service Oriented Architecture) are focusing on the dynamic services composition design and development techniques, leaving testing task with inefficient outcome and results. Many papers have presented novel testing approaches for supporting testability of SOA systems and Component Based Distributed systems; in which these systems are experiencing many testing challenges under real environments. Many new specifications and standards emerged aiming to define the implementation, trustworthiness, and the Quality of Services (QoS). Furthermore, these standards aid SOA testing in real-time environments. In this paper, we will review prior research works derived from the literature, which aid us to identifying the issues in the testability of SOA systems in real-time. Grounded in this principle, we will highlight the standards, protocols that may apply and aid SOA testing. We will propose an approach to which will make SOA testing on real-time, much relevant and applicable},
 duplicado = {false},
 inserir = {false},
 title = {A Framework for the Testability of Service Oriented Architecture},
 year = {2012}
}

@article{1043,
 abstract = {Web service business process execution language (WS-BPEL) is a most popular programming language which can be used to describe the logic to control and orchestrate multiple Web services participating in a business process. It allows a company to quickly create or adapt new business processes by coordinating different Web services. As other traditional software, the WS-BPEL process needs a lot of testing to ensure its correctness. However, the WS-BPEL processes are complex and difficult to understand. This makes testing Web service compositions a challenging task. Nevertheless, many existing tools for testing WS-BPEL processes are solely based on the functional testing which may not be adequate to ensure the structural correctness. In this thesis, we propose a structural testing approach for Web Service Compositions based on the WS-BPEL. In our approach, we construct two test models based on the defferent aspect of the WS-BPEL process. Using the graphical concept to represent the WS-BPEL process structure and the execution logic based on the aspect of the architecture and the flow respectively which can help user to easily understand the process architecture and the control flow, and then user can edit test case to ensure the correctness of the WS-BPEL process. We also develop a WS-BPEL structural testing tool (WSTT) base on our approach. This tool can construct a WS-BPEL control flow graph and edit test cases for testing the structure of the WS-BPEL process.},
 duplicado = {false},
 inserir = {false},
 title = {A Structural Testing Approach and Tool for Web Service Compositions Based on the WS-BPEL},
 year = {2008}
}

@article{1045,
 abstract = {Objectives: Business to Consumer (B2C) E-Commerce activities are developed with a large number through agent-based systems. Case Based Reasoning (CBR) has been applied in these systems by analyzing the consumer buying behavior to provide consumers, a support to the decision making process. Analysis: Current applications of CBR to E-Commerce are limited to fixed, unchangeable products. To make the environment support for configurable products, an interactive operator based customization approach from CBR can be applied. Findings: In this work, to make the process more reliable and efficient, real time data from provisional stores has been taken and the system is trained to predict the consumer buying behavior along with CBR to pave way for a consumer to make a better decision making process. Applications/ Improvements: This work also applies big data concepts in predicting the behavior of the consumers. It thereby also led the customers to mine about their preferences in purchasing necessary products.},
 duplicado = {false},
 inserir = {false},
 title = {Decision Making Process for B2C Model Using Behavior Analysis with Big Data Technologies},
 year = {2016}
}

@article{1049,
 abstract = {Abstract:
The semantic Web is the second generation of the Web, which helps sharing and reusing data across application, enterprise, and community boundaries. Ontology defines a set of representational primitives with which a domain of knowledge is modeled. The main purpose of the semantic Web and ontology is to integrate heterogeneous data and enable interoperability among disparate systems. Ontology has been used to model software engineering knowledge by denoting the artifacts that are designed or produced during the engineering process. The semantic Web allows publishing reusable software engineering knowledge resources and providing services for searching and querying. This paper classifies the ontologies developed for software engineering, reviews the current efforts on applying the semantic Web techniques on different software engineering aspects, and presents the benefits of their applications. We also foresee the possible future research directions.},
 duplicado = {false},
 inserir = {false},
 title = {Ontology Classification for Semantic-Web-Based Software Engineering},
 year = {2009}
}

@article{1060,
 abstract = {Abstract:
One major agreed upon factor responsible for popularity of software systems, is graphical user interface. Besides the efforts and desires of development organizations, testing a graphical user interface thoroughly, is still almost a nightmare. Manual effort required to complete this task is very large. One major breakthrough to automate this manual effort of GUI testing is to map GUI events with some models and graphs. Event-flow graph is relatively a fresh and useful addition to cope up with automation of GUI testing. In this paper we are presenting an idea of using ontology for GUI testing. This ontology is supposed to work on the basis of semantics of possible events and then annotations will be used to generate the test cases and work as an oracle for verification of the output of testing effort. This work still is based on theoretical concepts and needs practical verification, which will be completed in short time.},
 duplicado = {false},
 inserir = {true},
 title = {Ontology driven semantic annotation based GUI testing},
 year = {2010}
}

@article{1061,
 abstract = {Knowledge can be captured and made available to both
machines and humans by an ontology. Ontology can be served
as a structured knowledge representation scheme, capable of
assisting the construction of a personalized learning path. This
paper describes the processes of conceptualization and
specification, or building of, an ontology. The domain for
which the ontology has been constructed is software risk
identification. The required concepts, the semantic description
of the concepts and the interrelationship among the concepts
along with all other ontological components have been
collected from various literatures and experience of the people
from software industry. From which, a taxonomy has been
constructed by using the property isA? and the design
architecture for the required ontology has also been sketched
out manually with nearly four different types of properties. In
order to reduce implementation efforts, the Protege platform, a
scalable and integrated framework for ontological engineering,
has been used to construct the ontology. The constructed
ontology has been represented in owl format, which makes it
more machine understandable. Then the semantic
representation of the knowledge has been made using the OWL
document generator, which automatically generates a set of
documents from the ontology. In order to understand the
knowledge in more detailed way again the ontology has been
visualized using ontoviz tool},
 duplicado = {false},
 inserir = {false},
 title = {Design and Development of SRIONTO: An Educational Ontology Representing Software Risk Identification Knowledge},
 year = {2011}
}

@article{1062,
 abstract = {Abstract:
This paper presents detail analysis of generic and specific Software Engineering ontologies. The analysis results can be used as a road map for the optimization of existing or developing new ontologies. For this purpose, state of the art ontologies have been selected and analyzed in terms of their domain covered, scalability, use of Software Engineering standards, scope and application. Salient features and limitations of these ontologies are presented in this paper which can be considered by ontology engineers and research community for enhancing the knowledge in the field of Software Engineering and specifically Knowledge-based Software developing processes. Based on the presented analysis, we have developed a generic Software Engineering ontology and tried to address such issues which are previously neglected.},
 duplicado = {false},
 inserir = {false},
 title = {Towards optimization of Software Engineering ontologies},
 year = {2014}
}

@article{1064,
 abstract = {The use of ontologies and taxonomies contributes by providing means to define concepts, minimize the ambiguity, improve the interoperability and manage knowledge of the security domain. Thus, this paper presents a literature survey on ontologies and taxonomies concerning the Security Assessment domain. We carried out it to uncover initiatives that aim at formalizing concepts from the Information Security and Test and Assessment fields of research. We applied a systematic review approach in seven scientific databases. 138 papers were identified and divided into categories according to their main contributions, namely: Ontology, Taxonomy and Survey. Based on their contents, we selected 47 papers on ontologies, 22 papers on taxonomies, and 11 papers on surveys. A taxonomy has been devised to be used in the evaluation of the papers. Summaries, tables, and a preliminary analysis of the selected works are presented. Our main contributions are: 1) an updated literature review, describing key characteristics, results, research issues, and application domains of the papers; and 2) the taxonomy for the evaluation process. We have also detected gaps in the Security Assessment literature that could be the subject of further studies in the field. This work is meant to be useful for security researchers who wish to adopt a formal approach in their methods and techniques.},
 duplicado = {false},
 inserir = {false},
 title = {The Security Assessment Domain: A Survey of Taxonomies and Ontologies},
 year = {2017}
}

@article{1065,
 abstract = {An economic view on software quality is essential for company success. An economic view is also needed for the assessment whether software is ready for release. To give an economical software release recommendation, we must trade off the consequential costs against the removal costs. Simply causing release recommendations on failure-based metrics is not sufficient. We must also regard the test quality if the software release depends on failure statistics.

In this work, we survey existing release recommendation approaches. We conclude that existing approaches do not sufficiently regard costs or test quality. Thus, none of the approaches can give an economical release recommendation. We present a release recommendation framework. It focuses on trading off the failure consequential costs against the failure removal costs for each failure at the end of the test process. The test quality is explicitly regarded as a fundamental aspect to ensure a valid release recommendation. We show the applicability of our framework in a hypothetical case study comparing traditional approaches with our framework.},
 duplicado = {false},
 inserir = {false},
 title = {Towards economical software release recommendations},
 year = {2010}
}

@article{1067,
 abstract = {A literature survey on ontologies concerning the Security Assessment domain has been carried out to uncover initiatives that aim at formalizing concepts from the Security Assessment field of research. A preliminary analysis and a discussion on the selected works are presented. Our main contribution is an updated literature review, describing key characteristics, results, research issues, and application domains of the papers. We have also detected gaps in the Security Assessment literature that could be the subject of further studies in the field. This work is meant to be useful for security researchers who wish to adopt a formal approach in their methods.},
 duplicado = {false},
 inserir = {false},
 title = {A Survey of Security Assessment Ontologies},
 year = {2017}
}

@article{1070,
 abstract = {Abstract

Reusable and evolvable Software Engineering Environments (SEEs) are essential to software production and have increasingly become a need. In another perspective, software architectures and reference architectures have played a significant role in determining the success of software systems. In this paper we present a reference architecture for SEEs, named RefASSET, which is based on concepts coming from the aspect-oriented approach. This architecture is specialized to the software testing domain and the development of tools for that domain is discussed. This and other case studies have pointed out that the use of aspects in RefASSET provides a better Separation of Concerns, resulting in reusable and evolvable SEEs.},
 duplicado = {false},
 inserir = {false},
 title = {An aspect-oriented reference architecture for Software Engineering Environments},
 year = {2011}
}

@article{1071,
 abstract = {Content modeling plays a fundamental role in the development process of educational
modules.In spite of its relevance, there are few approaches for modeling
educational content. Motivated by this scenario, in a previous work we proposed
IMA-CID (Integrated Modeling Approach Conceptual, Instructional, Didactic)
an integrated approach for modeling educational content.In this work we discuss
the evolution of IMA-CID by exploring the use of ontologies at its conceptual level.
The goal is to provide a better comprehension of the knowledge domain to be taught
as well as to ease the knowledge sharing and reuse among authors. We illustrate
our ideas by using an ontology of software testing for developing an educational
module on this domain. The development of a supporting tool to help on the importation
of ontologies and on the automated edition, interpretation and execution of
the IMA-CID models is also discussed.},
 duplicado = {false},
 inserir = {false},
 title = {Using Ontologies for Modeling Educational Content},
 year = {2009}
}

@article{1072,
 abstract = {Running multiples experiments in Software Engineering
introduces the need of recording data as well as transferring
knowledge across them, specially considering that several
researchers are involved on replicating experiments. In this
work we explore ontologies to support knowledge transfer,
helping to elucidate the associated concepts of controlled
experiments and their relationships. Based on our expertise
on conducting controlled experiments, we have proposed an
ontology to experimental studies, named EXPEROntology.
The ontology proposed is intended to be used as a tool
for knowledge transfer, assisting researchers, reviewers,
and meta-analysts in designing, conducting, and evaluating
controlled experiments. In order to validate our ontology
we have instantiated it in to a controlled experiment.},
 duplicado = {false},
 inserir = {false},
 title = {An Ontology for Controlled Experiments on Software Engineering},
 year = {2008}
}

@article{1074,
 abstract = {Content modeling plays a fundamental role in the development of educational modules, helping the author to determine the main concepts to be taught and providing a systematic way to structure the relevant parts of the knowledge domain. Despite its relevance, there are few approaches for modeling educational content. In this perspective, an integrated approach for content modeling, named IMA--CID, was proposed. IMA--CID is composed of a set of models, each one considering specific aspects of the development of educational content; however, applying it without an automated support can be an error-prone activity. Motivated by this scenario, in this paper we describe IMATool -- a supporting tool for content modeling, particularly designed for helping the open and distributed construction of the EMAIL@IMA--CID models. Mechanisms for content generation are also available. We illustrate our ideas by applying IMA--CID and IMATool in the development of an educational module for the software testing domain. The preliminary results obtained provide evidences on the practical use of such mechanisms for modeling and generating content.},
 duplicado = {false},
 inserir = {false},
 title = {Towards the establishment of supporting mechanisms for modeling and generating educational content},
 year = {2011}
}

@article{1075,
 abstract = {Abstract : We present hereafter the experimental work of building an ontology of the European Rail Traffic Management System (ERTMS) domain. ERTMS is a railway complex control system defined on the basis of publicly available specification documents, the System Requirement Specification (SRS). We will describe the methodology that we used to define an initial structure for an ERTMS ontology. The main goal of this work is to supply a first formalization of the ERTMS knowledge in order to provide the basis of a later development process i.e validating the specifications, developing the software/hardware components and finally validating the system.},
 duplicado = {false},
 inserir = {false},
 title = {Ontology for complex railway systems application to ERTMS/ETCS system},
 year = {2013}
}

@article{1076,
 abstract = {Abstract. The development of educational modules concise units of study,
composed by theoretical and practical content which can be delivered to learners
by using technological resources has been extensively explored in the
context of Teaching and Learning. In this paper we present and discuss supporting
mechanisms for developing educational modules, emphasizing aspects
of modeling and generating content. We present AIM?CID an integrated
approach for modeling educational content which considers different didactic
perspectives. We also propose a collaborative tool that supports modeling and
generating content based on the AIM?CID.},
 duplicado = {false},
 inserir = {false},
 title = {A Contribution to Modeling and Automatic Generation of Educational Content},
 year = {2009}
}

@article{1080,
 abstract = {The paper describes the 101haskell chrestomathy---a collection of Haskell programs implementing features of a hypothetical information system in a manner to represent knowledge about functional programming useful for learning (and teaching). The programs are enriched with documentation, metadata, and links to other knowledge resources such as Wikipedia and Haskell textbooks. The underlying ontology is informed by a process of knowledge integration which derives a consolidated vocabulary mainly by text mining and summarization from textbooks. The usefulness of 101haskell for teaching is demonstrated with a functional programming course that is directly based on 101haskell.},
 duplicado = {false},
 inserir = {false},
 title = {The 101haskell Chrestomathy: A Whole Bunch of Learnable Lambdas},
 year = {2013}
}

@article{1083,
 abstract = {Abstract. The development of educational modules concise units of study,
composed by theoretical and practical content which can be delivered to learners
by using technological resources has been extensively explored in the
context of Teaching and Learning. In this paper we present and discuss supporting
mechanisms for developing educational modules, emphasizing aspects
of modeling and generating content. We present AIM?CID an integrated
approach for modeling educational content which considers different didactic
perspectives. We also propose a collaborative tool that supports modeling and
generating content based on the AIM?CID.},
 duplicado = {false},
 inserir = {false},
 title = {Uma Contribuicao a Modelagem e Geracao Automatica de Conteudos Educacionais},
 year = {2009}
}

@article{1084,
 abstract = {In philology and linguistics, the term chrestomathy refers to a
collection of sample texts in one language designed to be useful
for learning the language by demonstrating some language aspects
such as language development or literary style.
In programming, the term program chrestomathy refers to a collection
of sample programs in one or more programming languages
designed to be useful for learning programming (or becoming more
proficient in programming) by demonstrating some programming
language aspects such as comparison of programming style, expressiveness,
and applicable programming techniques in one language
or across different languages.
More broadly, the term software chrestomathy [1] refers to a
collection of software systems relying on one or more software languages
as well as any number of software technologies; a software
chrestomathy demonstrates aspects of programming and software
development. When compared to a program chrestomathy, a software
chrestomathy collects systems rather than programs, thereby
possibly covering additional details such as building, testing, and
sample data.
The 101haskell chrestomathy is a collection of tiny or small
Haskell-based software systems designed to be useful for learning
functional programming in Haskell. The collected systems
present Haskell-based solutions to a number of general system
requirements as defined by the 101companies project [1]. Also,
the systems exercise alternative applicable programming techniques
and technologies (e.g., libraries) for the requirements. The
101haskell chrestomathy is the Haskell-specific sub-chrestomathy
of the 101companies chrestomathy which covers dozens of programming
and software languages. Following the terminology
of 101companies, the collected systems are called contributions,
thereby emphasizing the community aspect of collection.
It happens that Haskell has played a special role in the 101companies
project, i.e., Haskell has been used to bootstrap and demonstrate
various capabilities of 101. (To a slightly reduced extent, this
is also true for Java.) Here is a partial list of such capabilities:},
 duplicado = {false},
 inserir = {false},
 title = {The 101haskell chrestomathy},
 year = {2013}
}

@article{1085,
 abstract = {Abstract
Software Architecture (SA) plays a critical role in designing, developing and evolving cloud-based platforms that can be used to provision different types of services for consumers on demand. In this paper, we present a Reference Architecture (RA) for designing cloud-based Tools as a service SPACE (TSPACE), which can provision a bundled suite of tools following the Software as a Service (SaaS) model. The reference architecture has been designed by leveraging information structuring approaches and by using well-known architecture design principles and patterns. The RA has been documented using view-based approach and has been presented in terms of its context, goals, the RA meta-model, information structuring and relationship models using ontologies and components of the RA. We have demonstrated the feasibility and applicability of the RA with the help of a prototype and have used the prototype to provision software architecting tools. We have also evaluated the RA in terms of effectiveness of the design decisions and the RA's completeness and feasibility using scenario-based architecture evaluation method. The proposed TSPACE RA can provide valuable insights to information structure approaches and guidelines for designing and implementing TSPACE for various domains.},
 duplicado = {false},
 inserir = {false},
 title = {A Reference Architecture for provisioning of Tools as a Service: Meta-model, Ontologies and Design Elements},
 year = {2017}
}

@article{1086,
 abstract = {The testing of concurrent programs is essential to ensure the quality of such programs. One of the main challenges of such testing activity is to provide tools that enable it at a reasonable operational cost. The service orientation paradigm provides guidelines for the development of tools as services, addressing such requirement. The division of the structural testing of concurrent programs into services faces fundamental challenges. The objective of this paper is to provide structural testing of concurrent programs as a Web service composition. We divided this monolithic structure by defining the concepts, relations and parameters of the structural testing of concurrent programs to support this activity using a service composition. Our main contributions are in the definition of Web services for the structural testing of concurrent software, focusing on the service contracts and capabilities. The developed services present flexible contracts and capabilities that can be reused in different contexts.},
 duplicado = {false},
 inserir = {false},
 title = {ValiPar Service: Structural Testing of Concurrent Programs as a Web Service Composition},
 year = {2016}
}

@article{1088,
 abstract = {Abstract:
In this paper we discuss the establishment and application of IMA Tool - a web-based tool for the modeling and automatic generation of educational content. The tool is based on IMA-CID - an approach for modeling educational content, capable of addressing conceptual, instructional and didactic issues altogether, in an integrated way. IMA-CID and IMA Tool have been applied in the development of an educational module for software testing. The preliminary results indicate the effectiveness of such mechanisms for modeling and generating content.},
 duplicado = {false},
 inserir = {false},
 title = {IMA-Tool: A tool for modeling and automatic generation of educational content},
 year = {2013}
}

@article{1089,
 abstract = {Abstract:
Software architectures have received increasing attention by playing a significant role in determining the success and quality of software systems. In particular, reference architecture is a special type of architecture that captures the essence of software systems of a specific domain, achieving therefore well-recognized understanding of that domain. In spite of this, it is not observed concern in order to widely disseminate reference architectures and, as a consequence, the knowledge encompassed by these architectures. At the same time, Open Source Software (OSS) has been largely developed and used in both academy and industry. A diversity of OSS has been made available and has contributed to the software development through dissemination of knowledge that is encompassed mainly in the source code. Specifically, its success is due to OSS licences that have adequately supported its development and evolution. Thus, applying this same idea in order to disseminate reference architectures seems to be very interesting. The main contribution of this paper is to propose Open Source Reference Architecture (OSRA) that, based on principles of OSS, aims at promoting dissemination and evolution of reference architectures, intending to contribute to a more effective software development.},
 duplicado = {false},
 inserir = {false},
 title = {Towards the Open Source Reference Architectures},
 year = {2011}
}

@article{1090,
 abstract = {Systems-of-Systems (SoS) are a special type of systems composed by other systems.
These systems together can reach goals that they could not benefit when operating on
their own. However, this emerging discipline is not consolidated yet and there is a lack
of standardization in the terminology used. In this context, knowledge representation
approaches can be used to support activities in the SoS field. They can also play an
important role in formalizing concepts by providing a common understanding among
the community and practitioners. In this technical report we present a Systematic
Literature Review (SLR) conducted to identify how knowledge representation has been
applied to SoS. Our results show that interoperability is the most addressed topic. We
also noticed that there is a lack of formal approaches for establishing communication in
the SoS field.},
 duplicado = {false},
 inserir = {false},
 title = {A Systematic Literature Review on Systems-of-Systems Knowledge Representation},
 year = {2015}
}

@article{698,
 abstract = {Background: A Systematic Literature Review (SLR) is a methodology used to aggregate relevant evidence related to one or more research questions. Whenever new evidence is published after the completion of a SLR, this SLR should be updated in order to preserve its value. However, updating SLRs involves significant effort. Objective: The goal of this paper is to investigate the application of forward snowballing to support the update of SLRs. Method: We compare outcomes of an update achieved using the forward snowballing versus a published update using the search-based approach, i.e., searching for studies in electronic databases using a search string. Results: Forward snowballing showed a higher precision and a slightly lower recall. It reduced in more than five times the number of primary studies to filter however missed one relevant study. Conclusions: Due to its high precision, we believe that the use of forward snowballing considerably reduces the effort in updating SLRs in Software Engineering; however the risk of missing relevant papers should not be underrated.},
 duplicado = {false},
 inserir = {false},
 title = {Using Forward Snowballing to update Systematic Reviews in Software Engineering},
 year = {2016}
}

@article{699,
 abstract = {Abstract: This paper presents the construction and evaluation of SERP-test, a taxonomy aimed to improve communication between researchers and practitioners in the area of software testing. SERP-test can be utilized for direct communication in industry academia collaborations. It may also facilitate indirect communication between practitioners adopting software engineering research and researchers who are striving for industry relevance. SERP-test was constructed through a systematic and goal-oriented approach which included literature reviews and interviews with practitioners and researchers. SERP-test was evaluated through an online survey and by utilizing it in an industry academia collaboration project. SERP-test comprises four facets along which both research contributions and practical challenges may be classified: Intervention, Scope, Effect target and Context constraints. This paper explains the available categories for each of these facets (i.e., their definitions and rationales) and presents examples of categorized entities. Several tasks may benefit from SERP-test, such as formulating research goals from a problem perspective, describing practical challenges in a researchable fashion, analyzing primary studies in a literature review, or identifying relevant points of comparison and generalization of research.},
 duplicado = {false},
 inserir = {true},
 title = {SERP-test: a taxonomy for supporting industry academia communication},
 year = {2017}
}

@article{700,
 abstract = {Abstract: A mapping study provides a broad overview of a research area in order to determine whether there is research evidence on a particular topic. Results of a systematic mapping may identify suitable areas for performing future research. In this paper, we discuss our experience in using the findings of a mapping study on Knowledge Management (KM) in Software Testing for performing a real research project, which also applied other empirical approaches. The main goals of this paper are: (i) to reinforce the importance of a systematic mapping in the conduction of a research project by discussing a real case of such application, and (ii) to present the results of our survey on the most important aspects of KM when applied to software testing.},
 duplicado = {false},
 inserir = {false},
 title = {Using the Findings of a Mapping Study to Conduct a Research Project: A Case in Knowledge Management in Software Testing},
 year = {2015}
}

@article{701,
 abstract = {Abstract: Learning ontologies from software requirements specifications with individuals and relations between individuals to represent detailed information, such as input, condition and expected result of a requirement, is a difficult task. System specification ontologies (SSOs) can be developed from software requirement specifications to represent requirements and can be used to automate some time-consuming activities in software development processes. However, manually developing SSOs to represent requirements and domain knowledge of a software system is a time-consuming and a challenging task. The focus of this PhD is how to create ontologies semi-automatically from SRS. We will develop a framework that can be a possible solution to create semi-automatically ontologies from SRS. The developed framework will mainly be evaluated by using the constructed ontologies in the software testing process and automating a part of it. i.e. test case generation.},
 duplicado = {false},
 inserir = {true},
 title = {Ontology Learning from Software Requirements Specification (SRS)},
 year = {2016}
}

@article{702,
 abstract = {Abstract: Software testing is a complex and critical process for achieving product quality. Its importance has been increasing and well recognized, and there is a growing concern in improving the accomplishment of this process. In this context, Knowledge Management (KM) emerged as an important supporting approach to improve the software testing process. However, managing relevant testing knowledge requires effective means to represent and to associate semantics to a large volume of testing information. To address this concern, we have developed a Reference Ontology on Software Testing (ROoST). ROoST establishes a common conceptualization about the software testing domain, which can serve several KM-related purposes, such as defining a common vocabulary for knowledge workers with respect to the testing domain, structuring testing knowledge repositories, annotating testing knowledge items, and for making search for relevant information easier. In this paper, we present ROoST, and we discuss how it was developed using two ontology pattern languages. Moreover, we discuss how we evaluated ROoST following four complementary approaches: assessment by humans, data-driven evaluation, ontology testing, and application-based evaluation.},
 duplicado = {false},
 inserir = {true},
 title = {ROoST: Reference Ontology on Software Testing},
 year = {2017}
}

@article{703,
 abstract = {In software testing process a large amount of information is required and generated. This information can be stored as knowledge that needs to be managed and maintained using principles of knowledge management. Ontologies can act as a bridge by representing this testing knowledge in an accessible and understandable way. The purpose of this master thesis is to develop a Top domain ontology (TDO) which represents general software testing knowledge. This can be achieved by unifying the domain vocabularies that are used in the software testing. This top domain ontology can be used to link existing software testing ontologies. It can act as an interface between top-level and domain ontologies and guide the development of new software testing ontologies. The standards of ISTQB were used after careful consideration as the main source of knowledge, other sources such as existing software testing ontologies were also used to develop the ontology. The available ontologies for software testing were collected and evaluated against a list of evaluation criteria. The study shows that the available software testing ontologies do not fulfill the purpose for a TDO. In this work, we developed a TDO by using a combination of two ontology development methods: Ontology 101 and Methontology. The resources used for gaining knowledge and reusing the concepts from available ontologies made it possible for this TDO to have a better coverage in the field of software testing. The ontology was evaluated by using two methods: Competency questions and Ontology experts evaluation. The evaluation based on competency questions focuses on the structure of the ontology and shows that the ontology is well formed and delivers expected result. The evaluation by ontology experts was done against a set of quality criteria which represented the quality and coverage of ontology. The results shows that the ontology developed can be used as a TDO after fixing some comments from the evaluators. The evaluators agree that the ontology can be adapted to different application of software testing and that it fulfils the main purpose of top domain ontology. The developed ontology could be made better by evaluating and reusing the ontologies that are not published (e.g. STOWS). Ontology maintenance is an ongoing process. Ontology needs to be updated with new knowledge of software testing that emerges with research. },
 duplicado = {false},
 inserir = {true},
 title = {A Top Domain Ontology For Software Testing},
 year = {2016}
}

@article{704,
 abstract = {In software testing process a large amount of information is required and generated. This information can be stored as knowledge that needs to be managed and maintained using principles of knowledge management. Ontologies can act as a bridge by representing this testing knowledge in an accessible and understandable way.},
 duplicado = {false},
 inserir = {true},
 title = {A Top-Domain Ontology for Software testing},
 year = {2016}
}

@article{705,
 abstract = {Abstract: Software testing is a critical process for achieving product quality. Its importance is more and more recognized, and there is a growing concern in improving the accomplishment of this process. In this context, Knowledge Management emerges as an important supporting tool. However, managing relevant knowledge to reuse is difficult and it requires some means to represent and to associate semantics to a large volume of test information. In order to address this problem, we have developed a Reference Ontology on Software Testing (ROost). ROost is built reusing ontology patterns from the Software Process Ontology Pattern Language (SP-OPL). In this paper, we discuss how ROost was developed, and present a fragment of Roost that concerns with software testing process, its activities, artifacts, and procedures.},
 duplicado = {false},
 inserir = {true},
 title = {Using Ontology Patterns for Building a Reference Software Testing Ontology},
 year = {2013}
}

@article{706,
 abstract = {Abstract: Authors propose a transformation method of the glossary Standard glossary of terms used in Software Testing created by ISTQB document into a basic concept map. By applying natural language processing techniques and analyzing the discovered relations between concepts the most essential aspects of the software testing domain are elicited and integrated. As the result a browsable concept map is created. Browsable concept map can be used as a learning support tool.},
 duplicado = {false},
 inserir = {true},
 title = {Transformation of the Software Testing Glossary into a Browsable Concept Map},
 year = {2015}
}

@article{707,
 abstract = {Study includes software testing terms and ideas inventory, software testing overview and schematization on meta-level and structuring of lower level elements related to software testing such as testing oracles, testing levels, software quality characteristics, testing approaches, methods, and techniques. Main testing controversies are collected and described. Scientific basis is laid under proper use of such terms as testing approach, testing method, and testing technique.},
 duplicado = {false},
 inserir = {true},
 title = {Inventory of Testing Ideas and Structuring of Testing Terms},
 year = {2013}
}

@article{708,
 abstract = {Abstract: In this paper, we propose an experimental tool for analysis and graphical representation of glossaries. The original heuristic algorithms and analysis methods incorporated into the tool appeared to be useful to improve the quality of the glossaries. The tool was used for analysis of ISTQB Standard Glossary of Terms Used in Software Testing. There are instances of problems found in ISTQB glossary related to its consistency, completeness, and correctness described in the paper.},
 duplicado = {false},
 inserir = {true},
 title = {Heuristic Method to Improve Systematic Collection of Terminology},
 year = {2016}
}

@article{711,
 abstract = {Abstract. Authors propose an experimental tool for analysis and graphical representation of glossaries. The original heuristic algorithms and analysis methods incorporated into the tool GlossToolset appeared to be useful to improve the quality of the glossaries. The GlossToolset generates concept system with various representations. Authors analyze a glossary ISTQB Standard Glossary of Terms Used in Software Testing. There are instances of problems found in ISTQB glossary related to its consistency, completeness, and correctness described in the paper},
 duplicado = {false},
 inserir = {true},
 title = {Building of Concept System to Improve Systematic Collection of Terminology},
 year = {2016}
}

@article{712,
 abstract = {Abstract:
Whitening the testing of service-oriented applications can provide service consumers confidence on how well an application has been tested. However, to protect business interests of service providers and to prevent information leakage, the implementation details of services are usually invisible to service consumers. This makes it challenging to determine the test coverage of a service composition as a whole and design test cases effectively. To address this problem, we propose an approach to whiten the testing of service compositions based on events exposed by services. By deriving event interfaces to explore only necessary test coverage information from service implementations, our approach allows service consumers to determine test coverage based on selected events exposed by services at runtime without releasing the service implementation details. We also develop an approach to design test cases effectively based on event interfaces concerning both effectiveness and information leakage. The experimental results show that our approach outperforms existing testing approaches for service compositions with up to 49 percent more test coverage and an up to 24 percent higher fault-detection rate. Moreover, our solution can trade off effectiveness, efficiency, and information leakage for test case generation.},
 duplicado = {false},
 inserir = {false},
 title = {Whitening SOA Testing via Event Exposure},
 year = {2013}
}

@article{713,
 abstract = {Abstract:
An external web service may evolve without prior notification. In the course of the regression testing of a workflow-based web service, existing test case prioritization techniques may only verify the latest service composition using the not-yet-executed test cases, overlooking high-priority test cases that have already been applied to the service composition before the evolution. In this paper, we propose Preemptive Regression Testing (PRT), an adaptive testing approach to addressing this challenge. Whenever a change in the coverage of any service artifact is detected, PRT recursively preempts the current session of regression test and creates a sub-session of the current test session to assure such lately identified changes in coverage by adjusting the execution priority of the test cases in the test suite. Then, the sub-session will resume the execution from the suspended position. PRT terminates only when each test case in the test suite has been executed at least once without any preemption activated in between any test case executions. The experimental result confirms that testing workflow-based web service in the face of such changes is very challenging; and one of the PRT-enriched techniques shows its potential to overcome the challenge.},
 duplicado = {false},
 inserir = {false},
 title = {Preemptive Regression Testingof Workflow-Based Web Services},
 year = {2014}
}

@article{715,
 abstract = {Abstract
To predict web service quality, based on quality attributes set, experiments were carried out on QWS dataset. This study investigates the efficiency of web service classifiers.},
 duplicado = {false},
 inserir = {false},
 title = {Performance Evaluation of Web-Services Classification},
 year = {2014}
}

@article{716,
 abstract = {Abstract:
A workflow-based web service may use ultra-late binding to invoke external web services to concretize its implementation at run time. Nonetheless, such external services or the availability of recently used external services may evolve without prior notification, dynamically triggering the workflow-based service to bind to new replacement external services to continue the current execution. Any integration mismatch may cause a failure. In this paper, we propose Preemptive Regression Testing (PRT), a novel testing approach that addresses this adaptive issue. Whenever such a late-change on the service under regression test is detected, PRT preempts the currently executed regression test suite, searches for additional test cases as fixes, runs these fixes, and then resumes the execution of the regression test suite from the preemption point.},
 duplicado = {false},
 inserir = {false},
 title = {Preemptive Regression Test Scheduling Strategies: A New Testing Approach to Thriving on the Volatile Service Environments},
 year = {2012}
}

@article{717,
 abstract = {Software test is a technique to obtain information about software systems quality. Performance test is a type of software test that aims at evaluating software performance at a given load scenario, but it requires specialized knowledge about tools, activities and metrics of the domain. Since ontology is a promising knowledge representation technique, this paper presents a literature review to identify trends and compare researches of ontologies in the fields of software testing and software performance. Also, to investigate this issue from a practical perspective, it was developed an ontology for representing the core knowledge of performance testing. This paper presents the ontology and compare it with related ones. Then, semantic technologies are explored to demonstrate the practical feasibility of developing ontology-based applications for assisting testers with performance test planning and management.},
 duplicado = {false},
 inserir = {true},
 title = {An Ontology for Guiding Performance Testing},
 year = {2014}
}

@article{718,
 abstract = {Abstract:
Service-Oriented Computing allows building applications by reusing web-accessible services. However, current approaches still involve a large effort both at discovery of services and their successful integration. This paper presents a novel approach to assist developers at discovery, selection and integration of services. In particular, the paper focuses on the selection method that involves two main evaluations on candidate services to achieve a concrete decision upon the most appropriate service. Initially, a syntactic Interface Compatibiliy assessment characterizes the list of candidate services according to a calculated syntactic distance to then proceed with a Behavior Compatibility evaluation that is based on a blackbox testing framework. The usefulness of the selection method is highlighted through a series of case studies.},
 duplicado = {false},
 inserir = {false},
 title = {Testing-Based Process for Service-Oriented Applications},
 year = {2011}
}

@article{719,
 abstract = {Abstract. Service-Oriented Computing promotes building applications by
consuming reusable services. However, facing the selection of adequate
services for a specific application still is a major challenge. Even with a reduced
set of candidate services, the assessment effort could be overwhelming. On
previous work we have presented an approach to assist developers on the
selection of services from a syntactic viewpoint of a matchmaking process for
interfaces compatibility. In this paper we extend the approach to assess the
behavior of services taking advantage of a black-box testing framework to
verify compatibility on the expected execution behavior of a candidate service.
This paper analyzes the selection method through a case study, to show its
potential on determining the best choice of a service among a set of candidates.},
 duplicado = {false},
 inserir = {false},
 title = {Behavior Assessment based Selection Method for Service Oriented Applications Integrability},
 year = {2012}
}

@article{720,
 abstract = {Abstract:
Writing software is a difficult and expensive task. The large volume of software, lack of or partial software requirement and frequent update in software requirement makes software vulnerable without rigorous testing. Manual testing is infeasible in such an arena and lead to unwanted changes in code and specification. Because of increasing use of software in industry, automation of the software testing process has thrown a challenge to the researchers in this field. In this paper, we have presented a comprehensive guideline to tackle the challenge.},
 duplicado = {false},
 inserir = {false},
 title = {A proposed framework for full automation of software testing process},
 year = {2012}
}

@article{721,
 abstract = {Abstract:
This paper proposes an intelligent broker approach to service composition and collaboration. The broker employs a planner to generate service composition plans according to service usage and workflow knowledge, dynamically searches for services according to the plan, then invokes and coordinates the executions of the selected services at runtime. A prototype called I-Broker has been implemented to support the approach, which can be instantiated by populating the knowledge-base with domain specific knowledge to form domain specific brokers. This paper also reports experiments that evaluate the scalability of the approach.},
 duplicado = {false},
 inserir = {false},
 title = {An Intelligent Broker Approach to Semantics-Based Service Composition},
 year = {2011}
}

@article{722,
 abstract = {Testing on services-based systems faces the challenges of dynamic collaboration. Services are distributed software that can be bound to establish collaborations on-demand. To verify and validate the services, testing needs to react automatically in a coordinated approach. Software agents, which are characterized by persistence, autonomy, social ability and reactivity, are thus introduced to facilitate test deployment, execution, collaboration, and run-time decision making. This paper proposes a design of test agent model, including agents' knowledge, events, actions and interpreter. The knowledge represents the detected environment status, such as test results and changes in services under test. The action models testing behavior such as test configuration, test deployment and test schedule. The Interpreter defines the rules to select actions or parameters on certain events and conditions. In this way, given a set of knowledge at a certain time, a test agent dynamically adjusts its behavior according to its pre-defined rules and strategies. Case studies and experiments are exercised to apply the generic agent design to specific testing tasks such as performance testing and coverage-based testing.},
 duplicado = {false},
 inserir = {true},
 title = {Design of intelligent agents for collaborative testing of service-based systems},
 year = {2011}
}

@article{723,
 abstract = {Abstract:
This paper introduces a testing strategy that is suitable for testing service-based applications. We describe an architecture that responds to changes of service operation, operation arguments and service composition changes. Our proof-of-concept test system performs runtime testing on our model atomic and composite web services using a random testing technique. A novel change identification method was developed to capture changes at the service interface. The test system is able to identify changes that occur in service operations and operational arguments in a service description of a test candidate. Our approach uses a new method to detect changes in a service inventory. Automated reconfiguration is used to support the continuous operation of the testing systems during a test candidate change.},
 duplicado = {false},
 inserir = {false},
 title = {Dynamic Test Reconfiguration for Composite Web Services},
 year = {2014}
}

@article{724,
 abstract = {Abstract:
A web-based service consists of layers of programs (components) in the technology stack. Analyzing program executions of these components separately allows service vendors to acquire insights into specific program behaviors or problems in these components, thereby pinpointing areas of improvement in their offering services. Many existing approaches for testing as a service take an orchestration approach that splits components under test and the analysis services into a set of distributed modules communicating through message-based approaches. In this paper, we present the first work in providing dynamic analysis as a service using a virtual machine (VM)-based approach on dynamic data race detection. Such a detection needs to track a huge number of events performed by each thread of a program execution of a service component, making such an analysis unsuitable to use message passing to transit huge numbers of events individually. In our model, we instruct VMs to perform holistic dynamic race detections on service components and only transfer the detection results to our service selection component. With such result data as the guidance, the service selection component accordingly selects VM instances to fulfill subsequent analysis requests. The experimental results show that our model is feasible.},
 duplicado = {false},
 inserir = {false},
 title = {Architecturing Dynamic Data Race Detection as a Cloud-Based Service},
 year = {2015}
}

@article{725,
 abstract = {Abstract:
As Web Services becomes mature and popular, they are always integrated together, forming systems to carry out coherent tasks. The distributed application of Web Services involves many standard protocols and various runtime behaviors and thus makes the systems' automated testing more difficult. In this paper we propose a series of applicable automated testing methods for Web Services system. First, deduce abstract test cases from interaction requirement properties of Web Services system. Second, specify test cases according to SWRL (Semantic Web Rule Language) properties and abstract test cases. Finally, generate mutants under AOP (Aspect-Oriented Programming) technology support, drive them by specific test cases using improved Fit (Framework for Integrated Test), and then kill mutants based on business logic. Experiments have shown that our algorithms meet the applied demands and perform well as an automated testing tool for Web Services system.},
 duplicado = {false},
 inserir = {true},
 title = {Automated testing of Web Services system based on OWL-S},
 year = {2012}
}

@article{726,
 abstract = {Abstract:
Software-as-a-Service (SaaS) is a model of cloud computing in which software functions are delivered to the users as services. The past few years have witnessed its global flourishing. In the foreseeable future, SaaS applications will integrate with the Internet of Things, Mobile Computing, Big Data, Wireless Sensor Networks, and many other computing and communication technologies to deliver customizable intelligent services to a vast population. This will give rise to an era of what we call Big SaaS systems of unprecedented complexity and scale. They will have huge numbers of tenants/users interrelated in complex ways. The code will be complex too and require Big Data but provide great value to the customer. With these benefits come great societal risks, however, and there are other drawbacks and challenges. For example, it is difficult to ensure the quality of data and metadata obtained from crowd sourcing and to maintain the integrity of conceptual model. Big SaaS applications will also need to evolve continuously. This paper will discuss how to address these challenges at all stages of the software lifecycle.},
 duplicado = {false},
 inserir = {false},
 title = {Big SaaS: The Next Step beyond Big Data},
 year = {2015}
}

@article{727,
 abstract = {Abstract. Connecting services to rapidly developing service-oriented
applications is a challenging issue. Selection of adequate services implies to
face an overwhelming assessment effort, even with a reduced set of candidate
services. On previous work we have presented an approach for service selection
addressing the assessment of WSDL interfaces and the expected execution
behavior of candidate services. In this paper we present a plugin for the Eclipse
IDE to support the approach and to assist developers daily tasks on exploring
services integrability. Particularly for behavioral compatibility we make use of
two testing frameworks: JUnit and MuClipse to achieve a compliance testing
strategy. },
 duplicado = {false},
 inserir = {false},
 title = {A Software Tool for Selection and Integrability on Service Oriented Applications},
 year = {2014}
}

@article{728,
 abstract = {Web Service Business Process Execution Language (WS-BPEL) is one of the most popular service-oriented workflow applications. The unique features (e.g. dead path elimination semantics and correlation mechanism) of WS-BPEL applications have raised enormous problems to its test case generation, especially in unit testing. Existing studies mainly assume that each path in the control flow graphs that correspond to WS-BPEL applications is feasible, which always yields imprecise test cases or complicates testing results. The current study tackles this problem based on satisfiability modulo theory solvers. First, a new coverage criterion is proposed to measure the quality of test sets for testing WS-BPEL applications. Second, decomposition algorithms are presented to obtain test paths that meet the proposed coverage criterion. Finally, this paper symbolically encodes each test path with several constraints by capturing the unique features of WS-BPEL. These constraints are solved and the test cases (test paths and test data) are obtained with the help of satisfiability modulo theory solvers to test WS-BPEL applications effectively. Experiments are conducted using our approach and other typical approaches (e.g. message-sequence generation-based approach and concurrent path analysis approach) with 10 WS-BPEL applications. Experimental results demonstrate that the test cases generated by our approach can avoid instantiating idle instance and expose more faults.},
 duplicado = {false},
 inserir = {false},
 title = {Generating effective test cases based on satisfiability modulo theory solvers for service-oriented workflow applications},
 year = {2016}
}

@article{729,
 abstract = {Abstract:
With the advent of Web 2.0 application, and the increasing number of browsers and platforms on which the applications can be executed, cross-browser incompatibilities (XBIs) are becoming a serious problem for organizations to develop web-based software. Although some techniques and tools have been proposed to identify XBIs, they cannot assure the same execution when the application runs across different browsers as only explicit user activity is considered, and thus prone to generating both false positives and false negatives. To address this limitation, this paper describes X-Check, a platform that enables cross-browser testing as a service by leveraging record/replay technique. Comparing to existing techniques and tools, X-Check supports to detect cross-browser issues with high accuracy. It also provides useful support to developers for diagnosis and (eventually) elimination of XBIs. Our empirical evaluation shows that X-Check is effective, improves the state of the art.},
 duplicado = {false},
 inserir = {false},
 title = {X-Check: A Novel Cross-Browser Testing Service Based on Record/Replay},
 year = {2016}
}

@article{730,
 abstract = {The dynamic composition of services owned by different vendors demands a high degree of test automation, which must be able to cope with the diversity of service implementation techniques and to meet a wide range of test requirements on-the-fly. These goals are hard to achieve because of the lack of software artefacts of the composed services and the lack of the means of control over test executions and the means of observations on the internal behaviours of composed services. Yet, such integration testing on-the-fly must be non-intrusive and non-disruptive while the composed services are in operation. This chapter presents a test automation framework for such on-the-fly testing of service compositions to facilitate the collaboration between test services through utilisation of Semantic Web Services techniques. In this framework, an ontology of software testing called STOWS are used for the registration, discovery and invocation of test services. The composition of test services is realized by using test brokers, which are also test services but specialized in the coordination of other test services. The ontology can be extended and updated through an ontology management service so that it can support a wide open range of test activities, methods, techniques and types of software artefacts. We also demonstrate the uses of the framework by two running examples.},
 duplicado = {false},
 inserir = {true},
 title = {A Test Automation Framework for Collaborative Testing of Web Service Dynamic Compositions},
 year = {2014}
}

@article{731,
 abstract = {Abstract:
Test case prioritization for regression testing is an approach that schedules test cases to improve the efficiency of service-oriented workflow application testing. Most of existing prioritization approaches range test cases according to various metrics (e.g., Statement coverage, path coverage) in different application context. Service-oriented workflow applications orchestrate web services to provide value-added service and typically are long-running and time-consuming processes. Therefore, these applications need more precise prioritization to execute earlier those test cases that may detect failures. Surprisingly, most of current regression test case prioritization researches neglect to use internal structure information of software, which is a significant factor influencing the prioritization of test cases. Considering the internal structure information and fault propagation behavior of modifications respect to modified version for service-oriented workflow applications, we present in this paper a new regression test case prioritization approach. Our prioritization approach schedules test cases based on dependence analysis of internal activities in service-oriented workflow applications. Experimental results show that test case prioritization using our approach is more effective than conventional coverage-based techniques.},
 duplicado = {false},
 inserir = {false},
 title = {Modification Impact Analysis Based Test Case Prioritization for Regression Testing of Service-Oriented Workflow Applications},
 year = {2015}
}

@article{732,
 abstract = {Abstract
Building Service-oriented Applications implies the
selection of adequate services to fulfill required
functionality. Even a reduced set of candidate
services involves an overwhelming assessment
effort. In a previous work we have presented an
approach to assist developers in the selection of
Web Services. In this paper we detail its behavioral
assessment procedure, which is based on
compliance testing. This is done by a specific
Behavioral Test Suite for exposing required
messages interchange from/to a client application
and a Web Service. In addition, helpful information
takes shape to build the needed adaptation logic to
safely integrate the selected candidate into a
Service-oriented Application. A concise case study
shows the potential of this approach for both
selection and integration of a service among a set of
candidates. },
 duplicado = {false},
 inserir = {false},
 title = {Testing-based Behavioral Assessment for Service Selection},
 year = {2013}
}

@article{733,
 abstract = {XML is extensively used in web services for integration and data exchange. Its popularity and wide adoption make it an attractive target for attackers and a number of XML-based attack types have been reported recently. This raises the need for cost-effective, automated testing of web services to detect XML-related vulnerabilities, which is the focus of this paper. We discuss a taxonomy of the types of XML injection attacks and use it to derive four different ways to mutate XML messages, turning them into attacks (tests) automatically. Further, we consider domain constraints and attack grammars, and use a constraint solver to generate XML messages that are both malicious and valid, thus making it more difficult for any protection mechanism to recognise them. As a result, such messages have a better chance to detect vulnerabilities. Our evaluation on an industrial case study has shown that a large proportion (78.86%) of the attacks generated using our approach could circumvent the first layer of security protection, an XML gateway (firewall), a result that is much better than what a state-of-the-art tool based on fuzz testing could achieve.},
 duplicado = {false},
 inserir = {false},
 title = {Automated and effective testing of web services for XML injection attacks},
 year = {2016}
}

@article{734,
 abstract = {Abstract Web services are designed to be discovered and
composed dynamically, which implies that testing must also
be done dynamically. This involves both the generation of test
cases and the checking of test results. This paper presents
algorithms for both of these using the technique of algebraic
specification. It focuses in particular on the problem that web
services, when they are third-party, have poor controllability
and observability, and introduces a solution known as monic
floating checkable test cases. A prototype tool has implemented
the proposed testing technique and it is applied to a case study
with a real industry application GoGrid, demonstrating that
the technique is both applicable and feasible},
 duplicado = {false},
 inserir = {false},
 title = {Algebraic Testing of Web Services: The Monic Approach},
 year = {2015}
}

@article{735,
 abstract = {This project will introduces a testing policy that is appropriate
for testing service-based applications. And enhance web service
performance. I describe an architecture that responds to changes
of service operation , arguments and changed service
composition. To prove test system performs runtime testing by
random testing technique on my model atomic and composite
web services. to support the continuous operation of the testing
systems during a test candidate change done by Automated
reconfiguration.},
 duplicado = {false},
 inserir = {false},
 title = {A Survey on Technique for Dynamic Test Reconfiguration for Composite Web Services},
 year = {2015}
}

@article{736,
 abstract = {Abstract
Context: Recent years have witnessed growing interests in semantic web and its related technologies. While various frameworks have been proposed for designing semantic web services (SWS), few of them aim at testing.
Objective: This paper investigates into the technologies for automatically deriving test cases from semantic web service descriptions based on the Web Service Modeling Ontology (WSMO) framework.
Method: WSMO goal specifications were translated into B abstract machines. Test cases were generated via model checking with calculated trap properties from coverage criteria. Furthermore, we employed mutation analysis to evaluate the test suite. In this approach, the model-based test case generation and code-based evaluation techniques are independent of each other, which provides much more accurate measures of the testing results.
Results: We applied our approach to a real-world case study of the Amazon E-Commerce Service (ECS). The experimental results have validated the effectiveness of the proposed solution.
Conclusion: It is concluded that our approach is capable of automatically generating an effective set of test cases from the WSMO goal descriptions for SWS testing. The quality of test cases was measured in terms of their abilities to discover the injected faults at the code level. We implemented a tool to automate the steps for the mutation-based evaluation.},
 duplicado = {false},
 inserir = {true},
 title = {Goal-based testing of semantic web services},
 year = {2009}
}

@article{737,
 abstract = {Cloud computing brings new business opportunities and services on infrastructure, platform and software level. It provides a new way for testing software applications known as Testing-as-a-Service (TaaS). TaaS eliminates the need of installing and maintaining testing environments on customers side and reduces the testing cost on pay-per-use basis. Availability of on-demand testing services allows testers to provide raw cloud resources at run time, when and where needed. This paper addresses TaaS benefits by proposing a TaaS-enabled framework offering cloud-based testing services. The framework, called Testing as a Service Software Architecture (TASSA), supports testing of web service compositions described with Business Process Execution Language for Web Services (WS-BPEL). It consists of two main components: (1) TaaS functionality for fault injection and dependencies isolation of the application under test and (2) Graphical User Interface (GUI) for test case design and execution. TASSA framework could be installed on a local computer or used for building a cloud test lab on a virtual machine. Its feasibility is proved through a case study on a sample business process from wine industry.},
 duplicado = {false},
 inserir = {false},
 title = {Automated Web Service Composition Testing as a Service},
 year = {2016}
}

@article{739,
 abstract = {Abstract-Non-functional testing is an
extremely important activity in any Web
service based project. Regardless of the
architectural style being used, nonfunctional
aspects must be fulfilled by a
system. Even if GUI of application is
attractive, user will not visit the
application if it takes more time to load
the pages or crashes when multiple users
access it. In most of the situations, nonfunctional
requirements such as
performance and scalability are validated
using tools. SoapUI can be used in
functional as well as non-functional
testing. Performance, scalability, and
usability are some of the key nonfunctional
attributes expected from any
application. This paper look into the
usage of SoapUI to do performance tests
of web services. In Section II, we discuss
why it is important to consider
performance implications in an SOA. We
proceed with describing performance test
planning of web services. Next, we look at
how SoapUI can be used for various
performance tests with our example.},
 duplicado = {false},
 inserir = {false},
 title = {Web Service Performance Testing},
 year = {2008}
}

@article{740,
 abstract = {Abstract:
Web services are designed to be discovered and composed dynamically, which implies that testing must also be done dynamically. This involves both the generation of test cases and the checking of test results. This paper presents algorithms for both of these using the technique of algebraic specification. It focuses in particular on the problem that web services, when they are third-party, have poor controllability and observability, and introduces a solution known as monic floating checkable test cases. A prototype tool has implemented the proposed testing technique and it is applied to a case study with a real industry application GoGrid, demonstrating that the technique is both applicable and feasible.},
 duplicado = {false},
 inserir = {false},
 title = {Monic Testing of Web Services Based on Algebraic Specifications},
 year = {2016}
}

@article{742,
 abstract = {This paper is a report on The 8th IEEE/ACM International Workshop on Automation of Software Test (AST 2013) at the 35th International Conference on Software Engineering (ICSE 2013). It sets a special theme on testing-as-a-service (TaaS). Keynote speech and charette discussions are organized around this special theme. Eighteen full research papers and six short papers will be presented in the two-day workshop. The report will give the background of the workshop and the selection of the special theme, and report on the organization of the workshop. The provisional program will be presented with a list of the sessions and papers to be presented at the workshop.},
 duplicado = {false},
 inserir = {false},
 title = {8th international workshop on automation of software test (AST 2013)},
 year = {2013}
}

@article{743,
 abstract = {Abstract
Regression test selection, which is well known as an effective technology to ensure the quality of modified BPEL applications, is regarded as an optimal control issue. The BPEL applications under test serves as a controlled object and the regression test selection strategy functions as the corresponding controller. The performance index is to select fewest test cases to test modified BPEL applications. In addition, a promising controller (regression test selection approach) should be safe, which means that it can select all test cases in which faults might be exposed in modified versions under controlled regression testing from the original test suite. However, existing safe controllers may rerun some test cases without exposing fault. In addition, the unique features (e.g., dead path elimination semantics, communication mechanism, multi-assignment etc.) of BPEL applications also raise enormous problems in regression test selection. To address these issues, we present in this paper a safe optimal controller for BPEL applications. Firstly, to handle the unique features mentioned above, we transform BPEL applications and their modified versions into universal BPEL forms. Secondly, For our optimal controller, BPEL program dependence graphs corresponding to the two universal BPEL forms are established. Finally, guided by behavioral differences between the two versions, we construct an optimal controller and select test cases to be rerun. By contrast with the previous approaches, our approach can eliminate some unnecessary test cases to be selected. We conducted experiments with 8 BPEL applications to compare our approach with other typical approaches. Experimental results show that the test cases selected using our approach are fewer than other approaches.},
 duplicado = {false},
 inserir = {false},
 title = {Optimal control based regression test selection for service-oriented workflow applications},
 year = {2017}
}

@article{744,
 abstract = {Nowadays, the External Markup Language (XML) is the most commonly used technology in web services for enabling service providers and consumers to exchange data. XML is also widely used to store data and configuration files that control the operation of software systems. Nevertheless, XML suffers from several well-known vulnerabilities such as XML Injections (XMLi). Any exploitation of these vulnerabilities might cause serious and undesirable consequences, e.g., denial of service and accessing or modifying highly-confidential data. Fuzz testing techniques have been investigated in the literature to detect XMLi vulnerabilities. However, their success rate tends to be very low since they cannot generate complex test inputs required for the detection of these vulnerabilities. Furthermore, these approaches are not effective for real-world complex XML-based enterprise systems, which are composed of several components including front-end web applications, XML gateway/firewall, and back-end web services.

In this dissertation, we propose several automated security testing strategies for detecting XML-based vulnerabilities. In particular, we tackle the challenges of security testing in an industrial context. Our proposed strategies, target various and complementary aspects of security testing for XML-based systems, e.g., test case generation for XML gateway/firewall. The development and evaluation of these strategies have been done in close collaboration with a leading financial service provider in Luxembourg/Switzerland, namely SIX Payment Services (formerly known as CETREL S.A.). SIX Payment Services processes several thousand financial transactions daily, providing a range of financial services, e.g., online payments, issuing of credit and debit cards.

The main research contributions of this dissertation are:
-A large-scale and systematic experimental assessment for detecting vulnerabilities in numerous widely-used XML parsers and the underlying systems using them. In particular, we targeted two common XML parsers vulnerabilities: (i) XML Billion Laughs (BIL), and (ii) XML External Entities (XXE).
- A novel automated testing approach, that is based on constraint-solving and input mutation techniques, to detect XMLi vulnerabilities in XML gateway/firewall and back-end web services.
- A black-box search-based testing approach to detect XMLi vulnerabilities in front-end web applications. Genetic algorithms are used to search for inputs that can manipulate the application to generate malicious XML messages.
- An in-depth analysis of various search algorithms and fitness functions, to improve the search-based testing approach for front-end web applications.
- Extensive evaluations of our proposed testing strategies on numerous real-world industrial web services, XML gateway/firewall, and web applications as well as several open-source systems.},
 duplicado = {false},
 inserir = {false},
 title = {Automated and Effective Security Testing for XML-based Vulnerabilities},
 year = {2017}
}

@article{745,
 abstract = {Abstract In the wake of Service Oriented Architecture (SOA) and its applications that integrated services provided by different businesses,
reliability of services play a vital role in the success of those businesses. Testing is the widely accepted approach used to find reliability
of services. Testing web services that are composed to form SOA applications is non-trivial and hard to achieve. Web services with BPEL
workflow drive most of the businesses that affect all stakeholders. Building comprehensive test strategies for cohesiveness of underlying
components and loosely coupled nature of integrated heterogeneous pieces of software is very challenging. Moreover business processes
tend to change that makes the job of testing much more complex. Many researchers contributed towards testing of web services. In this paper
we review the present state-of-the-art of web service composition methods, testing web services, automatic test case generating and
automatic test case reconfiguration. This paper provides insights found in the literature in terms of test methodologies, tools and techniques
used for testing web services and web service compositions. },
 duplicado = {false},
 inserir = {false},
 title = {Review of the Present State-of-the-Art of Dynamic Test Reconfiguration of Composite Web Services},
 year = {2016}
}

@article{746,
 abstract = {Abstract - Testing a service based application with more
suitable approach is introduced in this paper. Paper has an
architecture that responds to changes of service operation ,
service composition changes and allow all Footprints by Tree.
Our test system is able to identify changes that occur in service
operations and operational arguments in a service description
of a test candidate. Automated reconfiguration is used to
support the continuous operation of the testing systems during
a test candidate change. Our proof-of-concept test system
performs runtime testing on our model atomic and composite
web services using a random testing technique with previous
footprints records},
 duplicado = {false},
 inserir = {false},
 title = {A Technique for Testing Composed Web Services including Footprint},
 year = {2017}
}

@article{747,
 abstract = {Abstract:
A concrete service consists of a number of program components, each of which is integrated to the service at either design time or runtime. In testing a concrete service, testers should validate the correctness of each of its components under diverse service consumption scenarios. Analyzing the program executions of these components under different configurations allows developers to compare and pinpoint issues therein. There is surprisingly little work in bridging this gap. In this paper, to the best of our knowledge, we propose the first work in designing dynamic analysis-as-a-service using a multi-virtual machine (multi-VM) approach to dynamic data race detection. Almost all existing work on dynamic data race detection focuses on improving detection precision, efficiency, or coverage of thread interleaving scenarios on the same but single compiled concurrent program component. Our model continually selects VM instances, each hosting a different compiled version of the same program component and running a state-of-the-art detector to detect data races. As such, our model innovatively takes existing race detectors as building blocks and operates at a higher level of abstraction. We have evaluated our proposal through an experiment. The experiment reveals that the multi-VM approach is feasible in monitoring multiple compiled versions and can detect different races both in amount and in detection probability. Under a limited execution budget constraint, the multi-VM approach is also significantly more effective in detecting races than approaches that use single compiled versions only. Some races hidden deeply in one compiled version have been found to be significantly more detectable in some other compiled versions of the same service component.},
 duplicado = {false},
 inserir = {false},
 title = {SDA-CLOUD: A Multi-VM Architecture for Adaptive Dynamic Data Race Detection},
 year = {2016}
}

@article{748,
 abstract = {Abstract:
This paper presents a test framework for a large-scale message broker system for consumer devices, where communication is between devices and goes through a message broker. In testing such a system, administering tests is a burdensome task, because a tester has to design and operate a test application managing multiple connections and to validate complicated traffic patterns between clients via a broker. In addition, a tester has to validate service availability when numerous devices use the system and some message brokers have gone down or the system is scaled-out/in. In such an eventuality, a message broker in operation is removed from the system or a new message broker is added to the system. In our framework, a tester can write a test scenario to validate message transfers between devices and service availability in the case of changes in the system structure on the fly. In a case study, we implement a prototype framework on Apache JMeter for remote control system for home appliances. We evaluate the target system with our framework in several test scenarios and confirm service availability when one million devices use the system and when scale-out/in occurs.},
 duplicado = {false},
 inserir = {false},
 title = {A test framework for large-scale message broker system for consumer devices},
 year = {2015}
}

@article{749,
 abstract = {Software testers have great challenges in testing of web services therefore testing technique must be developed
for testing of web services. Web service composition is an active research area over last few years. This paper
proposes a framework for testing of fault tolerant composition of web services. It will tolerate faults while
composition of web services. Exception handling and transaction techniques are used as fault handling
mechanisms. After composition web services are deployed on WS-BPEL engine. Testing Framework will fetch
results of composite web service from WS-BPEL engine and check whether composed web service is fault
tolerant and it is in the consistent state. },
 duplicado = {false},
 inserir = {false},
 title = {A TESTING FRAMEWORK FOR FAULT TOLERANT COMPOSITION OF TRANSACTIONAL WEB SERVICES},
 year = {2012}
}

@article{750,
 abstract = {Service-centric System Testing (ScST) is more challenging than testing traditional software
due to the complexity of service technologies and the limitations that are imposed
by the SOA environment. One of the most important problems in ScST is the problem
of realistic test data generation. Realistic test data is often generated manually or using
an existing source, thus it is hard to automate and laborious to generate. One of the
limitations that makes ScST challenging is the cost associated with invoking services
during testing process.
This thesis aims to provide solutions to the aforementioned problems, automated
realistic input generation and cost reduction in ScST. To address automation in realistic
test data generation, the concept of Service-centric Test Data Generation (ScTDG) is
presented, in which existing services used as realistic data sources. ScTDG minimises
the need for tester input and dependence on existing data sources by automatically generating
service compositions that can generate the required test data. In experimental
analysis, our approach achieved between 93% and 100% success rates in generating realistic
data while state-of-the-art automated test data generation achieved only between
2% and 34%.
The thesis addresses cost concerns at test data generation level by enabling data
source selection in ScTDG. Source selection in ScTDG has many dimensions such as
cost, reliability and availability. This thesis formulates this problem as an optimisation
problem and presents a multi-objective characterisation of service selection in ScTDG,
aiming to reduce the cost of test data generation.
A cost-aware pareto optimal test suite minimisation approach addressing testing cost concerns during test execution is also presented. The approach adapts traditional multi-objective minimisation approaches to ScST domain by formulating ScST concerns,
such as invocation cost and test case reliability. In experimental analysis, the
approach achieved reductions between 69% and 98.6% in monetary cost of service
invocations during testing.},
 duplicado = {false},
 inserir = {false},
 title = {Automated Realistic Test Input Generation and Cost Reduction in Service-centric System Testing},
 year = {2013}
}

@article{752,
 abstract = {The foremost realization of Service-Oriented Architecture (SOA) is Web Services (WS). It defines a
framework for nimble and adaptable amalgamation among self directed services based on Internet open standards.
SOA allows composition of distributed applications free from their platform and thus reduces the cost of such
compositions and makes them easier and faster to develop. However, testing has been confronted due the vibrant
and collaborative nature of WS. This paper analyses the key techniques present in testing of WS and shows the
strength and weakness of current web service testing technology. },
 duplicado = {false},
 inserir = {false},
 title = {Survey on Collaborative Testing of Web Services},
 year = {2013}
}

@article{753,
 abstract = {Abstract As we know, software testing is an important part of software development lifecycle. More than 50% of the whole system development work and total cost were spend on the software testing. And its estimated that almost 60% of the total test time and cost were spent on the design of test cases. In recent years, the automated testing gradually replaces the traditional manual testing to become an important branch of software testing. In automated testing, the most important thing is to design and generate valid test case automatically. In this paper, In order to reduce the workload of testers on the test case generation, improve the test efficiency and makes test experiences be passed on, we design a common description method for the test case based on ontology; propose a semantic similarity measure method to retrieve the usable test cases from test case library based on WordNet; establish the relationship between the test case and test requirement though rules to modify the retrieved test case; generate the final test case sequence. At last, the practical applicability of the approach is evaluated through an experiment. },
 duplicado = {false},
 inserir = {true},
 title = {The Use of Ontology in Case Based Reasoning for Reusable Test Case Generation},
 year = {2015}
}

@article{758,
 abstract = {Abstract: Product development time estimation is important for project management tasks. This study investigates the impact of requirements reuse on product development duration for different products in a similar domain. We propose an analytical tool to estimate the minimum time to be saved given the percentage of requirements reused from earlier projects. This paper presents basis for the formal reuse of hardware components as a strategy to reduce the task for verifying new hardware elements. Assuming the existence of a library of formally verified hardware components, we propose to make effective reuse of these existing elements when creating new ones. The concept of software component reuse is simple: the idea of building and using "software preferred parts." By building systems out of carefully designed, pre-tested components, one will save the cost of designing, writing and testing new code. The practice of reuse has not proven to be this simple however, and there are many misconceptions about how to implement and gain benefit from software reuse. One case from organization has been studied for software and system development projects, which consist of hardware and software components. The results of the case studies are compared with a study in the literature on product development time.},
 duplicado = {false},
 inserir = {false},
 title = {IMPROVING DEVELOPMENT TIMES BY REUSING FLEXIBLE HARDWARE AND SOFTWARE COMPONENT},
 year = {2013}
}

@article{759,
 abstract = {Accurate development time estimation is crucial for project management in general, and critical for software intensive systems projects, in particular. Before beginning the project, little information is available for development details. Therefore, development time may not be estimated correctly. If data on previous projects in the same domain is available, this can be used for development time estimations. At the beginning of the project, requirements are defined and requirements specification document is created as a formal document in the
v organizations. By using the reused requirements from previous projects, a similarity analysis can be performed and this analysis can be used for development time estimation. This study investigates the applicability of a model that was proposed earlier for project management in general, in software intensive systems development projects. In this scope, the impact of requirements reuse on product development duration for different products in a similar domain is studied. Similarity analysis has been performed for different products in the same domain and the result of this analysis is used to estimate the development time. For development time estimation, Griffins model [9] is used. For the applicability of Griffins model for industrial companies, nine case studies from different organizations have been performed for software and system development projects which consist of hardware and software components. The results of the case studies are compared with Griffins model. According to the empirical results, a modification to Griffins formulation for product development time is proposed for software projects. For the projects which include only software or both software and hardware, the proposed model will guide project managers to estimate project budgets more accurately.},
 duplicado = {false},
 inserir = {false},
 title = {IN PARTIAL FULFILLMENT OF THE REQUIREMENTS FOR THE DEGREE OF DOCTOR OF PHILOSOPY IN THE DEPARTMENT OF INFORMATION SYSTEM},
 year = {2013}
}

@article{760,
 abstract = {Abstract: The BSS (Business Support Systems) has played a critical role in strictly competition telecom market. They constantly face stringent challenges such as quality assurance as well as time to market and budget. The test case design is an important step and a higher cost in system quality assurance phase. However by knowledge few test data correlation based approach has some drawbacks such as the coverage limitations and effectives of test case selection. We believe if there is a suitable testing case reusability approaches would have good and enough coverage and lower cost. Therefore, in this paper, we present a quality assurance framework and reusability based approach to design and select test case effectively. On the other hand the study introduces a real world BSS quality assurance works as case study to practice and examine the proposed approach. This article is a continuous work of the BSS transformation project.},
 duplicado = {false},
 inserir = {false},
 title = {A Quality Assurance Approach and Case Study in BSS},
 year = {2016}
}

@article{761,
 abstract = {We extend an existing model proposed for estimating project duration for industrial projects in general, to software intensive systems projects. We show, through nine dierent cases studies from dierent sectors, that product similarity, measured in terms of requirements reuse, can be incorporated into that model to improve its applicability in software intensive systems projects.},
 duplicado = {false},
 inserir = {false},
 title = {Accounting for Product Similarity in Software Project Duration Estimation},
 year = {2016}
}

@article{762,
 abstract = {Abstract: In the implementation of the software project, the realizing of user requirement can perfect the software functionalities and personalize software customization, and this will further satisfy users' real needs. During this process, the key is quality assurance. In this paper, we propose a software test technique for the whole life cycle. This article proposes a set of whole business life cycle test method from the process, management, and technology. This method contains the testing process of the whole business life cycle test, agile cooperative mode, reusable test assets, and performance monitoring and so on. We also show the practice results of Campus E-card project in Northeastern University.},
 duplicado = {false},
 inserir = {false},
 title = {Research and practice on the whole life cycle test process model},
 year = {2016}
}

@article{764,
 abstract = {Abstract: Reference architectures have emerged as a special type of software architecture that achieves well-recognized understanding of specific domains, promoting reuse of design expertise and facilitating the development, standardization, and evolution of software systems. Because of their advantages, several reference architectures have been proposed and have been also successfully used, including in the industry. However, the most of these architectures are still built using an ad-hoc approach, lacking of a systematization to their construction. If existing, these approaches could motivate and promote the building of new architectures and also support evolution of existing ones. In this scenario, the main contribution of this paper is to present the evolution of ProSA-RA, a process that systematizes the design, representation, and evaluation of reference architectures. ProSA-RA has been already applied in the establishment of reference architectures for different domains and this experience was used to evolve our process. In this paper, we illustrate an application of ProSA-RA in the robotics domain. Results achieved through the use of ProSA-RA have showed us that it is a viable, efficient process and, as a consequence, it could contribute to the reuse of knowledge in several applications domains, by promoting the establishment of new reference architectures.},
 duplicado = {false},
 inserir = {false},
 title = {Consolidating a Process for the Design, Representation, and Evaluation of Reference Architectures},
 year = {2014}
}

@article{765,
 abstract = {This chapter first discusses the definitions given to reference architectures; Then, It clarifies the differences between the reference architecture and reference model, as well as between the reference architecture and product line architecture (i.e. the architecture of software product line). Following this, It presents a model that describes the set of elements that could be found in reference architectures. The chapter presents a process for aiming at systematizing reference architecture engineering. The process, named ProSA-RA2PLA, is an iterative process that systematizes the steps to build product line architectures using the knowledge and elements contained in the reference architectures. Various perspectives of applying these architectures are also discussed. The chapter ends with examples of these architectures and future perspectives for them.},
 duplicado = {false},
 inserir = {false},
 title = {Reference Architectures},
 year = {1998}
}

@article{767,
 abstract = {Abstract: Producing high quality software systems has been one of the most important software development concerns. In this perspective, Software Architecture and Software Testing are two important research areas that have contributed in that direction. The attention given to the software architecture has played a significant role in determining the success of software systems. Otherwise, software testing has been recognized as a fundamental activity for assuring the software quality; however, it is an expensive, error-prone, and time consuming activity. For this reason, a diversity of testing tools and environments has been developed; however, they have been almost always designed without an adequate attention to their evolution, maintenance, reuse, and mainly to their architectures. Thus, this paper presents our main contributions to systematize the development of testing tools and environments, aiming at improving their quality, reuse, and productivity. In particular, we have addressed architectures for software testing tools and environments and have also developed and made available testing tools. We also state perspectives of research in this area, including open research issues that must be treated, considering the unquestionable relevance of testing automation to the testing activity.},
 duplicado = {false},
 inserir = {false},
 title = {Contributions and Perspectives in Architectures of Software Testing Environments},
 year = {2011}
}

@article{768,
 abstract = {Organizational Information Systems (IS) collect, store, and manage personal and business information through web applications and services. Due to regulation laws and to protect the privacy of users, clients, and business partners, such information must be kept private. This paper proposes a privacy reference architecture that can serve as foundation for the analysis, design and development of web applications with privacy concerns. Using the proposed reference architecture, these applications can manage personal information in a more secure manner, protecting such information from different sources of privacy violation. Also, it can be used as a standardization model that facilitates system integration and communication. The architecture was evaluated regarding four key quality attributes: completeness, applicability, usability and feasibility. Results show that it brings values for the stakeholders and is an important tool in the analysis and implementation of applications with privacy protection.},
 duplicado = {false},
 inserir = {false},
 title = {Requirements, design and evaluation of a privacy reference architecture for web applications and services},
 year = {2015}
}

@article{769,
 abstract = {Abstract. During the execution of software projects, it is necessary to collect, store and analyze data to support project and organizational decisions. Software measurement is a fundamental practice for project management and process improvement. It is present in the main models and standards that address software process improvement, such as ISO/IEC 12207, CMMI and MR MPS.BR. In order to effectively perform software measurement, it is necessary an infrastructure to support data collection, storage and analysis. This infrastructure can be defined by means of an architecture, which describes the components necessary to support software measurement. In this paper we present the main results obtained from a systematic mapping study that investigated software measurement architectures and an approach proposed aiming to help organizations define software measurement architectures.},
 duplicado = {false},
 inserir = {false},
 title = {A Levels-based Approach for Defining Software Measurement Architectures},
 year = {2014}
}

@article{770,
 abstract = {Abstract: Reliability-aware software architecture design has recently been gaining growing attention among software architects. This chapter tackles the issue by proposing an ontology-based, reliability-aware software architecture design and evaluation approach, called OntoArch, which incorporates quantitative reliability evaluation in software architecture design by the means of the OntoArch ontology and the OntoArch tool. The OntoArch approach is characterized by: (1) integration of software reliability engineering and software architecture design; (2) proposing a reliability-aware software architecture design process model; (3) developing the OntoArch ontology in the context of software architecture design and software reliability engineering; and (4) the OntoArch tool not only enabling software architects to design architectures and model reliabilities, but also functioning as a knowledge management platform relying on reliability-aware software architecture design. The OntoArch approach is validated for a software architecture design; for example, Personal Information Repository (PIR), with the use cases of OntoArch-based software architecture knowledge management, software reliability profiling, and software architecture modeling and evaluation.},
 duplicado = {false},
 inserir = {false},
 title = {OntoArch Reliability-Aware Software Architecture Design and Experience},
 year = {2011}
}

@article{771,
 abstract = {During the execution of software projects, it is necessary to collect, store and analyze data in order to support project and organizational making decisions. Software measurement is a fundamental practice for project management and process improvement. It is present in the main models and standards that address software process improvement, such as ISO/IEC 12207 (ISO/IEC, 2008), CMMI (Capability Maturity Model Integration) (SEI, 2010) and MR-MPS-SW (Reference Model for Process Improvement of Brazilian Software) (SOFTEX, 2012). In models that address software process improvement in levels, such as CMMI and MR-MPS-SW, measurement starts in the initial levels (CMMI level 2 and MR-MPS-SW level F) and evolves as the maturity level increases. In the high maturity, characterized in CMMI levels 4 and 5 and MR-MPS-SW level B and A, measurement includes carrying out statistic process control (SPC). In order to perform the measurement process in an effective way, a computational infrastructure able to support measurement collection, storage and analysis is necessary. In this sense, this work proposes a reference architecture for software measurement that considers both traditional and high maturity measurement. The proposed architecture is platform independent and has been defined based on the Reference Ontology for Software Measurement (BARCELLOS, 2009). It can be used as a basis for defining specific architectures for computational solutions that support the software measurement process. As a proof of concept, the reference architecture was used as a basis for a specific architecture and a tool. In addition, aiming at evaluating the reference architecture proposed, an experimental study was carried out and some preliminary results were obtained},
 duplicado = {false},
 inserir = {false},
 title = {Uma Arquitetura de Referencia para Medicao de Software},
 year = {2013}
}

@article{772,
 abstract = {Embedded systems, especially in consumer electronics, are becoming increasingly complex, requiring the use of new technologies and approaches for their development. In particular, the development of interactive applications for digital TV requires new programming techniques and software engineering practices in order to facilitate the reliability and maintenance of these systems. In a parallel perspective, reference architectures, an special type of software architecture, have been proposed for several applications domain and have effectively contributed to the development, standardization, and evolution of software systems on such domains. However, the use of reference architectures has not been in depth explored in the digital TV domain. In this context, this work proposes a reference architecture for this domain. This architecture enables the development of applications for the procedural middleware environment for the digital TV receiver. The main result achieved in this work is the contribution to the development of interactive TV middleware-based applications, aiming at promoting this area that has been substantially explored in recent years},
 duplicado = {false},
 inserir = {false},
 title = {Estabelecimento de uma arquitetura de referencia para aplicacoes de televisao digital},
 year = {2012}
}

@article{774,
 abstract = {Abstract: Software Reference Architecture (SRA), which is a generic architecture solution for a specific type of software systems, provides foundation for the design of concrete architectures in terms of architecture design guidelines and architecture elements. The complexity and size of certain types of software systems need customized and systematic SRA design and evaluation methods. In this paper, we present a software Reference Architecture Design process Framework (RADeF) that can be used for analysis, design and evaluation of the SRA for provisioning of Tools as a Service as part of a cloud-enabled workSPACE (TSPACE). The framework is based on the state of the art results from literature and our experiences with designing software architectures for cloud-based systems. We have applied RADeF SRA design two types of TSPACE: software architecting TSPACE and software implementation TSPACE. The presented framework emphasizes on keeping the conceptual meta-model of the domain under investigation at the core of SRA design strategy and use it as a guiding tool for design, evaluation, implementation and evolution of the SRA. The framework also emphasizes to consider the nature of the tools to be provisioned and underlying cloud platforms to be used while designing SRA. The framework recommends adoption of the multi-faceted approach for evaluation of SRA and quantifiable measurement scheme to evaluate quality of the SRA. We foresee that RADeF can facilitate software architects and researchers during design, application and evaluation of a SRA and its instantiations into concrete software systems.},
 duplicado = {false},
 inserir = {false},
 title = {A Process Framework for Designing Software Reference Architectures for Providing Tools as a Service},
 year = {2016}
}

@article{776,
 abstract = {In automated test generation, granting control to test experts over test selection can enhance quality of generated test suites. However, in many cases test experts control is limited and they can not define custom coverage criteria. This work proposes a general framework for application of knowledge engineering to software testing, which facilitates specification of custom coverage criteria and provides means for generating test cases from different artifacts in dissimilar domains.},
 duplicado = {false},
 inserir = {true},
 title = {An Ontology-based Software Test Generation Framework},
 year = {2010}
}

@article{778,
 abstract = {Software testing is a technique used to provide information about the quality of a software operating in a specific context. The test responsible for assessing the performance and efficiency of a software in a usage scenario is known as performance testing. The development of a performance test is a task that requires testers with expertise related to the tools, activities and metrics of the domain. To represent this knowledge, this work proposes an ontology on the domain of performance testing. Ontology is a knowledge representation technique considered state of the art within the Artificial Intelligence. Moreover, one contribution of this research is to identify, based on the analysis of related works, what is known about the use of ontologies in software testing. Finally, the proposed ontology is evaluated by domain experts, compared with related ontologies and explored in applications designed to help testers regarding the planning and elaboration of performance tests.},
 duplicado = {false},
 inserir = {false},
 title = {Ontologia para teste de desempenho de software},
 year = {2013}
}

@article{779,
 abstract = {Abstract: Context: Quality assurance effort, especially testing effort, is frequently a major cost factor during software development. Consequently, one major goal is often to reduce testing effort. One promising way to improve the effectiveness and efficiency of software quality assurance is the use of data from early defect detection activities to provide a software testing focus. Studies indicate that using a combination of early defect data and other product data to focus testing activities outperforms the use of other product data only. One of the key challenges is that the use of data from early defect detection activities (such as inspections) to focus testing requires a thorough understanding of the relationships between these early defect detection activities and testing. An aggravating factor is that these relationships are highly context-specific and need to be evaluated for concrete environments. Objective: The underlying goal of this paper is to help companies get a better understanding of these relationships for their own environment, and to provide them with a methodology for finding relationships in their own environments. Method: This article compares three different strategies for evaluating assumed relationships between inspections and testing. We compare a confidence counter, different quality classes, and the F-measure including precision and recall. Results: One result of this case-study-based comparison is that evaluations based on the aggregated F-measures are more suitable for industry environments than evaluations based on a confidence counter. Moreover, they provide more detailed insights about the validity of the relationships. Conclusion: We have confirmed that inspection results are suitable data for controlling testing activities. Evaluated knowledge about relationships between inspections and testing can be used in the integrated inspection and testing approach In2Test to focus testing activities. Product data can be used in addition. However, the assumptions have to be evaluated in each new context.},
 duplicado = {false},
 inserir = {false},
 title = {Analyzing the relationships between inspections and testing to provide a software testing focus},
 year = {2014}
}

@article{780,
 abstract = {Abstract: Testing of a software system is resource-consuming activity. One of the promising ways to improve the efficiency of the software testing process is to use ontologies for testing. This paper presents an approach to test case generation based on the use of an ontology and inference rules. The ontology represents requirements from a software requirements specification, and additional knowledge about components of the software system under development. The inference rules describe strategies for deriving test cases from the ontology. The inference rules are constructed based on the examination of the existing test documentation and acquisition of knowledge from experienced software testers. The inference rules are implemented in Prolog and applied to the ontology that is translated from OWL functional-style syntax to Prolog syntax. The first experiments with the implementation showed that it was possible to generate test cases with the same level of detail as the existing, manually produced, test cases.},
 duplicado = {false},
 inserir = {false},
 title = {Application of Inference Rules to a Software Requirements Ontology to Generate Software Test Cases},
 year = {2016}
}

@article{782,
 abstract = {Context: Software testing is a knowledge intensive process and the use of Knowledge Management (KM) methods and principles makes software testing even more beneficial. Thus there is a need of adapting KM into software testing core process and attain the benefits that it provides in terms of cost, quality etc. There has been an extensive literature published in the context of KM in software testing. But it is still unclear about the importance of KM with respect to testing techniques as well as testing aspects i.e. each activity that takes part during testing and the outcomes that they result such as test artifacts is considered as testing aspect. Thus there is a requisite for studies to focus on identifying the challenges faced due to lack of KM along with the importance of KM with respect to testing aspects, testing techniques and thus can provide recommendations to apply Knowledge Management to those that get benefited from it. Objectives: In this thesis, we investigate the usage and implementation of KM in Software testing. The major objectives of current thesis include, To identify various software testing aspects that receive more attention while applying KM. To analyze the software testing techniques i.e. test design, test execution and test result analysis and evaluate them and highlight which of these have more involvement of KM. To identify the software testing techniques where tacit or explicit knowledge is currently used.
To gather challenges faced by industry due to lack of KM initiatives in software testing. Methods: We conducted a Systematic Literature Review (SLR) through a snowballing method based on the guidelines from Wohlin in order to identify various software testing aspects and testing techniques that have more involvement of KM and challenges that are faced due to lack of KM. A questionnaire intended for web-based survey was prepared from the gathered literature results to complement and further supplement them and to categorize the testing techniques based on the type of knowledge they utilize. The studies were analyzed in relation to their rigor and relevance to assess the quality of the results. The data obtained from survey were statistically analyzed using descriptive statistics and Chi-square test of significance. Results: We identified 35 peer reviewed papers among which 31 were primary and 4 were secondary studies. The literature review results indicated 9 testing aspects being in focus when applying KM within various adaptation contexts. In addition, few testing techniques were found to get benefited from the application of KM. Several challenges were identified from the literature review such as improper selection and application of better suited techniques, low reuse rate of Software Testing knowledge, barriers in Software testing knowledge transfer, impossible to quickly achieve the most optimum distribution of human resources during testing etc. 54 full answers were received to the survey. The survey showed that Knowledge Management was being applied in software testing in most of the industries. It was observed that test result analysis, test case design, test planning and testing techniques stood out as the most important testing aspects being focused while KM is applied. Regarding software testing techniques, 17 test design techniques, 5 test execution techniques and 5 test result analysis techniques gain more attention in the context of KM. Moreover, the results suggest that tacit knowledge was utilized for most of these techniques. Several new challenges are obtained from the survey such as lacking quality in terms of testing results or outcomes, difficulties in finding relevant information and resources during testing, applying more effort than required during testing, having a huge loss of know-how by neglecting explicit and tacit knowledge during test design etc. Conclusions. To conclude, various challenges are being faced due to the lack of KM. Our study also brings supporting evidence that applying KM in Software Testing is necessary i.e. to increase test effectiveness, selection and application of better suited techniques and so on. It was also observed that perceptions vary between the literature and the survey results obtained from the practitioners regarding testing aspects and testing techniques, as few aspects and techniques which are being categorized as the most important in the literature are not given the same priority by the respondents. Thus the final list of testing aspects and testing techniques is provided and empirical findings can likewise help practitioners to specifically apply KM more for those that are very much in need of it. Besides, it was found that most of the techniques require and utilize tacit knowledge to apply them and techniques such as shadowing, observing, training and recording sessions can help to store tacit knowledge for those that are in need of it. Thus researchers can recognize the advantages from this thesis and can further extend to various software life cycle models.},
 duplicado = {false},
 inserir = {false},
 title = {Knowledge Management in Software Testing},
 year = {2009}
}

@article{783,
 abstract = {Abstract:
Testing-as-a-service (TaaS) is a new model to provide testing capabilities to end users. Users save the cost of complicated maintenance and upgrade effort, and service providers can upgrade their services without impact on the end-users. Due to uneven volumes of concurrent requests, it is important to address the elasticity of TaaS platform in a cloud environment. Scheduling and dispatching algorithms are developed to improve the utilization of computing resources. We develop a prototype of TaaS over cloud, and evaluate the scalability of the platform by increasing the test task load, analyze the distribution of computing time on test task scheduling and test task processing over the cloud, and examine the performance of proposed algorithms by comparing others},
 duplicado = {false},
 inserir = {false},
 title = {Testing as a Service over Cloud},
 year = {2010}
}

@article{784,
 abstract = {The Service-Oriented Computing (SOC) paradigm is allowing computer systems to interact with each other in new
ways. According to the literature, SOC allows composition of distributed applications free from their platform and thus
reduces the cost of such compositions and makes them easier and faster to develop. Currently web services are the most
widely accepted service technology due to the level of autonomy and platform-independency they provide. However,
web services also bring challenges. For example, testing web services at the client side is not as straightforward as
testing traditional software due to the complex nature of web services and the absence of source code. This paper
surveys the previous work undertaken on web service testing, showing the strengths and weaknesses of current web
service testing strategies and identifying issues for future work.},
 duplicado = {false},
 inserir = {false},
 title = {Testing Web Services: A Survey},
 year = {2010}
}

@article{785,
 abstract = {Service-oriented architecture (SOA) is gaining momentum as an emerging distributed system architecture for business-to-business collaborations. This momentum can be observed in both industry and academic research. SOA presents new challenges and opportunities for testing and verification, leading to an upsurge in research. This paper surveys the previous work undertaken on testing and verification of service-centric systems, which in total are 177 papers, showing the strengths and weaknesses of current strategies and testing tools and identifying issues for future work. },
 duplicado = {false},
 inserir = {false},
 title = {Testing and verification in service-oriented architecture: a survey},
 year = {2013}
}

@article{786,
 abstract = {Abstract:
Semantic modeling for the Internet of Things has become fundamental to resolve the problem of interoperability given the distributed and heterogeneous nature of the "Things". Most of the current research has primarily focused on devices and resources modeling while paid less attention on access and utilisation of the information generated by the things. The idea that things are able to expose standard service interfaces coincides with the service oriented computing and more importantly, represents a scalable means for business services and applications that need context awareness and intelligence to access and consume the physical world information. We present the design of a comprehensive description ontology for knowledge representation in the domain of Internet of Things and discuss how it can be used to support tasks such as service discovery, testing and dynamic composition.},
 duplicado = {false},
 inserir = {false},
 title = {A Comprehensive Ontology for Knowledge Representation in the Internet of Things},
 year = {2012}
}

@article{788,
 abstract = {Abstract:
As cloud services proliferate, it becomes difficult to facilitate service composition and testing in clouds. In traditional service-oriented computing, service composition and testing are carried out independently. This paper proposes a new approach to manage services on the cloud so that it can facilitate service composition and testing. The paper uses service implementation selection to facilitate service composition similar to Google's Guice and Spring tools, and apply the group testing technique to identify the oracle, and use the established oracle to perform continuous testing for new services or compositions. The paper extends the existing concept of template based service composition and focus on testing the same workflow of service composition. In addition, all these testing processes can be executed in parallel, and the paper illustrates how to apply service-level MapReduce technique to accelerate the testing process.},
 duplicado = {false},
 inserir = {false},
 title = {An Approach for Service Composition and Testing for Cloud Computing},
 year = {2011}
}

@article{789,
 abstract = {Software-as-as-Service (SaaS) is a new approach for developing software, and it is characterized by its multi-tenancy architecture and its ability to provide flexible customization to individual tenant. However, the multi-tenancy architecture and customization requirements have brought up new issues in software, such as database design, database partition, scalability, recovery, and continuous testing. This paper proposes a hybrid test database design to support SaaS customization with two-layer database partitioning. The database is further extended with a new built-in redundancy with ontology so that the SaaS can recover from ontology, data or meta-data failures. Furthermore, constraints in metadata can be used either as test cases or policies to support SaaS continuous testing and policy enforcement.},
 duplicado = {false},
 inserir = {true},
 title = {Towards a scalable and robust multi-tenancy SaaS},
 year = {2010}
}

@article{790,
 abstract = {The Internetware is a new initiative to develop software on the web for web applications. The open and dynamic nature of Internet applications suggest new ways of thinking will be needed for this initiative. This paper discusses several important issues in Internetware and put forward to some relevant research directions. The relevant issues include lifecycle models, ontology and context systems, modeling and simulation, social networking, and adaptive control.},
 duplicado = {false},
 inserir = {false},
 title = {Internetware computing: issues and perspective},
 year = {2009}
}

@article{791,
 abstract = {Abstract:
Increasingly, service-based applications (SBAs) are composed of third-party services available over the Internet. Even if third-party services have shown to work during design-time, they might fail during the operation of the SBA due to changes in their implementation, provisioning, or the communication infrastructure. As a consequence, SBAs need to dynamically adapt to such failures during run-time to ensure that they maintain their expected functionality and quality. Ideally the need for an adaptation is proactively identified, i.e., failures are predicted before they can lead to consequences such as costly compensation and roll-back activities. Currently, approaches to predict failures are based on monitoring. Due to its passive nature, however, monitoring might not cover all relevant service executions, which can diminish the ability to correctly predict failures. In this paper we demonstrate how online testing, as an active approach, can improve failure prediction by considering a broader range of service executions. Specifically, we introduce a framework and prototypical implementation that exploits synergies between monitoring, online testing and quality prediction. For online test selection and assessment we adapt usage-based testing strategies. We experimentally evaluate the strengths of our approach in predicting the need for an adaptation of an SBA},
 duplicado = {false},
 inserir = {false},
 title = {Usage-Based Online Testing for Proactive Adaptation of Service-Based Applications},
 year = {2011}
}

@article{792,
 abstract = {Abstract:
Generating realistic test data is a major problem for software testers. Realistic test data generation for certain input types is hard to automate and therefore laborious. We propose a novel automated solution to test data generation that exploits existing web services as sources of realistic test data. Our approach is capable of generating realistic test data and also generating data based on tester-specified constraints. In experimental analysis, our prototype tool achieved between 93% and 100% success rates in generating realistic data using service compositions while random test data generation achieved only between 2% and 34%.},
 duplicado = {false},
 inserir = {false},
 title = {Automatically generating realistic test input from web services},
 year = {2011}
}

@article{793,
 abstract = {Service Oriented Architecture (SOA) has become a major application development paradigm. As a basic unit of SOA applications, Web services significantly affect the quality of the applications constructed from them. In the context of SOA, the specification and implementation of Web services are completely separated. The lack of source code and the restricted control of Web services limit the testability of Web services, and make the oracle problem prominent. In this context, can one alleviate the test oracle problem, or effectively and efficiently test such Web services even without oracles? It is an important issue which has not been yet adequately addressed. To address the challenge of testing Web services, the authors propose a metamorphic relation-based approach to testing Web services without oracles. The proposed approach leverages so-called metamorphic relations to generate test cases and evaluate test results. To make the proposed approach practical and effective, the authors proposed a framework taking into account the unique features of SOA, and developed a prototype which partially automates the framework. Three case studies are conducted to validate the feasibility and effectiveness of the proposed approach. The work presented in the paper not only alleviates the test oracle problem of testing Web services, but also delivers an effective and efficient test technique without oracles.},
 duplicado = {false},
 inserir = {false},
 title = {A metamorphic relation-based approach to testing web services without oracles},
 year = {2012}
}

@article{794,
 abstract = {Abstract:
Service Oriented Architecture (SOA) has become a major application development paradigm. As a basic unit of SOA applications, Web services significantly affect the quality of the applications constructed from them. Since the development and consumption of Web services are completely separated under SOA environment, the consumers are normally provided with limited knowledge of the services and thus have little information about test oracles. The lack of source code and the restricted control of Web services limit the testability of Web services. To address the prominent oracle problem when testing Web services, we propose a metamorphic testing framework for Web services taking into account the unique features of SOA. We conduct a case study where the new metamorphic testing framework is employed to test a Web service that implements the electronic payment. The results of case study show the feasibility of the framework for web services, and also the efficiency of metamorphic testing. The work presented in the paper alleviates the test oracle problem when testing Web services under SOA.},
 duplicado = {false},
 inserir = {false},
 title = {Metamorphic Testing for Web Services: Framework and a Case Study},
 year = {2011}
}

@article{795,
 abstract = {Abstract:
Web services are widely known as the building blocks of typical service oriented applications. The performance of such an application system is mainly dependent on that of component web services. Thus the effective load testing of web services is of great importance to understand and improve the performance of a service oriented system. However, existing Web Service load testing tools ignore the real characteristics of the practical running environment of a web service, which leads to inaccurate test results. In this work, we present WS-TaaS, a load testing platform for web services, which enables load testing process to be as close as possible to the real running scenarios. In this way, we aim at providing testers with more accurate performance testing results than existing tools. WS-TaaS is developed on the basis of our existing Cloud PaaS platform: Service4All. First, we briefly introduce the functionalities and main components of Service4All. Second, we provide detailed analysis of the requirements of Web Service load testing and present the conceptual architecture and design of key components. Third, we present the implementation details of WS-TaaS on the basis of Service4All. Finally, we perform a set of experiments based on the testing of real web services, and the experiments illustrate that WS-TaaS can efficiently facilitate the whole process of Web Service load testing. Especially, comparing with existing testing tools, WS-TaaS can obtain more effective and accurate test results.},
 duplicado = {false},
 inserir = {false},
 title = {WS-TaaS: A Testing as a Service Platform for Web Service Load Testing},
 year = {2012}
}

@article{796,
 abstract = {Abstract
Semantic web services are gaining more attention as an important element of the emerging semantic web. Therefore, testing semantic web services is becoming a key concern as an essential quality assurance measure. The objective of this systematic literature review is to summarize the current state of the art of functional testing of semantic web services by providing answers to a set of research questions. The review follows a predefined procedure that involves automatically searching 5 well-known digital libraries. After applying the selection criteria to the results, a total of 34 studies were identified as relevant. Required information was extracted from the studies and summarized. Our systematic literature review identified some approaches available for deriving test cases from the specifications of semantic web services. However, many of the approaches are either not validated or the validation done lacks credibility. We believe that a substantial amount of work remains to be done to improve the current state of research in the area of testing semantic web services.},
 duplicado = {false},
 inserir = {false},
 title = {A systematic review on the functional testing of semantic web services},
 year = {2013}
}

@article{798,
 abstract = {Abstract:
Web services are the basic building blocks for the business which is different from web applications. Testing of web services is difficult and increases the cost due to the unavailability of source code. Researchers have, web services are tested based on the syntactic structure using Web Service Description Language (WSDL) for atomic web services. This paper proposes an automated testing framework for composite web services based on semantics where the domain knowledge of the web services is described using prote?ge? tool and the behaviour of the entire business operation flow for the composite web service is described by Ontology Web Language for services (OWL-S). Prioritization of test cases is performed based on various coverage criteria for composite web services. Series of experiments were conducted to assess the effectiveness of prioritization and empirical results shown that prioritization techniques perform well in detecting faults compared to traditional techniques.},
 duplicado = {false},
 inserir = {true},
 title = {Black box test case prioritization techniques for semantic based composite web services using OWL-S},
 year = {2011}
}

@article{799,
 abstract = {Abstract:
Web Services are the basic building blocks for every e-business applications now-a-days. They provide efficient reusability mechanism, thereby reducing the development time and cost. Mostly the source code of web services is unavailable to other developers who use these services. The manual effort spent by them in testing these web services is very large in order to increase the interoperability. Thus automated testing needs to be developed for testing these Web Services, which is possible by adding semantics to Web Service Description Language (WSDL). Also the test case reduction technique is very much required for regression testing. This paper generates test cases for Web Services using reduction techniques Pair-Wise Testing (PWT) and Orthogonal Array Testing (OAT) and compares the two techniques. The structure of Web Services is specified using UML diagrams. The pre and post conditions for the service rule are specified using Object Constraint Language (OCL). The framework developed by this paper transforms this into WSDL-S specifications. These specifications are parsed and transformed into structured DOM tree. Test data with different factors, levels and strengths are generated for PWT and OAT techniques. Test data set generated by this framework would satisfy the constraints of the WSDL. The test cases are then developed based on the data generated, documented in XML based test files called Web Service Test Specifications (WSTS) and executed. The number of test cases required by general testing, PWT, OAT are compared and the better testing technique for testing Web Services is determined.},
 duplicado = {false},
 inserir = {false},
 title = {A test case reduction method for semantic based web services},
 year = {2010}
}

@article{800,
 abstract = {Abstract:
Many composite workflow services utilize non-imperative XML technologies such as WSDL, XPath, XML schema, and XML messages. Regression testing should assure the services against regression faults that appear in both the workflows and these artifacts. In this paper, we propose a refinement-oriented level-exploration strategy and a multilevel coverage model that captures progressively the coverage of different types of artifacts by the test cases. We show that by using them, the test case prioritization techniques initialized on top of existing greedy-based test case prioritization strategy form a subsumption hierarchy such that a technique can produce more test suite permutations than a technique that subsumes it. Our experimental study of a model instance shows that a technique generally achieves a higher fault detection rate than a subsumed technique, which validates that the proposed hierarchy and model have the potential to improve the cost-effectiveness of test case prioritization techniques.},
 duplicado = {false},
 inserir = {false},
 title = {A Subsumption Hierarchy of Test Case Prioritization for Composite Services},
 year = {2014}
}

@article{801,
 abstract = {Abstract
Many web services not only communicate through XML-based messages, but also may dynamically modify their behaviors by applying different interpretations on XML messages through updating the associated XML Schemas or XML-based interface specifications. Such artifacts are usually complex, allowing XML-based messages conforming to these specifications structurally complex. Testing should cost-effectively cover all scenarios. Test case prioritization is a dimension of regression testing that assures a program from unintended modifications by reordering the test cases within a test suite. However, many existing test case prioritization techniques for regression testing treat test cases of different complexity generically. In this paper, the authors exploit the insights on the structural similarity of XML-based artifacts between test cases in both static and dynamic dimensions, and propose a family of test case prioritization techniques that selects pairs of test case without replacement in turn. To the best of their knowledge, it is the first test case prioritization proposal that selects test case pairs for prioritization. The authors validate their techniques by a suite of benchmarks. The empirical results show that when incorporating all dimensions, some members of our technique family can be more effective than conventional coverage-based techniques.},
 duplicado = {false},
 inserir = {false},
 title = {Test Pair Selection for Test Case Prioritization in Regression Testing for WS-BPEL Programs},
 year = {2013}
}

@article{802,
 abstract = {In this work, we face the problem of generating good quality test suites and test cases for web services. We present a framework to test web services based on their formal description, following a black-box approach and using Property-Based Testing.

Web services are a popular solution to integrate components when building a software system, or to allow communication between a system and third-party users, providing a flexible, reusable mechanism to access its functionalities. Testing of web services is a key activity: we need to verify their behaviour and ensure their quality as much as possible, as efficiently as possible.

By automatically deriving QuickCheck models from its WSDL description and its OCL semantic constraints, we enable generation and execution of great amounts of automatically generated test cases. Thus, we avoid the usual compromise between effort and cost, which too often leads to smaller and less exhaustive test suites than desirable.

To illustrate the advantages of our framework, we present an industrial case study: a distributed system which serves media contents customers' TV screens.},
 duplicado = {false},
 inserir = {false},
 title = {Turning web services descriptions into quickcheck models for automatic testing},
 year = {2013}
}

@article{803,
 abstract = {Abstract:
As Graphical User Interfaces (GUIs) have almost become ubiquitous for users to interacting with software system, GUI testing becomes an essential task. GUI testing, whose basic steps are test case generation and execution result validation, is a knowledge intensive process that requires both knowledge of GUI systems and testers' experience. In this paper, an ontology-based approach is proposed to make test case generation much effective by involving testers experience. The approach first establishes a GUI testing ontology by analysing source code with reverse engineering techniques. Next test case generation rules which used to generate test cases are extracted from testers experience. Then the proposed approach is evaluated. Finally conclusions are drawn and further research directions are speculated.},
 duplicado = {false},
 inserir = {true},
 title = {An Ontology-Based Approach for GUI Testing},
 year = {2009}
}

@article{804,
 abstract = {Abstract:
OWL-S is a standard for specifying the ontology of web services. It enables web services to be discovered, invoked, and composed automatically. However, OWSL-S web service is difficult to analyze and test as XML-based OWL-S description is hard to understand. In addition, OWL-S introduces various control constructs to compose multiple services and allows describing the preconditions and expected effects of the services, which further complicates the analysis and testing of web services. This paper proposes a flow graph-based test model for OWL-S web services. In particular, the model abstracts the structural test artifacts of OWL-S web services. It can facilitate testers to understand and analyze the control flow of OWL-S web services. Based on the test model, test paths can be derived to ensure the correctness of OWL-S web services.},
 duplicado = {false},
 inserir = {true},
 title = {A flow graph-based test model for OWL-S web services},
 year = {2011}
}

@article{805,
 abstract = {Abstract
In this chapter, we provide an overview of recently proposed approaches and tools for functional and structural testing of SOA services. Typically, these two classes of approaches have been considered separately. However, since they focus on different perspectives, they are generally non-conflicting and could be used in a complementary way. Accordingly, we make an attempt at such a combination, briefly showing the approach and some preliminary results of the experimentation. The combined approach provides encouraging results from the point of view of the achievements and the degree of automation obtained. A very important concern in designing and developing web services is security. In the chapter we also discuss the security testing challenges and the currently proposed solutions.},
 duplicado = {false},
 inserir = {false},
 title = {Approaches to Functional, Structural and Security SOA Testing},
 year = {2012}
}

@article{806,
 abstract = {Abstract:
In an end-to-end, regression testing framework employing a safe regression test selection (RTS) technique that uses control-flow graphs (CFGs) to model Web service interactions, service providers must share their CFGs to participate. However, CFGs are sensitive implementation details and service providers are unwilling to expose them especially across autonomous systems. In order to encourage participation in the proposed framework, several privacy-preserving techniques are employed designed to protect the sensitive information contained within CFGs while still maintaining the overall effectiveness of the approach. The privacy-preserving techniques protect the information contained within CFGs by sanitizing individual nodes and altering the shape of the CFG. A case study will be presented to help illuminate the framework and provide a measure of the overall effectiveness of the approach.},
 duplicado = {false},
 inserir = {false},
 title = {Employing Privacy-Preserving Techniques to Protect Control-Flow Graphs in a Decentralized, End-to-End Regression Test Selection Framework for Web Services},
 year = {2011}
}

@article{807,
 abstract = {Cloud computing has received significant attention recently as it is a new computing
infrastructure to enable rapid delivery of computing resources as a utility in
a dynamic, scalable, and visualized manner. SaaS (Software-as-a-Service) provide a
now paradigm in cloud computing, which goal is to provide an effective and intelligent
way to support end users on-demand requirements to computing resources,
including maturity levels of customizable, multi-tenancy and scalability. To meet
requirements of on-demand, my thesis discusses several critical research problems
and proposed solutions using real application scenarios:
Service providers receive multiple requests from customers, how to prioritize
those service requests to maximize the business values is one of the most important
issues in cloud. An innovative prioritization model is proposed, which uses different
types of information, including customer, service, environment and workflow
information to optimize the performance of the system. To provide on-demand
services, an accurate demand prediction and provision become critical for the successful
of the cloud computing. An effective demand prediction model is proposed,
and applied to a real mortgage application.
To support SaaS customization and fulfill the various functional and quality requirements
of individual tenants, a unified and innovative multi-layered customization
framework is proposed to support and manage the variability of SaaS applications.
To support scalable SaaS, a hybrid database design to support SaaS customization
with two-layer database partitioning is proposed. To support secure
SaaS, O-RBAC, an ontology based RBAC (Role based Access Control) model is
ii
used for Multi-Tenancy Architecture in clouds. To support a significant number of
tenants, an easy to use SaaS construction framework is proposed.
As a summary, this thesis discusses the most important research problems in
cloud computing, towards effective and intelligent SaaS. The research in this thesis is
critical to the development of cloud computing and provides fundamental solutions
to those problems.},
 duplicado = {false},
 inserir = {false},
 title = {Towards effective and intelligent multi-tenancy SaaS},
 year = {2011}
}

@article{809,
 abstract = {Semantic web services (SWSs), which integrate the concept of ontology to support incorporation of machine understandable semantics into web services, have drawn much attention in recent years. By introducing semantics into web services, the quality and robustness of web service discovery, selection, and invocation can be greatly improved. However, web ontology language for service (OWL-S), the most widely accepted standard used to describe SWS, is extremely complex and hard to understand. This makes OWL-S compositions difficult to analyse and test. In view of this, this paper proposes a data flow testing approach for web service compositions based on OWL-S. The data flow test artefacts introduced by OWL-S are identified and thoroughly analysed. A test model that considers a variety of control constructs and semantics of OWL-S is proposed to abstract these test artefacts. Based on the test model, test paths can be derived to effectively uncover defects caused by improper data handling and message exchanges of OWL-S compositions.},
 duplicado = {false},
 inserir = {true},
 title = {Data flow analysis and testing for OWL-S semantic web service compositions},
 year = {2013}
}

@article{810,
 abstract = {Abstract:
Many web services represent their artifacts in the semi-structural format. Such artifacts may or may not be structurally complex. Many existing test case prioritization techniques however treat test cases of different complexity generically. In this paper, we exploit the insights on the structural similarity of XML-based artifacts between test cases, and propose a family of test case prioritization techniques that iteratively selects test case pairs without replacement. The validation experiment shows that these techniques can be more cost-effective than the studied existing techniques in exposing faults.},
 duplicado = {false},
 inserir = {false},
 title = {Prioritizing Structurally Complex Test Pairs for Validating WS-BPEL Evolutions},
 year = {2013}
}

@article{811,
 abstract = {Abstract
Graphical User Interface (GUI) testing is a knowledgeintensive process. In this paper, ontology is introduced to generate usercentric GUI test cases. First, GUI and nonGUI components are captured by reverse engineering techniques. Next, relations among GUI components are analysed and a GUI ontology is constructed by representing all analysed results. Then, test case generation rules are defined and used to simplify test cases. After that, a case study is demonstrated on a general communication application, which shows that the proposed approach is technically feasible and ontology can facilitate GUI testing by utilising knowledge of GUI systems and experience of testers.},
 duplicado = {false},
 inserir = {true},
 title = {Using ontology to generate test cases for GUI testing},
 year = {2011}
}

@article{812,
 abstract = {Abstract:
Web Services are evolving all the time. Web Service testing has received significant attention in both functional and non-functional aspects. Robustness has been considered as a crucial feature of Web Service. How to test whether the service is robust enough under its evolution has become a hard problem. OWL-S (Web Ontology Language for Services) is an ontology-based semantic specification of services I/O and workflows. In this paper, we present a novel approach to generate robustness test data based on the ontology specified in OWL-S. Our approach firstly analyzes the OWL-S (and related OWL specifications) to extract the ontology definitions associated with I/O parameters of SUT (Service Under Test). It then derives the semantic constraints on class, property, and parameter dependency from such ontology definitions. At last, it generates robustness test data by applying data mutation operators based on such constraints. We choose a flight ticket selling service in the travel domain as a case study. The preliminary results show that our approach is feasible and effective.},
 duplicado = {false},
 inserir = {true},
 title = {Ontology-based Web Service robustness test generation},
 year = {2009}
}

@article{813,
 abstract = {Abstract:
Mutation analysis is widely employed to evaluate the effectiveness of various software testing techniques. In most situations, mutation operators are uniformly applied to the original programs, while the faults tend to be clustered in practice. This may result in the inappropriate simulation of faults, and thus cannot deliver the reliable evaluation results. To overcome this, we propose a distribution-aware mutation analysis technique and conducted empirical studies to investigate the impact of the mutation distribution on the effectiveness evaluation of testing techniques. As an illustration, we select the commonly practiced random testing technique and two versions of dynamic random testing techniques and apply them to testing Web services. Results of empirical studies suggest that the mutation distribution significantly affects the evaluation results. This observation further indicates that the effectiveness of testing techniques previously evaluated with the uniform mutation analysis needs further realignments.},
 duplicado = {false},
 inserir = {false},
 title = {Distribution-Aware Mutation Analysis},
 year = {2012}
}

@article{814,
 abstract = {In this paper, we present WS-TaaS, a Web services load testing platform built on a global platform PlanetLab. WS-TaaS enables load testing process to be simple, transparent, and as close as possible to the real running scenarios of the target services. First, we briefly introduce the base of WS-TaaS, Service4All. Second, we provide detailed analysis of the requirements of Web service load testing and present its conceptual architecture as well as algorithm design for improving resource utilization. Third, we present the implementation details of WS-TaaS. Finally, we perform the evaluation of WS-TaaS with a set of experiments based on the testing of real Web services, and the results illustrate that WS-TaaS can efficiently facilitate the whole process of Web service load testing. Especially, comparing with existing testing tools, WS-TaaS can obtain more effective and accurate test results. },
 duplicado = {false},
 inserir = {false},
 title = {Delivering Web service load testing as a service with a global cloud},
 year = {2015}
}

@article{816,
 abstract = {An important mechanism of Service Oriented Architecture is the service registry (or service broker). It allows interaction among providers and consumers, offering a point to access the services developed and published in the registry. In this dissertation we propose the development of a service broker to support the publication, search and categorization of Web services, particularly those related to software testing tools. A common limitation of service brokers refers to searching facilities since they are primarily syntactic and thus can bring results that are not well related with the user's interest. To tackle this problem a test ontology was adapted and incorporated into the broker with the aim of improving the likelihood of finding the correct service in searches and also to add semantic information to the registered services. A generic service oriented architecture for the software engineering domain is presented and instantiated to the software testing domain with the purpose of facilitating the understanding and implementation of the proposed service registry. We also present some examples of software testing tools published in the registry and an example of search and interaction with the JaBUTiWS testing service, previously published in the registry, which aims to support structural testing of components and services},
 duplicado = {false},
 inserir = {false},
 title = {Desenvolvimento e avaliacao de um registro de servicos de ferramentas de teste},
 year = {2010}
}

@article{817,
 abstract = {Abstract
Context

The current validation tests for nuclear software are routinely performed by random testing, which leads to uncertain test coverage. Moreover, validation tests should directly verify the systems compliance with the original user s needs. Unlike current model-based testing methods, which are generally based on requirements or design models, the proposed model is derived from the original user s needs in text through domain-specific ontology, and then used to generate validation tests systematically.

Objective

Our first goal is to develop an objective, repeatable, and efficient systematic validation test scheme that is effective for large systems, with analyzable test coverage. Our second goal is to provide a new model-based validation testing method that reflects the user's original safety needs.

Method

A model-based scenario test case generation for nuclear digital safety systems was designed. This was achieved by converting the scenarios described in natural language in a Safety Analysis Report (SAR) prepared by the power company for licensing review, to Unified Modeling Language (UML) sequence diagrams based on a proposed ontology of a related regulatory standard. Next, we extracted the initial environmental parameters and the described operational sequences. We then performed variations on these data to systematically generate a sufficient number of scenario test cases.

Results

Test coverage criteria, which are the equivalence partition coverage of initial environment, the condition coverage, the action coverage and the scenario coverage, were met using our method.

Conclusion

The proposed model-based scenario testing can provide improved testing coverage than random testing. A test suite based on user needs can be provided.

Highlights

? We provide a systematic scenario test case generation method for nuclear domain. ? Our test case generation concerns safety behavior of the system under postulated initial events. ? We propose an ontology-based method to convert safety report from a natural language form to a processable structure. ? A visual method to create the scenario testing case can be provided. ? A model-based testing can be produced due to the constructed sequence diagrams.},
 duplicado = {false},
 inserir = {true},
 title = {Systematic scenario test case generation for nuclear safety systems},
 year = {2013}
}

@article{818,
 abstract = {Abstract:
The vision of service-oriented computing has been largely developed on the fundamental principle of building systems by composing and orchestrating services in their control flow. Nowadays, software development is notably influenced by service-oriented architectures (SOAs), in which the quality of software systems is determined by the quality of the involved services and their actual composition. Despite the efforts on improving their individual quality, adding or replacing services in an evolving system can introduce failures, thus compromising the satisfaction of the system's functional and extra-functional requirements. These failures erode the trust in the SOA vision. Thus, a key issue for the industrial adoption of SOA is providing service providers, integrators, and consumers the means to build confidence that services behave according to the contracted quality conditions. In this paper we present a first version of PA SCA NI, a framework for specifying and executing test specifications for service-oriented systems. From a test specification, PA SCA NI generates a configuration of testing services compliant with the Service Component Architecture (SCA) specification, which can be composed to integrate different testing strategies, being these tests traceable in an automated way. Our evaluation results show the applicability of the framework and a substantial gain in the tester's effort for developing tests.},
 duplicado = {false},
 inserir = {false},
 title = {A Framework for Automated and Composable Testing of Component-Based Services},
 year = {2014}
}

@article{819,
 abstract = {Modern software often uses ontologies as its key component to store data and their relationships. This is different from using an ontology as a stand-alone tool for knowledge sharing and representation. The ontology component needs to work with other software components and needs to evolve as the software evolves. Ontology design has been a research topic for years; however, most of these studies focus on using ontologies as stand-alone applications. This paper studies ontology patterns that can be applied to design ontologies as an integral part of a service-oriented application. The paper first briefly reviews various ontology design issues including a brief survey of existing ontology design patterns. The paper then outlines general principles for using ontologies in software applications, including the needs to incorporate ontology design process as a part of software development processes, design ontologies as a component of an overall software architecture, and use ontologies to enhance software evolution and the role that ontologies can play in software validation. The paper then proposes some common ontology patterns that can be used to design ontologies in service-oriented applications. This is followed by examining two international projects, SENSEI and FCINT, where ontologies are used in service-oriented applications and several ontology design patterns are used.},
 duplicado = {false},
 inserir = {false},
 title = {Ontology patterns for service-oriented software development},
 year = {2013}
}

@article{820,
 abstract = {Abstract:
Open Software Architecture (OSA) has been a prevalent design principle for integrating large, complex software systems from components. In OSA, interface specifications provide standard representations of the exposed software functionalities and constraints. Using an industry OSA system, the paper investigates the potential to extract domain model from standard interface specifications and to automate testing following the model driven approach. It focuses on modeling of service external behavior from the syntax and semantics defined by OSA interface standards. The domain model can be translated into test cases, either encoded in XML or specific programming languages, by predefined mapping mechanisms. The generated test scripts can be further compiled with target interfaces and executed under control. In this way, the domain models and test cases can be reused throughout system integration and regression testing, and for testing diversified component implementations following the same interface standards.},
 duplicado = {false},
 inserir = {false},
 title = {Interface-Based Automated Testing for Open Software Architecture},
 year = {2011}
}

@article{821,
 abstract = {Web services are a very popular solution to integrate components when building a software system, or to allow communication between a system and third-party users, providing a flexible, reusable mechanism to access its functionalities.

To ensure these properties though, intensive testing of web services is a key activity: we need to verify their behaviour and ensure their quality as much as possible, as efficiently as possible. In practise, the compromise between effort and cost leads too often to smaller and less exhaustive testing than it would be desirable.

In this paper we present a framework to test web services based on their WSDL specification and certain constraints written in OCL, following a black-box approach and using property-based testing. This combination of strategies allows us to face the problem of generating good quality test suites and test cases by automatically deriving those from the web service formal description. To illustrate the use of our framework, we present an industrial case study: a distributed system which serves media contents to customers TV screens.},
 duplicado = {false},
 inserir = {false},
 title = {Automatic Generation of Test Models for Web Services Using WSDL and OCL},
 year = {2013}
}

@article{822,
 abstract = {Abstract:
In recent years, Service Oriented Architecture (SOA) has been increasingly adopted to develop applications in the context of Internet. To develop reliable SOA-based applications, an important issue is how to ensure the quality of Web services. In this paper, we propose a dynamic random testing (DRT) technique for Web services which is an improvement of the widely practiced random testing. We examine key issues when adapting DRT to the context of SOA and develop a prototype for such an adaptation. Empirical studies are reported where DRT is used to test two real-life Web services and mutation analysis is employed to measure the effectiveness. The experimental results show that DRT can save up to 24% test cases in terms of detecting the first seeded fault, and up to 21% test cases in terms of detecting all seeded faults, both with the cases of uniformed mutation analysis and distribution-aware mutation analysis, which refer to faults being seeded in an even or clustered way, respectively. The proposed DRT and the prototype provide an effective approach to testing Web Services.},
 duplicado = {false},
 inserir = {false},
 title = {Towards Dynamic Random Testing for Web Services},
 year = {2012}
}

@article{823,
 abstract = {Testing is a relevant activity for the development life-cycle of Safety Critical Embedded systems. In particular, much effort is spent for analysis and classification of test logs from SCADA subsystems, especially when failures occur. The human expertise is needful to understand the reasons of failures, for tracing back the errors, as well as to understand which requirements are affected by errors and which ones will be affected by eventual changes in the system design. Semantic techniques and full text search are used to support human experts for the analysis and classification of test logs, in order to speedup and improve the diagnosis phase. Moreover, retrieval of tests and requirements, which can be related to the current failure, is supported in order to allow the discovery of available alternatives and solutions for a better and faster investigation of the problem.},
 duplicado = {false},
 inserir = {false},
 title = {Semantic Support for Log Analysis of Safety-Critical Embedded Systems},
 year = {2014}
}

@article{824,
 abstract = {In today's ever increasing demand on loosely coupled systems, software providers need to produce solutions that are flexible enough to be configured at runtime, yet maintaining high quality. One way to address the quality of such systems is through testing. Software testing provides software providers and their clients with techniques to characterize the internal and external quality of their systems, by specifying test cases to check how systems react under different circumstances. In the context of web services the situation changes, software providers become service providers. In other words, they do not provide a physical software to their clients, instead they allow them to invoke remote code from their system. This change affects the way we look at testing itself. In the web service paradigm clients do not own services, instead they invoke them. Different approaches for testing web services have been reported in the literature, with different techniques and using different input information to generate test cases. Considering the topic of web services testing, as a subject for this systematic review is a challenging undertaking, mainly due to the extensive amount of literature in the subject. To narrow it, we have focused on one single particularity that affects dramatically the testing process. That is the specifications used to generate test cases. To this end, in this paper we present a systematic review of the specifications used for testing web services. The outcome of this study answers key questions involved in the advantages and disadvantages of the existing specifications as well as the potential improvements that could lead to more robust web services testing process.},
 duplicado = {false},
 inserir = {false},
 title = {Specifications for Web Services Testing: A Systematic Review},
 year = {2015}
}

@article{825,
 abstract = {Abstract: Testing is one of the primary methods for Web service quality control. Test automation is necessary to enhance test
productivity and quality while reducing test effort. Test data generation is a critical issue of automated testing. The paper proposes a
method called interface semantic contract (ISC) for modeling services exposed functionalities using ontology and rule language.
Algorithms are developed to generate input partitions and test data based on ISC. Case studies are exercised to illustrate the proposed
approach. The results show that compared with conventional random testing, the proposed approach can enhance test coverage by 50%
with the same number of test cases, and reduce test effort by 90% to reach the same test coverage},
 duplicado = {false},
 inserir = {false},
 title = {????????? Web ????????},
 year = {2007}
}

@article{826,
 abstract = {Validation tests in the current nuclear industry practice are typically performed in an ad hoc fashion. This study presents a systematic and objective method of generating validation test cases from a Safety Analysis Report (SAR). A domain-specific ontology was designed and used to mark up a SAR; relevant information was then extracted from the marked-up document for use in automatically generating validation test cases that satisfy the proposed test coverage criteria; namely, single parameter coverage, use case coverage, abnormal condition coverage, and scenario coverage. The novelty of this technique is its systematic rather than ad hoc test case generation from a SAR to achieve high test coverage.},
 duplicado = {false},
 inserir = {false},
 title = {Validation test case generation based on safety analysis ontology},
 year = {2012}
}

@article{827,
 abstract = {Abstract: Web services are the basic building blocks for the business which is different
from web applications. Testing of web services is difficult and increases the
cost due to the unavailability of source coder. In previous work, web services were
tested based on the syntactic structure using Web Service Description Language
(WSDL) for atomic web services. This paper proposes an automated testing framework
for composite web services based semantics, where the domain knowledge of
the web services is described by protege tool [13] and the behavior of the entire
business operation flow for the composite web service is provided by Ontology Web
Language for services (OWL-S)[6]. Prioritization of test cases is performed based
on various coverage criteria for composite web services. Series of experiments were
conducted to assess the effects of prioritization on the coverage values and benefits
of prioritization techniques were found.},
 duplicado = {false},
 inserir = {true},
 title = {TEST CASE GENERATION AND PRIORITIZATION FOR COMPOSITE WEB SERVICE BASED ON OWL-S},
 year = {2011}
}

@article{828,
 abstract = {Abstract:
Web service playing the vital role in today's business environment. Business work flows are implemented as composite web service. Web services are growing day by day. Identifying the reliable web service is a tedious task. One of the reliability metric is correctness of the service. In this paper functional correctness model is designed for composite web service using black box testing technique.},
 duplicado = {false},
 inserir = {false},
 title = {Correctness evaluation model for composite web service},
 year = {2011}
}

@article{831,
 abstract = {Generating realistic test data is a major problem for software testers. Realistic test data
generation for certain input types is hard to automate and therefore laborious. We propose a
novel automated solution to test data generation that exploits existing web services as
sources of realistic test data. Our approach is capable of generating realistic test data and
also generating data based on tester-specified constraints. In experimental analysis, our
prototype tool achieved between 93% and 100% success rates in generating realistic data
using service compositions while random test data generation achieved only between 2%
and 34%. },
 duplicado = {false},
 inserir = {false},
 title = {Generating Realistic Test Input Using Web Services},
 year = {2011}
}

@article{832,
 abstract = {Software testing is considered as an important activity to ensure the quality of software systems. To support such activity, a diversity of testing tools have been developed. However, most of them have been separately built and have usually their particular structures and architectures, which has hindered the integration and reuse of these tools. In this context, efforts have been employed in order to provide service-oriented testing tools, i.e., tools that are based on SOA (Service Oriented Architecture). In another perspective, reference architectures have played an important role in the development of software systems, since they contain information about how to develop systems for a particular application domain, contributing to the success of systems in that domain. Thereby, our main objective is to establish a service-oriented reference architecture, named RefTEST-SOA (Reference Architecture for Software Testing Tools based on SOA), which aggregates the knowledge and experience about how to organize service-oriented testing tools, also aiming at integration, scalability and reuse provided by SOA. To establish this architecture, we have used ProSA-RA, a process that provides guidelines to the design, representation and evaluation of reference architectures. Results achieved by a conducted case study indicate that RefTEST-SOA is a viable and reusable architecture for developing service-oriented testing tools},
 duplicado = {false},
 inserir = {false},
 title = {Estabelecimento de uma arquitetura de referencia orientada a servicos para ferramentas de teste de software},
 year = {2010}
}

@article{833,
 abstract = {RESTful Web services are a solution that has been widely used for Web 2.0 development and API publication because of the simple and easy understanding interface, allied to high productivity framework support. RESTful Web services, as all software systems, must be tested to achieve an acceptable quality level so that can be used with trust by other systems. This test discipline should be integrated with development, since the beginning of the project and applied throughout the software development cycle. At the graduation work of Filipe Borges (2009), a solution was proposed (REST-Unit) to automatically generate test drivers from models specified using U2TP (UML 2.0 Test Profile) pattern, to validate RESTful Web services behavior. In this project, based in REST-Unit, REST-Unit+ was developed, which goal is to extend test drivers automatic generation, proposing a solution to relate them with their test data, creating data pools and data partitions. Through this process, the generated tests become more complete and their execution is facilitated, because the accepted data types are already specified and documented. REST-Unit+ is a solution for generating test drivers and test data from an U2TP model. The model, when exported to XMI, allows the test code to be generated. A prototype was implemented to validate the algorithm, and it was applied throughout an example that demonstrates the complete usage of this solution. This prototype can generate a RESTful Web service test driver and test data from a model. The time spent on test case specification is compensated by the time saved with test code generation. Besides, it has as an advantage, the well documented UML test case model, and test data models (data pools, data partitions and instances) used in these test cases, and the bigger quality that is achieved working at a higher abstraction level.},
 duplicado = {false},
 inserir = {false},
 title = {Estendendo rest-unit : geracao baseada em U2TP de drivers e dados de teste para RESTful Web Services},
 year = {2010}
}

@article{834,
 abstract = {Software oriented computing aims at developing software by the composition of services. It promotes software reuse and the implementation of dynamic, flexible and low coupling applications. Services provide specific business functionalities and are provided as a black-box. The use of services is only possible if the developers of service applications (integrators) trust the third party services. Particularly, testing is one of the solutions to obtain confidence on third party software. However, testers can only use specification based testing techiniques due to unavailability of the source code. In this context, testers cannot use the benefits of combining specification and implementation-based testing techniques. This works aims at proposing an an approach to introduce the structural testing technique in the context of service-based applications, but without revealing the source code. The proposed approach promotes the development of testable services, which are services with high testability and exposes operations through a testing interface to support structural testing. Integrators can test testable services and get, without having access to the source code, a coverage analysis on structural criteria. Test metadata are also provided along with testable services to help integrators on creating more test cases to increase the coverage obtained. The proposed approach is also used to support monitoring activities. The approach is generic and an instantiation is presented to create testable services written in Java. Formal experiments and case studies were conduct to validate the proposed approach and the instantiation. The results provide evidences of the applicability and the benefits of the approach for both testing and monitoring activities when compared to only using the functional approach},
 duplicado = {false},
 inserir = {false},
 title = {Uso da tecnica de teste estrutural para o teste e monitoracao de servicos},
 year = {2012}
}

@article{835,
 abstract = {Abstract- Web services are the basic building blocks for
every e-business applications now s- a-day. They provide
efficient reusability mechanism, thereby reducing the
development time and cost. Mostly the source code of web
services is unavailable to other developers who use these
services. Thus automated testing needs to be developed for
testing these web services, which is possible by adding
semantics to web services description language (WSDL).
Also the test case reduction technique is very much
required for regression testing. Previous work generates
test cases for web services using case pri8oritization is
need to done in reducing the test effort for web services
using reducing techniques pair wise testing (PWT) and
orthogonal array testing (OAT).in this work test case
prioritization is need to done in reducing the testing effort
for regression testing. To implement priority base testing
than find test specification for web based data. After that
perform orthogonal testing, pair wise testing and compare
with strength generation.},
 duplicado = {false},
 inserir = {false},
 title = {Regression Testing of Web Services Using Parsing and Test case Prioritization Approach},
 year = {2015}
}

@article{837,
 abstract = {This paper presents a survey of SOA testing. SOA drives the software through web services.  Web services present important challenges to software testers. These challenges have led to much complex task for testing web services.  The present paper comprehensive survey of existing work. According to Papazoglou [1], SOC is a new computing paradigm that utilizes services as the lightweight constructs to support the development of rapid, low-cost and easy composition of distributed applications. This is a widely used definition of SOA.},
 duplicado = {false},
 inserir = {false},
 title = {Survey on Service Oriented Architecture Testing},
 year = {2014}
}

@article{838,
 abstract = {A robot's software ecosystem often comprises a set of heterogeneous software
components, acquiring, exchanging, fusioning, and deriving data to trigger a desired
behaviour, state, or action of the robot. Due to the nature of component
based development [1] [2], and component interaction respectively, an essential,
and often crucial part of robotic system development is frequent integration testing.
While unit tests already provide an established way of checking modelled
constraints on single components, tests including complete component based
(sub-)systems have not been widely studied in robotics so far. We have identi-
fied the following problem statements with respect to component based system
testing so far.},
 duplicado = {false},
 inserir = {false},
 title = {Iterative Model Driven Integration Checks of Component Based Robotic Systems},
 year = {2013}
}

@article{840,
 abstract = {Abstract Semantic Web Services are Web Services that are
semantically annotated in order to make the services machine
understandable, thus allowing service discovery, selection,
composition, and invocation to be done automatically or with
minimum human intervention. The Semantic Web services
research community has been focusing on how these
semantics can facilitate service discovery, selection,
composition, and invocation. As of late, there have been some
growing research interests in the area of Semantic Web
services testing. However, it is not stated how the semantic
annotations in the Web services description can help improve
testing and how different it is from testing normal Web
services. This paper discusses current ongoing research on
testing Semantic Web services and classifies how testing uses
the semantics of the Semantic Web services description. },
 duplicado = {false},
 inserir = {false},
 title = {The Role of Semantics in Testing Semantic Web Services},
 year = {2012}
}

@article{843,
 abstract = {Web Services provide efficient reusability mechanism, thereby reducing the development time and cost. Mostly the source code of web services is unavailable to other developers who use these services. The manual effort spent by them in testing these web services is very large in order to increase the interoperability. Thus, automated testing needs to be developed for testing these Web services. This paper reviews test cases for Web Services using reduction techniques Pair-Wise Testing (PWT) and Orthogonal Array Testing (OAT) and compares the two techniques with general method. The structure of Web Services is specified using UML diagrams. The pre and post conditions for the service rule are specified using Object Constraint Language (OCL). The framework transforms into WSDLS specifications. These specifications are parsed and transformed into structured DOM tree. Test data set generated by this framework would satisfy the constraints of the WSDL. The test cases are then developed based on the data generated, documented in XML based test files. The number of test cases required by general testing, PWT, OAT are compared and the better testing technique for testing Web Services is determined.},
 duplicado = {false},
 inserir = {false},
 title = {A comparative study of testing semantic web services},
 year = {2014}
}

@article{844,
 abstract = {Web services provides a distributed computing architecture, with an emerging way of service oriented architecture (SQA). Here service oriented architecture is an interface to both computer systems and web services. Which implements an interaction with each other in new and different ways. According to service oriented architecture it virtually provides a platform for web services to communicate with each other. As it was an easy way for communicating with both clients and services. Many organizations and companies are either evaluating themselves into an enterprise information architectures, or they are in the process of getting adopt to the web services technology. As web services are platform independent it is playing a major role in the enterprise environment, and currently web services are widely accepted by many companies and organizations. So commonly web services possess some challenges to the enterprise environment. As a part of it web service must be tested before publish into a service oriented architecture. It involves large number of test cases, test scenarios that takes more time and effort. Testing management is needed so that it should control the time effort and should reduce the complexity of web service in a large software system, also in a real time world. Automation testing faces these challenges and fixes these issues. Automation testing has an ability to handle the complexities which are experiencing by the web services in a current environment. This paper presents the automatic testing strategies of a web service and detect the problems between both manual and automation testing. Finally results shows the proper effective report on improving the visibility of testing process based on the web approach to enhance the critical communication among multiple testing groups.},
 duplicado = {false},
 inserir = {false},
 title = {Intelligent Framework for Web-Service Testing},
 year = {2015}
}

@article{846,
 abstract = {Semantic Web Services (SWS) that integrate the concept of ontology to support incorporation of machine understandable semantics into web services has drawn much attention in recent years. Among the specification languages of SWS, OWL-S (Web Ontology Language for Service) is the most widely accepted standard proposed by W3C. OWL-S can be used to describe SWS and its associated semantic information, to define the operations of the SWS, and to select and compose loosely coupled SWS into a high-level composite service. However, OWL-S builds on the RDF (Resource Description Framework), OWL (Web Ontology Language), and SWRL (Semantic Web Rule Language) and is extremely complex and hard to understand. This makes OWL-S web services difficult to analyze and test. In view of this, this thesis proposes a data flow testing approach for web service compositions based on OWL-S. The test artifacts introduced by OWL-S are thoroughly analyzed. A test model is proposed to abstract the test information of OWL-S web services. The test model can facilitate testers to understand and analyze the control flow structures as well as the data flow dependencies and interactions of OWL-S web services. Based on the test model, algorithms are presented to compute data definition-use pairs and to derive test paths so as ensure the correctness of OWL-S web services. In addition, a semi-automatic tool is also developed to support the construction of the test model and the testing of OWL-S web services.},
 duplicado = {false},
 inserir = {false},
 title = {A Data Flow Testing Approach for Semantic Web Service Compositions Based on OWL-S},
 year = {2010}
}

@article{847,
 abstract = {Web Services (WSs) are gaining increasing attention as programming components and so is their quality. WSs offer many benefits, like assured interoperability, and reusability. Conversely, they introduce a number of challenges as far as their quality is concerned, seen from the perspectives of two different stakeholders: (1) the developer/provider of WSs and (2) the consumer of WSs. Developers are usually concerned about the correctness of the WS's functionality which can be assessed by functional testing. Consumers of WSs are usually careful about the reliability of WSs they are depending on (in addition to other qualities). They need to know whether the WSs are available (i.e., up and running), accessible (i.e., they actually accept requests) while available and whether they successfully deliver responses for the incoming requests. Availability, Accessibility, and Successability of WSs are directly related to WS reliability. Assessing these three factors via testing is usually only feasible at late stages of the development life-cycle. If they can be predicted early during the development, they can provide valuable information that may positively influence the engineering of WSs with regards to their quality. In this thesis we focus on assessing the quality of WSs via testing and via prediction. Testing of WSs is addressed by an extensive systematic literature review that focuses on a special type of WSs, the semantic WSs. The main objective of the review is to capture the current state of the art of functional testing of semantic WSs and to identify possible approaches for deriving functional test cases from their requirement specifications. The review follows a predefined procedure that involves automatically searching 5 well-known digital libraries. After applying the selection criteria to the search results, a total of 34 studies were identified as relevant. Required information was extracted from the studies, synthesized and summarized. The results of the systematic literature review showed that it is possible to derive test cases from requirement specifications of semantic WSs based on the different testing approaches identified in the primary studies. In more than half of the identified approaches, test cases are derived from transformed specification models. Petri Nets (and its derivatives) is the mostly used transformation. To derive test cases, different techniques are applied to the specification models. Model checking is largely used for this purpose. Prediction of Availability, Accessibility, and Successability is addressed by a correlational study in which we focused on identifying possible relations between the quality attributes Availability, Accessibility, and Successability and other internal quality measures (e.g., cyclomatic complexity) that may allow building statistically significant predictive models for the three attributes. A total of 34 students interacted freely with 20 pre-selected WSs while internal and external quality measures are collected using a data collection framework designed and implemented specially for this purpose. The collected data are then analyzed using different statistical approaches. The correlational study conducted confirmed that it is possible to build statistically significant predictive models for Accessibility and Successability. A very large number of significant models was built using two different approaches, namely the binary logistic regression and the ordinal logistic regression. Many significant predictive models were selected out of the identified models based on special criteria that take into consideration the predictive power and the stability of the models. The selected models are validated using the bootstrap validation technique. The result of validation showed that only two models out of the selected models are well calibrated and expected to maintain their predictive power when applied to a future dataset. These two models are for predicting Accessibility based on the number of weighted methods (WM) and the number of lines of code (LOC) respectively. The approach and the findings presented in this work for building accurate predictive models for the WSs qualities Availability, Accessibility, and Successability may offer researchers and practitioners an opportunity to examine and build similar predictive models for other WSs qualities, thus allowing for early prediction of the targeted qualities and hence early adjustments during the development to satisfy any requirements imposed on the WSs with regards to the predicted qualities. Early prediction of WSs qualities may help leverage trust on the WSs and reduces development costs, hence increases their adoption.},
 duplicado = {false},
 inserir = {false},
 title = {On the quality of Web Services},
 year = {2005}
}

@article{853,
 abstract = {The widespread use of service-oriented architectures (SOAs) and Web services in commercial software requires the adoption of development techniques to ensure the quality of Web services. Testing techniques and tools concern quality and play a critical role in accomplishing quality of SOA based systems. Existing techniques and tools for traditional systems are not appropriate to these new systems, making the development of Web services testing techniques and tools required. This article presents new testing techniques to automatically generate a set of test cases and data for Web services. The techniques presented here explore data perturbation of Web services messages upon data types, integrity and consistency. To support these techniques, a tool (GenAutoWS) was developed and applied to real problems.},
 duplicado = {false},
 inserir = {false},
 title = {Improving data perturbation testing techniques for Web services},
 year = {2011}
}

@article{858,
 abstract = {Abstract:
Combinatorial testing is considered effective for finding software faults. It is also efficient, since it keeps the number of tests relatively small. However, there seems to be very little research that considers combinatorial testing as a testing approach for web services. They are commonly tested by injecting fault-causing data perturbations into the network. It may be worthwhile to see if combinatorial testing can complement existing perturbations with the benefits of combinatorial testing. The approach proposed in this paper is called combinatorial fault-based testing. This type of testing combines existing fault-based testing techniques, such as fault injection, with combinatorial testing to attempt to find faults of varying strength within a web service. Combinatorial fault-based testing uses fault injection and helps reduce the problem with combinatorial explosion by focusing solely on fault-based combinations. This raises the following research question: Is there a way to take advantage of the benefits of combinatorial testing for web services assuming that source code will not be available, while minimizing the possibility of a combinatorial explosion? Combinatorial fault-based testing looks very promising for answering this question. As a side effect, it could potentially offer a way to determine the maximum strength of interactions to test for web services.},
 duplicado = {false},
 inserir = {false},
 title = {Introducing fault-based combinatorial testing to web services},
 year = {2010}
}

@article{865,
 abstract = {Web Services (WSs) are gaining increasing attention as programming components and so is their quality. WSs offer many benefits, like assured interoperability, and reusability. Conversely, they introduce a number of challenges as far as their quality is concerned, seen from the perspectives of two different stakeholders: (1) the developer/provider of WSs and (2) the consumer of WSs. Developers are usually concerned about the correctness of the WS's functionality which can be assessed by functional testing. Consumers of WSs are usually careful about the reliability of WSs they are depending on (in addition to other qualities). They need to know whether the WSs are available (i.e., up and running), accessible (i.e., they actually accept requests) while available and whether they successfully deliver responses for the incoming requests. Availability, Accessibility, and Successability of WSs are directly related to WS reliability. Assessing these three factors via testing is usually only feasible at late stages of the development life-cycle. If they can be predicted early during the development, they can provide valuable information that may positively influence the engineering of WSs with regards to their quality. In this thesis we focus on assessing the quality of WSs via testing and via prediction. Testing of WSs is addressed by an extensive systematic literature review that focuses on a special type of WSs, the semantic WSs. The main objective of the review is to capture the current state of the art of functional testing of semantic WSs and to identify possible approaches for deriving functional test cases from their requirement specifications. The review follows a predefined procedure that involves automatically searching 5 well-known digital libraries. After applying the selection criteria to the search results, a total of 34 studies were identified as relevant. Required information was extracted from the studies, synthesized and summarized. The results of the systematic literature review showed that it is possible to derive test cases from requirement specifications of semantic WSs based on the different testing approaches identified in the primary studies. In more than half of the identified approaches, test cases are derived from transformed specification models. Petri Nets (and its derivatives) is the mostly used transformation. To derive test cases, different techniques are applied to the specification models. Model checking is largely used for this purpose. Prediction of Availability, Accessibility, and Successability is addressed by a correlational study in which we focused on identifying possible relations between the quality attributes Availability, Accessibility, and Successability and other internal quality measures (e.g., cyclomatic complexity) that may allow building statistically significant predictive models for the three attributes. A total of 34 students interacted freely with 20 pre-selected WSs while internal and external quality measures are collected using a data collection framework designed and implemented specially for this purpose. The collected data are then analyzed using different statistical approaches. The correlational study conducted confirmed that it is possible to build statistically significant predictive models for Accessibility and Successability. A very large number of significant models was built using two different approaches, namely the binary logistic regression and the ordinal logistic regression. Many significant predictive models were selected out of the identified models based on special criteria that take into consideration the predictive power and the stability of the models. The selected models are validated using the bootstrap validation technique. The result of validation showed that only two models out of the selected models are well calibrated and expected to maintain their predictive power when applied to a future dataset. These two models are for predicting Accessibility based on the number of weighted methods (WM) and the number of lines of code (LOC) respectively. The approach and the findings presented in this work for building accurate predictive models for the WSs qualities Availability, Accessibility, and Successability may offer researchers and practitioners an opportunity to examine and build similar predictive models for other WSs qualities, thus allowing for early prediction of the targeted qualities and hence early adjustments during the development to satisfy any requirements imposed on the WSs with regards to the predicted qualities. Early prediction of WSs qualities may help leverage trust on the WSs and reduces development costs, hence increases their adoption.},
 duplicado = {false},
 inserir = {false},
 title = {On the quality of Web Services.},
 year = {2005}
}

@article{866,
 abstract = {Abstract: Software test process improvement (STPI) approaches are frameworks that guide software development organizations to improve their software testing process. We have identified existing STPI approaches and their characteristics (such as completeness of development, availability of information and assessment instruments, and domain limitations of the approaches) using a systematic literature review (SLR). Furthermore, two selected approaches (TPI NEXT and TMMi) are evaluated with respect to their content and assessment results in industry. As a result of this study, we have identified 18 STPI approaches and their characteristics. A detailed comparison of the content of TPI NEXT and TMMi is done. We found that many of the STPI approaches do not provide sufficient information or the approaches do not include assessment instruments. This makes it difficult to apply many approaches in industry. Greater similarities were found between TPI NEXT and TMMi and fewer differences. We conclude that numerous STPI approaches are available but not all are generally applicable for industry. One major difference between available approaches is their model representation. Even though the applied approaches generally show strong similarities, differences in the assessment results arise due to their different model representations.},
 duplicado = {false},
 inserir = {false},
 title = {Software test process improvement approaches: A systematic literature review and an industrial case study},
 year = {2016}
}

@article{869,
 abstract = {Abstract: Producing high-quality software has been one of the greatest challenges of the development market in the last few years. The software testing is the essence of the quality assurance, but the implementation of this activity is still difficult due to may factors. This paper presents some factors that influence on the adoption of a testing process. Some of these factors were outlined in the literature, and others were selected empirically based on the work experience obtained in a large company. We present the results obtained in the experiments that compare the software testing performance when implemented considering a formal testing process and when carried out in an ad-hoc way.},
 duplicado = {false},
 inserir = {false},
 title = {Barriers to Implement Test Process in Small-Sized Companies},
 year = {2010}
}

@article{870,
 abstract = {Abstract. Software Testing is a process used to help in improving quality and efficiency of a software product. In order to make the testing process less time consuming and efficient Automated Testing Tools are being used. The correct choice of an automated testing tool is a critical success factor for the developed product to reach and maintain market leadership. In the current paper a model for selecting an automated functional and regression testing tool using the Fuzzy Analytical Hierarchy Process (FAHP) is presented. The FAHP is used to compare the Testing Tools. The means of the triangular Fuzzy numbers produced by the experts from different CMM level 5 organizations were successfully used in the pair-wise comparison matrix.},
 duplicado = {false},
 inserir = {false},
 title = {Multi-attribute Comparison of Automated Functional and Regression Testing Tools using Fuzzy AHP},
 year = {2009}
}

@article{871,
 abstract = {Context: Test Process Improvement (TPI) approaches are frameworks or models that guide software development organizations to investigate, assess and improve their software testing process. Objectives: We extend existing work in the area of Test Process Improvement by identifying available approaches and by evaluating them in regards to their characteristics. Furthermore, two selected approaches are evaluated with respect to their content and assessment results. Methods: In the first part of this study we use a systematic literature review to identify the existing TPI approaches which are then used in the second part of the study. The second part of the study is an industrial case study in which two TPI approaches are applied in an industrial setting. Results: We contribute in providing (1) a complete, in our opinion, list of 16 existing TPI approaches and their characteristics, (2) a detailed comparison of the content and the results of the two applied approaches (TPI Next and TMMi) and (3) experience in applying them in industry. As a result of this research we found that the content as well as the assessment results of the two approaches are similar to a great extent. Conclusions: Numerous Test Process Improvement approaches are available, but not all are generally applicable for industry. One major difference between available approaches is their model representation. Even though, the applied approaches generally show strong similarities, differences in the assessment results are noticeable due to their different model representations.},
 duplicado = {false},
 inserir = {false},
 title = {Evaluation of Test Process Improvement approaches: An industrial case study},
 year = {2015}
}

@article{872,
 abstract = {Computing is becoming increasingly critical in the embedded applications space and depending on the software, its malfunction may result in a severe financial loss to the loss of human life. Considering this scenario, we presented a systematic literature review in order to investigate the evolution of work-related activity test critical embedded software in order to evaluate the level of compliance found in the work to the standard DO- 178B (Software Considerations in Airborne Systems and Equipment Certification). This research, in addition to conducting a systematic review of publications about this issue, has resulted in the composition of primary studies to define a process of quality testing and including the requirements of DO-178B at their different levels of criticality.},
 duplicado = {false},
 inserir = {false},
 title = {Estudo e Definicao de uma Metodologia de Teste de Software no Contexto de Sistemas Embarcados Criticos},
 year = {2011}
}

@article{873,
 abstract = {Background. Test process requires constant follow-ups to evolve their methodologies. To support the improvement of processes, we have the Test Maturity Models such as TMM and TMMi. However, the TMMi model does not provide instruments or framweworks that allows companies to check the adherence about the model, it is necessary the expert advice. For shortage of certifiers in the country, the cost becomes high and hinders to obtainment certification for small businesses. Aim. Propose a methodology of low cost to assess of test process, which shall support in the improvement and quality of testing procedures employed in Small Software Companies Methodology. An exploratory and qualitative research, conducted by: (i) Research instruments or frameworks which are available to assess testing process relative to TMMi levels through Systematic Review; and (ii) abstraction of evidence of the studies, collaborating with test process assessments. From the defined contributions and mandatory guidelines TAMAR define an assessment process with the focus to meet the limitations of small companies. Results. An evaluation process covering Planning activities; Preparation; Application; Analysis of the results; and Closing. The assessment instrument provided a new approach in the presentation of the issues. The questions were given by affinity groups, focusing the respondent to a specific stage of the test process. Conclusion.
Perform routine to prepare those involved brought a brief overview of the maturity model and reducing uncertainty for indication of evidence in addition to the new approach to provision of the issues by affinity groups. The validation has shown that the instrument is simple and assistance provided throughout the process enables the evaluation of small business process.},
 duplicado = {false},
 inserir = {false},
 title = {Avaliacao de processos de Teste pelo Modelo de Maturidade TMMi em pequenas empresas},
 year = {2016}
}

@article{875,
 abstract = {With the advent of unmanned systems and the movement toward greater complexity, the test and evaluation community faces new challenges. This paper presents an ontology for the unmanned and autonomous systems of systems test and evaluation (UASoS T&E) domain that is intended to help support addressing these challenges in T&E. The development of a UASoS T&E ontology provides a means for those interested in the field of UASoS T&E to understand the entities, relationships, and terminology within the domain. The ontology provides a common language and basis of understanding for UASoS T&E that can be leveraged as a foundation for other efforts.},
 duplicado = {false},
 inserir = {true},
 title = {An Ontology for Unmanned and Autonomous Systems of Systems Test and Evaluation},
 year = {2010}
}

@article{876,
 abstract = {Abstract: Context: Software testing practices and processes in many companies are far from being mature and are usually conducted in ad-hoc fashions. Such immature practices lead to various negative outcomes, e.g., ineffectiveness of testing practices in detecting all the defects, and cost and schedule overruns of testing activities. To conduct test maturity assessment (TMA) and test process improvement (TPI) in a systematic manner, various TMA/TPI models and approaches have been proposed. Objective: It is important to identify the state-of-the-art and the practice in this area to consolidate the list of all various test maturity models proposed by practitioners and researchers, the drivers of TMA/TPI, the associated challenges and the benefits and results of TMA/TPI. Our article aims to benefit the readers (both practitioners and researchers) by providing the most comprehensive survey of the area, to this date, in assessing and improving the maturity of test processes. Method: To achieve the above objective, we have performed a Multivocal Literature Review (MLR) study to find out what we know about TMA/TPI. A MLR is a form of a Systematic Literature Review (SLR) which includes the grey literature (e.g., blog posts and white papers) in addition to the published (formal) literature (e.g., journal and conference papers). We searched the academic literature using the Google Scholar and the grey literature using the regular Google search engine. Results: Our MLR and its results are based on 181 sources, 51 (29%) of which were grey literature and 130 (71%) were formally published sources. By summarizing what we know about TMA/TPI, our review identified 58 different test maturity models and a large number of sources with varying degrees of empirical evidence on this topic. We also conducted qualitative analysis (coding) to synthesize the drivers, challenges and benefits of TMA/TPI from the primary sources. Conclusion: We show that current maturity models and techniques in TMA/TPI provides reasonable advice for industry and the research community. We suggest directions for follow-up work, e.g., using the findings of this MLR in industry-academia collaborative projects and empirical evaluation of models and techniques in the area of TMA/TPI as reported in this article.},
 duplicado = {false},
 inserir = {false},
 title = {Software test maturity assessment and test process improvement: A multivocal literature review},
 year = {2017}
}

@article{877,
 abstract = {Software process assessment (SPA) is the foundation step for software process improvement. ISO/IEC 15504 defines the term process assessment as "the systematic evaluation of an organization's processes against a process reference model (PRM)". In process assessment, there is a need to set and maintain a mapping between an organization's processes and a PRM, where process experts transform the gap between the two into opportunities for process improvement. To maintain such a mapping requires a continuous tracking and alignment between the organization's processes and the PRM(s). The use of ontologies might be a suitable solution to provide computerized tool support for SPA that becomes erroneous and time-consuming if done manually. With an aim to understand the use and usefulness of ontologies in SPA, in this study, we have performed a systematic literature review (SLR). We have searched the most known digital libraries and selected 14 studies out of 54 initially selected and 571 initially retrieved. We analyzed the selected studies with respect to a number of research questions that address; contribution facet, targeted software processes, research facet, process improvement model used, process assessment model used, ontology representation language, purpose of ontology use, qualitative and quantitative benefits reported, and challenges faced. As a result, we synthesized a conceptual model of ontology-based support in SPA. We hope the results of our work will be useful for researchers and practitioners to direct their future studies on the use of ontologies for SPA.},
 duplicado = {false},
 inserir = {false},
 title = {On the Use of Ontologies in Software Process Assessment: A Systematic Literature Review},
 year = {2017}
}

@article{882,
 abstract = {Abstract- Cloud computing not only changes the way of obtaining computing resources (such as computers, infrastructures, data storage, and application services), but also changes the way of managing and delivering computing services, technologies, and solutions. Cloud computing leads an opportunity in offering testing as a service (TaaS) for SaaS and clouds. Meanwhile, it causes new issues, challenges and needs in software testing, particular in testing clouds and cloud-based applications. This paper provides a comprehensive tutorial on cloud testing and cloud-based application testing. It answers the common questions raised by engineers and managers, and it provides clear concepts, discusses the special objectives, features, requirements, and needs in cloud testing. It offers a clear comparative view between web-based software testing and cloud-based application testing. In addition, it examines the major issues, challenges, and needs in testing cloud-based software applications. Furthermore, it also summarizes and compares different commercial products and solutions supporting cloud testing as services.},
 duplicado = {false},
 inserir = {false},
 title = {Cloud Testing- Issues, Challenges, Needs and Practice},
 year = {2011}
}

@article{884,
 abstract = {Abstract: Cloud computing leads an opportunity in offering testing as a service (TaaS) for SaaS, clouds, and cloud-based applications. This brings new business opportunities, challenges, and demands in innovative service models, testing techniques, QoS standards, and requirements. This paper provides a comprehensive tutorial on testing as a service in a cloud environment. It answers the common questions raised by engineers and managers, and provides clear conceptual discussions about testing as a service (TaaS), including its scope, objectives, motivations and values, distinct features, required techniques, as well as testing environments. It not only presents a classification of different types of testing services in TaaS, but also offers a clear comparative view and perspectives between conventional software testing service and cloud-based testing as a service. In addition, it examines underlying issues, challenges, and emergent needs.},
 duplicado = {false},
 inserir = {false},
 title = {Testing as a Service (TaaS) on Clouds},
 year = {2013}
}

@article{886,
 abstract = {Abstract: In testing Cloud environment testing tasks requested by different tenants have many uncertainties. The arriving time, deadline and the number of tasks are unknown in advance. Especially, the relationships between testing tasks and testing environments are very complex. How to efficiently manage these tasks is really a challenging problem. This paper studies the special features of testing tasks and presents a task management framework. We analyze the dependencies and conflicts associated with testing tasks and their related runtime environments, using rule matching mechanism to derive the relationships supported by domain knowledge. Based on these analyses, improved algorithms are introduced to cluster and dynamically schedule testing tasks to minimize the make-span or meet deadlines with the consideration of testing task resource requirements and Cloud resource utilization balance at the same time. A fault tolerance mechanism is built to cope with testing errors, whose results are studied to ameliorate clustering and scheduling algorithms. A suite of experiments compares the effectiveness of the proposed approach with other algorithms.},
 duplicado = {false},
 inserir = {false},
 title = {Testing Tasks Management in Testing Cloud Environment},
 year = {2011}
}

@article{888,
 abstract = {With application performance catalysing business growth, software testing assumes a very significant role in the growth of an enterprise. Over time, the software testing function has become a challenging activity for enterprises due to increasing technological complexities, software sourcing challenges, rising costs and security issues among others. Typically, software testing is done either internally using the infrastructure that exists within the organization, or then is outsourced to software services providers. At the IT service providers side, software testing underwent a long drawn evolution cycle. From ad-hoc practices within different business units, it gradually evolved to a centralized Managed Test Center approach, and finally towards institutionalising a Testing Center of Excellence (TCoE) within the organisation. This offered customers a dynamically scalable and economical framework which enabled them to outsource their testing requirements and avoid complex contracts, long start-up times, and high levels on investment. The final stage of the evolution cycle of testing has manifested in the form of Testing-as-a-Service (TaaS). Today, Testing-as-a-Service is being increasingly considered a viable testing model by many organizations to achieve reduced costs and improved service for their IT test requirements. To address this importance, We have designed a service offering - Testing as a Service, using Cloud computing. This paper focuses on how organizations can optimize their IT budget through a strategic initiative in the form of On Demand testing},
 duplicado = {false},
 inserir = {false},
 title = {STUDY OF TESTING AS A SERVICE (TAAS) COST EFFECTIVE FRAMEWORK FOR TAAS IN CLOUD ENVIRONMENT},
 year = {2013}
}

@article{889,
 abstract = {Testing becomes an important process not only in term of exposure but also in terms of performance, safety, usability. For software quality assurance, software testing is the basic activity. Locating the test environments is too expensive in terms of hardware, software licenses. Testing as service (TaaS) is new model to provide testing capabilities to end user. Users save the cost of complicated maintenance and upgrade effort, and service providers without impact on end users. Due to uneven volumes of concurrent request it is important to address the elasticity of TaaS platform in cloud environment. With the emergence of extreme programming, test-driven development and other agile methods, unit testing has become an important part of almost every development effort. At the same time, many applications provide application programming interfaces (APIs) to allow code-level access to the functionality. These APIs, just like any other interface into the product, must be tested for web services over cloud before they are released to the end-users. We develop a tool of TaaS over cloud, that toolkit shall provide a rich GUI to allow even a novice user to perform API testing with ease.},
 duplicado = {false},
 inserir = {false},
 title = {API TESTING TOOL IN CLOUD},
 year = {2013}
}

@article{894,
 abstract = {Abstract:
Web services (WS) enables agile application development by orchestrating the existing service components. However, the dynamically constructed service-based system has to be tested dynamically and automatically at runtime without human intervention. To address the challenges of automatic WS test case generation, this paper proposes a model driven ontology-based approach with the purpose of improving test formalism and test intelligence. The semantic WS specification OWL-S is used to describe the application logic of composite service process. A Petri-Net model is created to provide a formal representation of the OWL-S (Web Ontology Language for Web service) process model. The Petri-net ontology is defined to incorporate the operation and IOPE (inputs, outputs, preconditions, and effects) semantics for test generation. Test cases are generated from two aspects. Test steps are generated by traversing various execution paths of the Petri-net graph. Test data are generated by reasoning over the IOPE ontology},
 duplicado = {false},
 inserir = {true},
 title = {Ontology-Based Test Case Generation for Testing Web Services},
 year = {2007}
}

@article{897,
 abstract = {Abstract:
Web services (WS) is currently the major implementation of service-oriented architecture (SOA). It defines a framework for agile and flexible integration among autonomous services based on Internet open standards. However, testing has been a challenge due the dynamic and collaborative nature of WS. This paper introduces an on-going project on a multiagent based framework to coordinate distributed test agents to generate, plan, execute, monitor and communicate tests on WS. Test agents are classified into different roles which communicate through XML-based agent test protocols. Test master accepts test cases from test generator, generates test plans and distributed them to various test groups. A set of test agents that implement a test plan are organized into a test group, which is coordinated by a test coordinator. Test runners execute the test scripts, collect test results and forward the results to test analyzer for quality and reliability analysis. The status of the test agents are monitored by the test monitor. Test agents are dynamically created, deployed and organized. Through the monitoring and coordinating mechanism, the agents can re-adjust the test plan and their behavior at runtime to be adaptive to the changing environment},
 duplicado = {false},
 inserir = {false},
 title = {A multi-agent based framework for collaborative testing on Web services},
 year = {2006}
}

@article{898,
 abstract = {Abstract:
In recent years, Web applications have grown so quickly that they have already become crucial to the success of businesses. However, since they are built on Internet and open standard technologies, Web applications bring new challenges to researchers, such as dynamic behaviors, heterogeneous representations, novel control flow and data flow mechanisms, etc. In this paper, we propose an agent-based approach for Web application testing. While the agent-based framework greatly reduces the complexity of Web applications, a four-level dataflow test approach can be employed to perform structure testing on them. In this approach, data flow analysis is performed as function level testing, function cluster level testing, object level testing, and Web application level testing, from low abstract level to high abstract level. Each test agent in the framework takes charge of the testing in an abstract level for a particular type of Web document or object.},
 duplicado = {false},
 inserir = {false},
 title = {An agent-based testing approach for Web applications},
 year = {2005}
}

@article{899,
 abstract = {A growing interest on the establishment of ontologies has
been observed for the most different knowledge domains.
This work presents OntoTest an ontology of software testing,
which has been developed to support acquisition, organization,
reuse and sharing of testing knowledge. OntoTest
has been built with basis on ISO/IEC 12207 standard
and intends to explore the different aspects involved in the
testing activity. Specifically, we are interested in defining a
common well-established vocabulary for testing, which can
be useful to develop supporting tools as well as to increase
the inter-operability among them.},
 duplicado = {false},
 inserir = {true},
 title = {Towards the Establishment of an Ontology of Software Testing},
 year = {2006}
}

@article{900,
 abstract = {To address the challenges of dynamic, distributed and collaborative Web Services (WS) testing, this paper proposes a trustworthy service broker architecture and a dependence-based progressive group testing technique. The new architecture extends the traditional Universal Description, Discovery and Integration (UDDI) service broker by introducing the check-in and check-out testing interfaces, which are associated with a set of automated tools. The design objectives of the trustworthy service broker are to perform rigorous testing and evaluation effectively and to ensure that only quality WS can be registered and released to the public. A group testing technique is applied to reduce the test cost by using a hierarchy of increasingly sophisticated test scripts, which progressively rules out low quality WS. WS are further ranked based on the group testing results using ranking criteria. Experiments show that this approach can effectively test and choose WS in an open environment and in reduced number of tests.},
 duplicado = {false},
 inserir = {false},
 title = {Design of a trustworthy service broker and dependence-based progressive group testing},
 year = {2007}
}

@article{901,
 abstract = {Abstract:
Software testing in general and Web applications testing in particular are knowledge-driven, labor intensive activities, which are best performed by intelligent, autonomous agents. The proposed framework is based on the Belief-Desire-Intention (BDI) model of rational agents and the Unified Modeling Language (UML). We describe how Web applications testing can be modeled and reasoned using the framework.},
 duplicado = {false},
 inserir = {false},
 title = {An agent-based framework for testing Web applications},
 year = {2004}
}

@article{902,
 abstract = {Abstract:
Borrowing the thoughts of component interaction, the concept of Logic Component (LC) is proposed, and a Web application is divided into LCs which is mapped into the actual physical components finally. This paper supposes that each component and LC is a black box which has been well tested. So Web applications can be regarded as a set of interactive components. It mainly concentrates on testing the interactions of components. An automaton is used to model each component, and compositions of automata are used to model the component interaction. For each component-test- sequence, a new automaton can be got by using composition of automata. Abstract test cases can be generated from the new automaton. By means of mapping the actions to the actual operations and adding the data of test space to them, the component test cases are generated.},
 duplicado = {false},
 inserir = {false},
 title = {An Approach to Generating Test Cases for Testing Component-based Web Applications},
 year = {2007}
}

@article{903,
 abstract = {Abstract
This chapter introduces the concept of software growth environments to support sustainable long-term evolution of Web-based application systems. A multiagent prototype system is designed and implemented with emphasis on software testing. In this environment, software tools are agents that cooperate effectively with each other and human testers through communications at a high level of abstraction. New tools can be integrated into the system with maximal flexibility. These are achieved through the design and utilisation of an ontology of software testing that represents the knowledge of software engineering and codifies the knowledge for computer processing as the contents of an agent communication language. The ontology is represented in UML (Unified Modeling Language) at a high level of abstraction so that it can be validated by human experts. It is also codified in XML (Extensible Markup Language) for computer processing to achieve the required flexibility and extendibility.},
 duplicado = {false},
 inserir = {false},
 title = {Developing Software Testing Ontology in UML for a Software Growth Environment of Web-Based Applications},
 year = {2005}
}

@article{904,
 abstract = {In recent years, Web applications (WAs) have grown so quickly that they have already become crucial to the success of businesses. However, since they are built on Internet and open standard technologies, WAs possess their own unique features, such as dynamic behaviors, heterogeneous representations, and novel data handling mechanisms. These features provide concrete support to the success of WAs, but they bring new challenges to researchers and developers, especially in regard to testing WAs and ensuring their quality. Testing approaches for non-WAs have to be extended to handle these features before they are used in WA testing. This paper presents an agent-based approach to perform data-flow testing of WAs. More precisely, the data-flow testing will be performed by autonomous test agents at the method level, object level, and object cluster level, from low abstraction level to high abstraction level. In the process of the recommended data-flow testing, an agent-based WA testing system (WAT) will automatically generate and coordinate test agents to decompose the task of testing an entire WA into a set of subtasks that can be accomplished by test agents. The test agents, rooted in the Belief Desire Intention (BDI) model, cooperate with each other to complete the testing of a WA. An example is used to show the feasibility of the proposed approach.},
 duplicado = {false},
 inserir = {false},
 title = {An agent-based data-flow testing approach for Web applications},
 year = {2006}
}

@article{905,
 abstract = {Abstract:
Software testing is a very expensive and time consuming process. It can account for up to 50% of the total cost of the software development. Distributed systems make software testing a daunting task. The research described in this paper investigates a novel multi-agent framework for testing 3-tier distributed systems. This paper describes the framework architecture as well as the communication mechanism among agents in the architecture. Web-based application is examined as a case study to validate the proposed framework. The framework is considered as a step forward to automate testing for distributed systems in order to enhance their reliability within an acceptable range of cost and time},
 duplicado = {false},
 inserir = {false},
 title = {A Multi-Agent Framework for Testing Distributed Systems},
 year = {2006}
}

@article{906,
 abstract = {Abstract:
This paper presents a service oriented architecture for testing Web Services. In this architecture, various parties interoperate with each other to complete testing tasks through testing service registration, discovery and invocation. The analysis of the architecture in a typical scenario shows that it has the advantages of supporting dynamic discovery and invocation of testing services as required by the dynamic discovery and invocation of normal functional services without compromising security, privacy and intellectual property rights. It is flexible and extendable. It also helps to reduce the risk of unnecessary disturbances to the normal operations of services due to testing activities. The paper reports a prototype implementation of the architecture by adapting and implementing the ontology of software testing using Semantic Web Services technology. A case study with the WS wrapping of an automated testing tool is also reported, which demonstrated that the architecture is technically feasible.},
 duplicado = {false},
 inserir = {false},
 title = {Ontology for Service Oriented Testing of Web Services},
 year = {2008}
}

@article{907,
 abstract = {Abstract:
The distributed structure of agents makes the test for large and complex Web Service composition possible. This paper discusses how to develop the multi-agent test environment for BPEL-based Web Service composition. The BPEL-based Web Service composition is modeled by HPN that can be easily referenced by test case generation and test evaluation. By representing agent in HPN, the agent can be dynamically bound at runtime. In order to implement the multi-agent test environment for BPEL-based Web Service composition, the ontology is analyzed and defined based on XML because of its flexibility, extensibility etc. This ontology is the basis of the agent communication and provides the terms for test case generation and test evaluation etc. The test case generation and test evaluation under this test environment are analyzed. The test process is provided to illustrate the inter-action between the agents under this multi-agent test environment to accomplish test task.},
 duplicado = {false},
 inserir = {true},
 title = {Multi-agent test environment for BPEL-based web service composition},
 year = {2008}
}

@article{908,
 abstract = {Abstract:
Service-oriented architecture (SOA) is becoming the mainstream of distributed system integration. Trustworthiness is critical for cross-domain service interaction, and testing is necessary to build the trust among the different parties involved in SOA. MAST, a multi-agent-based service testing framework, was proposed for testing service-based applications in our previous work. This paper further explores the agent coordination issues in the MAST framework to address the challenge of effective agent communication and interaction. A hybrid coordination architecture is presented which combines data-driven and control-driven models based on the reactive tuple space technique. Different tuple spaces are introduced to facilitate data sharing and asynchronous coordination among test agents. A subscription mechanism is introduced to associate programmable reactions to the events occurred and state changes on the tuple space. The mobile agent technique is also introduced to implement the test agents, which are created on line carrying the tasks, and migrate to the host computers to execute various tasks. A prototype system is designed and implemented to illustrate the proposed approach},
 duplicado = {false},
 inserir = {false},
 title = {A Tuple-Space-Based Coordination Architecture for Test Agents in the MAST Framework},
 year = {2006}
}

@article{909,
 abstract = {Abstract:
This work applies Lehman's theory of software evolution to analyse the characteristics of Web-based applications and identifies the essences and incidents that cause difficulties in developing high quality Web-based applications. It is argued that they belong to Lehman's E-type systems, hence satisfy Lehman's eight laws of software evolution. The uncertainties underlying the development of Web applications are analyzed and their implications are discussed. In order to support sustainable long term evolution of such systems, we proposed a cooperative multiagent system approach to support both development and maintenance activities. A prototype system with emphasis on testing and quality assurance is reported.},
 duplicado = {false},
 inserir = {false},
 title = {Cooperative agent approach to quality assurance and testing Web software},
 year = {2004}
}

@article{910,
 abstract = {Abstract:
Database systems lie at the core of almost every modern software application. The interaction between the application source code and the underlying database schema results in a dependency relationship that affects the application 's maintainability by raising a number of additional maintenance issues. To assess this effect and facilitate the maintenance process, a software engineering approach based on software agents is introduced. The distributed and cooperative nature of a software agent system provides the flexibility required to analyze modern multi-tier database applications such as web-based applications. A prototype system, which employs agent architecture in order to satisfy the requirements of the suggested approach, is presented.},
 duplicado = {false},
 inserir = {false},
 title = {An Agent-based Approach for the Maintenance of Database Applications},
 year = {2007}
}

@article{911,
 abstract = {Mobility, and, implicitly, context-awareness, offer significant opportunities to service providers to augment and differentiate their respective services. Though this potential has been long acknowledged, the dynamic nature of a mobile user's context can lead to various difficulties when engineering mobile, context-aware applications. Classic software engineering elements are well understood in the fixed computing domain. However, the mobile domain introduces further degrees of difficulty into the process. The testing phase of the software engineering cycle is a particular case in point as modelling the myriad of scenarios that mobile users may find themselves is practically impossible. In this paper, we describe a scalable framework that can be used both for initial prototyping and for the final testing of mobile context-aware applications. In particular, we focus on scenarios where the application is in essence a distributed multi-agent system, comprising a suite of agents running on both mobile devices and on fixed nodes on a wireless network.},
 duplicado = {false},
 inserir = {false},
 title = {Scalable Context Simulation for Mobile Applications},
 year = {2006}
}

@article{916,
 abstract = {Abstract
Application life-cycle management (ALM) tools are key for streamlining software development processes. However, small and medium development companies (SMBs) cannot afford to carry out time- and people-intensive tool evaluations for each project, and instead adopt fixed toolsets, thus losing flexibility. To simplify the tool selection process, this article proposes formalizing tool selection as a set of Multiple-Criteria Decision-Making (MCDM) problem, one for each ALM domain. Our domain-parametric recommender takes as inputs a domain, a process definition, and a set of tool evaluation criteria, and yields a ranked list of tools. The approach has been prototyped with the Testing domain and evaluated using a real process and project; the recommendations generated by our approach were quite similar to those of three Testing experts. Pending further evaluation, these results suggest that our approach can generate project-specific tool recommendations with results comparable to those of experts, but at a fraction of the cost.},
 duplicado = {false},
 inserir = {false},
 title = {Semi-automated Tool Recommender for Software Development Processes},
 year = {2014}
}

@article{917,
 abstract = {Abstract. This paper presents the key elements of an ontology that
formalizes part of the knowledge about behavioural modeling and the
associated verification and validation technologies. It summarizes the
concepts existing in this area of interest and the relationships among
them. We propose a classification of different modeling formalisms and
a representation of possible verification and validation methods. A system
is represented using several views conforming to different modeling
languages. Its properties can be assessed with verification and validation
technologies. We also describe existing V&V tools and how they are
related to the other elements.},
 duplicado = {false},
 inserir = {true},
 title = {First steps toward a Verification and Validation Ontology},
 year = {2012}
}

@article{918,
 abstract = {Lack of time and capital for software testing is an
oft encountered problem in most organizations. The
plethora of possible test cases for each software
component makes software testing a very expensive
and time consuming activity and can, in extreme cases,
account for up to 50% of the total cost of the software
development process. The advent of distributed systems
has augmented the complexity of the testing process
and has made it an even more daunting task than
before. This paper presents an innovative multi-agent
framework to ease some of the burdens associated with
the testing of generic distributed systems. The
architecture of the proposed MAS (Multi-Agent
System) is presented along with detailed descriptions
of the various models arrived at following the analysis
and design phases of the proposed system. The
proposed framework is considered as a step towards
the automation of the testing process for distributed
systems in order to enhance their reliability within an
acceptable range of cost and time.},
 duplicado = {false},
 inserir = {false},
 title = {A Multi-Agent Framework for Testing 3-Tier Distributed Systems Architecture},
 year = {2008}
}

@article{919,
 abstract = {Recently keen interests shown in studying the goal behind a user's Web query, so that this goal can be used to improve the quality of a search engine's results in turn improves the popularity of web pages. Advertisement fees can be decided based on this factor. Personalization is now becoming common term for improving E-commerce services and attract more users. Today s recommender system provides suggestion for specific items but drawback that service provider can increase the ratings of specific product and unnecessarily popularity increases. Traditional data protection mechanisms focus on access control and secure transmission, which provide security only against malicious third parties, but not the service provider. This leads to misguiding the users while purchasing some products, so privacy is violated. Many different approaches including web usage mining have been applied to the basic problem of developing accurate and efficient recommendation systems. Online business transactions and the success of E-commerce depend greatly on the effective design of a product recommender mechanism. Our proposal is founded on homomorphic encryption, which is used to obscure the private rating information of the customers from the service provider. While the user's privacy is respected by the service provider, by generating recommendations using encrypted customer ratings, the service provider's commercially valuable item item similarities are protected against curious entities, in turn. Agent based Web mining has advantages of both Web mining and Agent: it can mine data efficiently and intelligently, so it is becoming more and more important in modern E-business.},
 duplicado = {false},
 inserir = {false},
 title = {Role of Agent Technology in Web Usage Mining: Homomorphic Encryption Based Recommendation for E-commerce Applications},
 year = {2016}
}

@article{921,
 abstract = {Abstract:
Testing multi-tier database applications can be viewed as a distributed task performed at each tier by a number of agents. From this point of view, an agent based approach, originally introduced for the software maintenance of such applications, is extended to provide an effective, intelligent and extensible solution suitable for their testing.},
 duplicado = {false},
 inserir = {false},
 title = {Employing Agents Towards Database Applications Testing},
 year = {2007}
}

@article{922,
 abstract = {The ongoing trend towards multi-site software development not only brings the benefits but also creates additional challenges regarding remote communication and coordination. The Software Engineering Ontology (SE Ontology) was first developed to clarify the software engineering concepts and project information, and to enable knowledge sharing among dispersed teams. However, the current SE Ontology has the same passive structure as other ontologies that exist on the Web. Passive structure refers to a need to know exactly the concepts and relationships to which users are referring in the ontology. Otherwise, he/she may not be able to obtain the knowledge required. What is needed is active support that can help users find the information they need and provide them with meaningful output. In this paper, we propose an active Software Engineering Ontology through Multi-Agent System (SEOMAS) framework which is intended to provide active support to access software engineering domain knowledge and to recommend software project information captured in the SE Ontology. The UML and Agent UML are used to formalise the SEOMAS framework and to model interactions between the various agents. The prototype is developed and evaluated using several scenarios of the online shopping system development as a case study.},
 duplicado = {false},
 inserir = {false},
 title = {An Ontology-Based Multi-Agent System for Active Software Engineering Ontology},
 year = {2016}
}

@article{923,
 abstract = {Abstract:
This work employs and extends the Gaia methodol- ogy to design a formal open framework based on agent for automatically testing Web applications. In our framework, each test task corresponds to a role and the agent takes this role to achieve its test task or co- operates by interaction with other agent to finish the test tasks. The agent can not only join or leave agent society at will, but also take or release roles at run time dynamically. Our framework can be extended easily by adding new roles to provide much more func- tion. At the same time, agents and roles are loosely coupled; role classes and agent classes can be de- signed at the same time by different teams. The internal design of multi-agent system (MAS) is independent of the Web applications. This framework helps to imple- ment automatically testing of Web applications.},
 duplicado = {false},
 inserir = {false},
 title = {A Formal Open Framework Based on Agent for Testing Web Applications},
 year = {2007}
}

@article{925,
 abstract = {During the last decade the continuous growth of the Web resulted in a significant development
shift from simple types of software applications to distributed multi-tier web-based
applications. In general, distributed systems are by nature more complex than centralized
systems. As a result, the software engineering tasks of these systems are also complicated.
Unlike traditional software applications, Web-based applications are associated with a
plethora of special characteristics that impede the appliance of conventional software
engineering techniques. Among them, the most important include the distributed and
stateless nature of the Web, the impressively high changing frequency of implementation
technologies and the spread of dynamic Web pages. Furthermore, the vital role of databases
in both web and distributed applications raises a demand for introducing software
engineering techniques tailored for these applications. These applications, known as
database applications (DA), contain embedded SQL statements in the source code. Similarly
to web applications, the presence of such special statements turns out to impose a number of
limitations to the applicability of existing software engineering techniques while also
originating new issues.
In this chapter, the use of agent technology to confront with the software engineering task
will be illustrated. More precisely, the focus will be on the application of agent systems in
order to confront with the requirements of the software engineering process for distributed
software systems in general, paying particular attention to distributed database applications
and web applications.
Software agents can be described as intelligent and autonomous software entities that have
the ability to exhibit proactive behaviour and to collaborate with each other. The software
engineering process can be greatly enhanced by utilising agent technology and adopting the
architecture of an intelligent, flexible and extensible agent system. The multi-tier
architecture of most distributed applications offers a suitable foundation because of its
inherent complication that highlights the significant and novel contribution of a multi-agent
architecture.
The rationale behind utilizing agent technology has to do with the interoperability of the
software resources belonging to potentially disparate application components and disparate
domains. Towards this direction, agents offer a unified platform of interaction through
agent communication.
The application of agent technology for the software engineering task is certainly a new and
promising research area. However, a variety of approaches that attempt to exploit the
www.intechopen.com
140 Tools in Artificial Intelligence
benefits of agent technology have already made their appearance and it is expected that this
tendency will further evolve. At this point, it needs to be clarified that the chapter will not
focus on the research area that deals with the employment of software engineering
technology for agent systems. Although similar in title, this research area deals with
applying software engineering methodologies to assist the creation of multi-agent systems;
something completely different.
The first one has as a goal to provide an agent infrastructure to support software testing.
This is realised by suggesting multi-agent frameworks that can be used as a model to build
agent systems for testing service-oriented web applications. This research track aims at
presenting an agent system for tackling the issues of software maintenance and testing of
distributed applications.
Illustrating the research attempts that employ software agents on software engineering
tasks, they can be categorised according to two key target levels. The first one has an
infrastructural target. Some research work focuses on presenting communication and
coordination infrastructures for agents engaged in web software testing. Another research
direction targets the creation of a multi-agent framework for software testing but the goal is
on how an agent infrastructural framework can assist the job of constructing concrete agents
systems for service-oriented applications.
The second one has a more applied target. As a representative work, research in which
multi-agent system architectures are used in software testing of web-based applications can
be mentioned. Moreover, there is ongoing research where an agent system is being utilised
for the software engineering of distributed database applications. The first primary objective
is to assess the maintainability and to facilitate the maintenance of such applications in the
presence of changes on the schema of the underlying database. The second primary
objective is to support another major software engineering task namely structural and
regression software testing.
The remainder of this chapter is organised as follows. Section 2 outlines the fundamental
background scientific areas of Agent Systems and Software Engineering. Section 3
introduces the first primary research direction where agent frameworks are used in software
engineering. Section 4 continues the illustration covering the second primary research
direction where multi-agent systems are used in software engineering. Section 5 is about
Agent-Oriented Software Engineering and gives a brief description of the opposite view
where the idea of an agent is being utilised as a generic software engineering model. Finally,
section 6 concludes the chapter by offering an overall analysis of the current research status
by highlighting the commonalities and the differences of the above research approaches, in
a form of comparative evaluation, and providing a view of the scope of the current
approaches and potential future research courses of action. },
 duplicado = {false},
 inserir = {false},
 title = {Agent Systems in Software Engineering},
 year = {2004}
}

@article{926,
 abstract = {Abstract:
Nowadays, a new generation of Health Information systems (HIS) called e-health is developing. Health-related equipment can be defined as electronic devices that are able to be implanted in the patient's body to help his health state. Insulin pump and pacemaker device are some instances of such equipment. Most of the patients who use these devices do not have much technical knowledge. Therefore, these devices must be self-configuring and self-managing. More significantly, they should be able to tolerate and remove errors. As these equipment are often sensitive and may affect the life of human beings, their software are called vital software programs. Even a small error in these vital software programs may endanger lives of human being. The runtime test of these software programs is necessary. This article aims at presenting a solution to verify the performance of these devices. Pacemaker device as a sample is studied. The proposed strategy suggests the application of reactive model-based software agents which play the role of an oracle in a software test. This agent's knowledgebase is obtained by Petri net drawn based on the proper state of device implementation. Petri net first turns into matrix thanks to linear algebra and then changes to rules which the agent uses to make decisions. Using its intelligence, the agent directs the error, if identified, to the safe mode. The safe mode is meant to be the patient's proper heart rate. Checking at runtime by use of the agent in addition to the assurance of accurate performance of device can make quick decision makings at crisis time possible for those devices related to human health.},
 duplicado = {false},
 inserir = {false},
 title = {Medical software runtime checking using Petri-nets & software agents},
 year = {2014}
}

@article{929,
 abstract = {Web applications are becoming increasingly complex and yet important for companies. As web applications become more and more prevalent, their testing and quality assurance become more and more important and crucial. Due to the complexity of the underlying technologies of web applications, testing of such softwares is challenging.

Applying user sessions data as test cases can reduce the cost of regression testing process. In this paper we provide a multi agent software environment in order to support automatic replay of user sessions. In this environment different agents can perform replay of user sessions by being distributed on various platforms and geographic locations.},
 duplicado = {false},
 inserir = {false},
 title = {A multi-agent system apporach for user-session-based testing of web applications},
 year = {2007}
}

@article{930,
 abstract = {Abstract:
Connected health devices are used to in vivo cure and prevent abnormal conditions without manual intervention. This paper presents an intelligent agent-based method for runtime verification of cardiac pacemaker runtime behavior. Construct the agent's knowledge base based on the Fuzzy Colored Petri Net (FCPN). Based on our previous experiences, the FCPN will be reduced the scale of our network in comparison to the Petri-net(PN). Compared to a simple inference engine, the FCPN can cover the concurrent states and in addition intelligent agent can ensure the accuracy of the runtime verification operation of the cardiac pacemaker in vital and unexpected situation.},
 duplicado = {false},
 inserir = {false},
 title = {Utilizing Fuzzy colored Petri-Nets to monitor cardiac pacemaker behavior  Sign In or Purchase},
 year = {2016}
}

@article{931,
 abstract = {There is an urgent need to create awareness about the potential benefits of using agents in software test case generation and to identify the need to develop agent-based regression testing techniques and approaches. It may help in reducing time and cost required for testing. This study reports systematic literature review of existing test case generation approaches for regression testing and agent-based software testing systems. The emphasis is articulated on agent-based regression test case generation. Further research directions are recommended. In the systematic literature review, we framed three sets of research questions. Based on our inclusion and exclusion criteria, we identified 115 potential research papers on test case generation in regression testing and agent-based software testing. We explored journals, international conferences, workshops and identified 59 studies in test case generation for regression testing and 56 studies in agent-based software testing. The data extracted from our study are classified into seven broader areas of agent-based software testing. Based on our systematic literature survey, we recognized available techniques, approaches, platforms as well as methodologies for regression test case generation and developing agent-based software testing systems. This study will benefit the researchers to carry forward their work in the domain of regression test case generation and agent-based software testing. To cut down on schedule and cost, mobile agent-based software testing can be a promising alternative.},
 duplicado = {false},
 inserir = {false},
 title = {A Systematic Review of Agent-Based Test Case Generation for Regression Testing},
 year = {2018}
}

@article{932,
 abstract = {Large software development projects involve several participants who are distributed
geographically without face-to-face communication. To maintain collaborative work
through effective communication and coordination, it is necessary to have a common
understanding of terminology and methodology to clarify software engineering
concepts and enable knowledge exchange and reuse. We consider an ontology
designed for distributed software development can be a solution to improve
information and knowledge sharing in such scenarios. However, software team
members may not be familiar with the use of the ontology by themselves, an active
support is needed to proactively deliver knowledge and project information that is
semantically defined in the ontology based on appropriate context. In this paper, we
propose an ontology-based multi-agent system conceptual framework that can provide
intelligent assistance to access and recommend knowledge and project information
during multi-site software development. We use UML 2.1 to model interaction among
software agents. This framework is expected to improve the effectiveness of the
communication and coordination of software teams to reduce the unsuccessful rate of
the software development project.},
 duplicado = {false},
 inserir = {false},
 title = {Use and Design of Ontology-based Multi-agent System for Multi-site Software Development Environment},
 year = {2015}
}

@article{934,
 abstract = {Software test tools has been growing increasingly in terms of popularity. These testing tools are used by developers according to the their development platforms. Selection of a software test tool is a fundamental step for software testing. This paper presents a multi-agent framework facilitating the software test tool selection process. The framework has been designed for the Windows platform using the .NET architecture. The main design goals are to develop an effective and exible framework that enables a user to select different types of web-based testing tools. Also a UML model has been generated to build a multi-agent system. Thanks to the designed framework, a user is able to decide which tool should be used for detecting database, web-based and static code faults. Testing results of our framework are seen in Table 2. According to the obtained results, software test procedures can be instantly and efficiently managed based on multi-agent systems.},
 duplicado = {false},
 inserir = {false},
 title = {Development of a Multi-Agent Framework for Software Quality},
 year = {2015}
}

@article{936,
 abstract = {Testing is a software development activity, devoted to
evaluating quality and improving the end product by
identifying defects and problems. It's an important aspect in
software application development to insure the application
quality which include the application's performance,
reliability, speed, security and functionality. Testing can be
done by automation tools like Win runner, QTP or manually.
Manual testing, is another option, but it takes lot of time and
manpower. Automated testing has increased costs and most of
the times is not affordable for small or middle level
organizations. In this paper we are presenting agent based
testing which can fulfill the testing requirements on smaller
costs. We present an implementation framework of an online
application which can be accessed over the web and on
payment of small amounts can provide different kinds of
software tests for web based applications. The framework
uses Multi Agent Systems to manage, perform, report the
testing procedures. },
 duplicado = {false},
 inserir = {false},
 title = {Web Application Testing Framework using Agents},
 year = {2017}
}

@article{938,
 abstract = {Abstract:
Regression testing is one of the important fields of testing. High testability is the desirable goal. Our research work advocates the use of agent based technology for the Regression Testing. We have proposed agent based testing framework for enhancing the efficiency of Regression Testing. Different agents have been proposed for SRS Comparison, Code Comparison, Design Comparison, Impact analysis and Test Case Generation. The results have been achieved by using JADE platform.},
 duplicado = {false},
 inserir = {false},
 title = {Agent based regression testing framework},
 year = {2014}
}

@article{939,
 abstract = {Web applications are quickly replacing standalone applications for everyday tasks.
These web applications need to be tested to ensure proper functionality and reliability.
There have been substantial efforts to create tools that assist with the testing of web
applications, but there is no standard set of tools or a recommended workflow to
ensure speed of development and strength of application.
We have used and outlined the merits of a number of existing testing tools and
brought together the best among them to create what we believe is a fully-featured,
easy to use, testing framework and workflow for web application development.
We then took an existing web application, PolyXpress, and augmented its development
process to include our workflow suggestions in order to incorporate testing at
all levels. PolyXpress is a web application that allows you to create location-based
stories, build eTours, or create restaurant guides. It is the tool that will bring people
to locations in order to entertain, educate, or provide amazing deals.[10] After incorporating
our testing procedures, we immediately detected previously unknown bugs
in the software. In addition, there is now a workflow in place for future developers to
use which will expedite their testing and development.},
 duplicado = {false},
 inserir = {false},
 title = {CREATING A TESTING FRAMEWORK AND WORKFLOW FOR DEVELOPERS NEW TO WEB APPLICATION ENGINEERING},
 year = {2014}
}

@article{940,
 abstract = {In software testing process a large amount of information is required and generated. This information can be stored as knowledge that needs to be managed and maintained using principles of knowledge management. Ontologies can act as a bridge by representing this testing knowledge in an accessible and understandable way.},
 duplicado = {false},
 inserir = {true},
 title = {A Top-Domain Ontology for Software testing.},
 year = {2016}
}

@article{941,
 abstract = {Petri nets have been extensively used in the modelling and analysis of concurrent and
distributed systems. The verification and validation of Petri nets are of particular
importance in the development of concurrent and distributed systems. As a
complement to formal analysis techniques, testing has been proven to be effective in
detecting system errors and is easy to apply. An open problem is how to test Petri nets
systematically, effectively and efficiently. An approach to solve this problem is to
develop test criteria so that test adequacy can be measured objectively and test cases
can be generated efficiently, even automatically. In this paper, we present a
methodology of testing high-level Petri nets based on our general theory of testing
concurrent software systems. Four types of testing strategies are investigated, which
include state-oriented testing, transition-oriented testing, flow-oriented testing and
specification-oriented testing. For each strategy, a set of schemes to observe and
record testing results and a set of coverage criteria to measure test adequacy are
defined. The subsumption relationships and extraction relationships among the
proposed testing methods are systematically investigated and formally proved. },
 duplicado = {false},
 inserir = {false},
 title = {A methodology of testing high-level Petri nets},
 year = {2002}
}

@article{947,
 abstract = {Background/Objectives: Research in recent years has probed integration amongst research field of Software Engineering & Semantic Web technology, addressing the advantages of applying Semantic techniques to the field of Software Engineering. Prolifically published studies have further substantiated the benefits of ontologies to the field of Software Engineering, which clearly motivate us to explore further opportunities available in this collaborated field. This paper is a survey expounding such opportunities while discussing the role of ontologies as a Software Life-Cycle support technology. Method/Statistical Analysis: Survey centred on providing an overview of the state-of-art of all the ontologies available for Software Engineering followed by their categorization based on software life cycle phases and their application scope. Findings: Characterization of ontologies as a Software Life-cycle support technology, instigated by the increasing need to investigate the interplay between Semantic Web & Software Engineering with the ultimate goal of enabling & improving Software Engineering capabilities. Application/Improvements: This paper discusses the practical and potential applications of ontologies in the field of Software Engineering followed by the issues and challenges that will keep this field dynamic and lively for years to come.},
 duplicado = {false},
 inserir = {false},
 title = {Ontologies for Software Engineering: Past, Present and Future},
 year = {2016}
}

@article{949,
 abstract = {Abstract
Context

Software testing is a knowledge intensive process, and, thus, Knowledge Management (KM) principles and techniques should be applied to manage software testing knowledge.

Objective

This study conducts a survey on existing research on KM initiatives in software testing, in order to identify the state of the art in the area as well as the future research. Aspects such as purposes, types of knowledge, technologies and research type are investigated.

Method

The mapping study was performed by searching seven electronic databases. We considered studies published until December 2013. The initial resulting set was comprised of 562 studies. From this set, a total of 13 studies were selected. For these 13, we performed snowballing and direct search to publications of researchers and research groups that accomplished these studies.

Results

From the mapping study, we identified 15 studies addressing KM initiatives in software testing that have been reviewed in order to extract relevant information on a set of research questions.

Conclusions

Although only a few studies were found that addressed KM initiatives in software testing, the mapping shows an increasing interest in the topic in the recent years. Reuse of test cases is the perspective that has received more attention. From the KM point of view, most of the studies discuss aspects related to providing automated support for managing testing knowledge by means of a KM system. Moreover, as a main conclusion, the results show that KM is pointed out as an important strategy for increasing test effectiveness, as well as for improving the selection and application of suited techniques, methods and test cases. On the other hand, inadequacy of existing KM systems appears as the most cited problem related to applying KM in software testing.},
 duplicado = {false},
 inserir = {false},
 title = {Knowledge management initiatives in software testing: A mapping study},
 year = {2015}
}

@article{951,
 abstract = {Abstract - With the growth of data from several different sources
of knowledge within an organization, it becomes necessary to
provide computerized support for tasks of acquiring, processing,
analyzing and disseminating knowledge. In the software process,
testing is a critical factor for product quality, and thus there is an
increasing concern in how to improve the accomplishment of this
task. In software testing, finding relevant knowledge to reuse can
be a difficult and complex task, due to the lack of a strategy to
represent or to associate semantics to a large volume of test data,
including test cases, testing techniques to be applied and so on.
This paper aims to investigate, through a Systematic Mapping of
the Literature, some aspects associated with applying Knowledge
Management to Software Testing.},
 duplicado = {false},
 inserir = {false},
 title = {Knowledge Management Applied to Software Testing: A Systematic Mapping},
 year = {2013}
}

@article{953,
 abstract = {Abstract:
To solve the trustworthiness of reusable test cases, a trustworthiness framework of reusable test cases is proposed in this paper, including the analysis of trustworthiness attributes and trustworthiness evidence. Furthermore, the trustworthiness assurance processes related are presented to support management activities of reusable test case.},
 duplicado = {false},
 inserir = {false},
 title = {Trustworthiness framework of reusable test case},
 year = {2013}
}

@article{954,
 abstract = {Abstract:
Software testing is a sub area of software engineering which is also a knowledge intensive and collaborative activity. Our previous study results revealed that knowledge in the repositories were outdated, internal documents are unstructured and varied formats, less accessing facilities and lack of targeted delivery methods, such that software testers from software companies are highly affected by not being able to get vital information required to carryout their software testing activities. Ontologies emerge as one of the more appropriate knowledge management tools for supporting knowledge representation, processing, storage and retrieval. A Software testing ontology is designed to represent the necessary software testing knowledge within the software testers' context. The ontology-based Knowledge Sharing Portal is introduced into the semantic representation of software testing knowledge. SPARQL is used as the query language to retrieve software testing knowledge from the semantic storage. Both Ontology experts and non-experts evaluated the developed ontology.},
 duplicado = {false},
 inserir = {true},
 title = {An Ontology-Based Knowledge Sharing Portal for Software Testing},
 year = {2017}
}

@article{955,
 abstract = {Abstract
With the advent of Component-based software engineering (CBSE), large software systems are being built by integrating pre-built software components. The Semantic Web in association with CBSE has shown to offer powerful representation facilities and reasoning techniques to enhance and support querying, reasoning, discovery, etc. of software components. The goal of this paper is to research the applicability of Semantic Web technologies in performing the various tasks of CBSE and review the experimental results of the same in an easy and effective manner. To the best of our knowledge, this is the first study which provides an extensive review of the application of Semantic Web in CBSE from different perspectives. A systematic literature review of the Semantic Web approaches, employed for use in CBSE, reported from 2001 until 2015, is conducted in this research article. Empirical results have been drawn through the question-answer based analysis of the research, which clearly tells the year wise trend of the research articles, with the possible justification of the usage of Semantic Web technology and tools for a particular phase of CBSE. To conclude, gaps in the current research and potential future prospects have been discussed.},
 duplicado = {false},
 inserir = {false},
 title = {Software component and the semantic Web: An in-depth content analysis and integration history},
 year = {2017}
}

@article{959,
 abstract = {Software Fault Injection (SFI) is an established technique for assessing the robustness of a software under test by exposing it to faults in its operational environment. Depending on the complexity of this operational environment, the complexity of the software under test, and the number and type of faults, a thorough SFI assessment can entail (a) numerous experiments and (b) long experiment run times, which both contribute to a considerable execution time for the tests.

In order to counteract this increase when dealing with complex systems, recent works propose to exploit parallel hardware to execute multiple experiments at the same time. While PArallel fault INjections (PAIN) yield higher experiment throughput, they are based on an implicit assumption of non-interference among the simultaneously executing experiments. In this paper we investigate the validity of this assumption and determine the trade-off between increased throughput and the accuracy of experimental results obtained from PAIN experiments.},
 duplicado = {false},
 inserir = {false},
 title = { No PAIN, no gain?: the utility of PArallel fault INjections},
 year = {2015}
}

@article{960,
 abstract = {Abstract:
Software testing is becoming an increasingly expensive and time-consuming endeavor. With advances in cloud computing, new methods of testing in cloud environments are allowing testers to take advantage of the vast resources of the cloud while demanding less upfront costs. Testing as a service (TaaS) is a model of software testing that offers accessible services that handle testing activities for consumers on a pay-for-use basis. The goal of this paper is to describe the state of academic research within TaaS, including a basic architecture, example services, recent studies, benefits, challenges, needs, and a road ahead},
 duplicado = {false},
 inserir = {false},
 title = {Software Testing as a Service: An Academic Research Perspective},
 year = {2013}
}

@article{961,
 abstract = {This book summarizes the current hard problems in software testing as voiced by leading practitioners in the field. The problems were identified through a series of workshops, interviews, and surveys. Some of the problems are timeless, such as education and training, while others such as system security have recently emerged as increasingly important.

The book also provides an overview of the current state of Testing as a Service (TaaS) based on an exploration of existing commercial offerings and a survey of academic research. TaaS is a relatively new development that offers software testers the elastic computing capabilities and generous storage capacity of the cloud on an as-needed basis. Some of the potential benefits of TaaS include automated provisioning of test execution environments and support for rapid feedback in agile development via continuous regression testing.

The book includes a case study of a representative web application and three commercial TaaS tools to determine which hard problems in software testing are amenable to a TaaS solution. The findings suggest there remains a significant gap that must be addressed before TaaS can be fully embraced by the industry, particularly in the areas of tester education and training and a need for tools supporting more types of testing. The book includes a roadmap for enhancing TaaS to help bridge the gap between potential benefits and actual results.},
 duplicado = {false},
 inserir = {false},
 title = { Hard Problems in Software Testing: Solutions Using Testing as a Service (TaaS)},
 year = {2014}
}

@article{962,
 abstract = {Abstract:
The interoperability between various automation systems is considered as one of the major character of future automation systems. Service-oriented Architecture is a possible interoperability enabler between legacy and future automation systems. In order to prove the interoperability between those systems, a verification framework is essential. This paper proposes a configurable cloud-based validation environment for interoperability tests between various distributed automation systems. The testing framework is implemented in a multi-layer structure which provides automated closed-loop testing from the protocol level to the system level. The testing infrastructure is also capable for simulating automation systems as well as wireless sensor networks in the cloud. Test cases could be automatically generated and executed by the framework.},
 duplicado = {false},
 inserir = {false},
 title = {A configurable cloud-based testing infrastructure for interoperable distributed automation systems},
 year = {2014}
}

@article{965,
 abstract = {Software testing is a crucial phase of the software development lifecycle, responsible for assuring that the system under test meets quality standards, requirements, and consumer needs. Unfortunately, software testing is not without flaws. Some problems are timeless while others are brought on by new technologies and methodologies. As software systems grow in size and complexity, quality becomes significantly more difficult to ensure. With recent advancements in cloud computing, the internets vast and elastic resources are available for testing. Testing as a Service (TaaS) offers accessible services that handle testing activities to consumers on a pay-as-you-test basis in hopes of providing a more efficient and effective way of guaranteeing software quality.
This thesis presents the top industry issues and concerns as identified through the Hard Problems in Software Testing survey, followed by a thorough overview of the current state of TaaS based on an exploration of existing commercial offerings and a survey of academic research. These problems are then examined to determine where TaaS can be applied to overcome the issue or offer improvements. The remaining shortcomings are analyzed to generate a roadmap for enhancing TaaS by addressing the hard problems plaguing the industry.
The evaluation of three existing tools against academic research and the hard problems indicated by the survey revealed a gap that must be overcome before TaaS can be fully embraced by the industry. While many of the industry concerns were reduced or eliminated by TaaS tools, a few still remain. These challenges appeared the most prominent in the areas of tester education and training, and a need for better tools, including issues such as incorporating fully-automated test case generation, offering greater compatibility and extensibility for external tools, promoting more types of testing, and enhanced security.},
 duplicado = {false},
 inserir = {false},
 title = {Overcoming Hard Problems in Software Testing with Testing as a Service},
 year = {2014}
}

@article{967,
 abstract = {Fault injection (FI) is an experimental technique to assess the robustness of software by deliberately exposing it to faulty inputs and stressful environmental conditions specified by fault models. As computing hardware is becoming increasingly parallel, software execution is becoming increasingly concurrent. Moreover, to exploit the potential of increasingly parallel and interconnected computing systems, the complexity of the software stack, comprising operating systems, drivers, protocol stacks, middleware and distributed applications, also grows. As a consequence, the fault models classically used for in-lab robustness assessments no longer match the reality of fault conditions that modern systems are exposed to.
In my thesis I account for this development by proposing the construction of higher order fault models from classical models by different means of composition. To demonstrate the effectiveness of such higher order models, I define a set of four comparative fault model efficiency metrics and apply them for both classical and higher order models. The results show that higher order models identify robustness issues that classical models fail to detect, which supports their adoption in the assessment of modern software systems. While higher order models can be implemented with moderate effort, they result in a combinatorial explosion of possible fault conditions to test with and, more severely, they introduce an ambiguity to experimental result interpretation that results in an increased number of required tests to maintain the expressiveness of assessment results.
To mitigate the overhead that the adoption of higher order fault models entails, I propose to increase the experiment throughput by concurrently executing experiments on parallel hardware. The results show that such parallelization yields the desired throughput improvements, but also that care has to be taken in order not to threaten the metrological compatibility of the results. To account for resource contention in concurrent experiment executions that can lead to result deviations, I present a calibration approach that provides timeout values for the safe configuration of hang failure detectors.
The experimental environment to conduct concurrent experiments is based on a generic FI framework developed in the context of my thesis that is highly adaptable to a variety of different target systems and that has been released under an open source license.},
 duplicado = {false},
 inserir = {false},
 title = {On the Utility of Higher Order Fault Models for Fault Injections},
 year = {2015}
}

@article{968,
 abstract = {Abstract:
In the testing Cloud platform, there exist too many testing tasks waiting for scheduling at the same time. How to design scheduling strategy is really a challenging problem. In this paper, we firstly analyze the relationship between the testing tasks and establish the task relationship model. Based on these analyses, we propose a dynamic task scheduling strategy using genetic algorithm, which not only ensures to get the least execution time but also guarantee load balance. The dynamic strategy based on genetic algorithm is being compared with traditional static genetic algorithm on cloudsim. The experimental result shows the high the effectiveness of the proposed strategy.},
 duplicado = {false},
 inserir = {false},
 title = {Dynamic Scheduling Strategy for Testing Task in Cloud Computing},
 year = {2014}
}

@article{969,
 abstract = {It is critical to evaluate the quality-of-service (QoS) properties of enterprise distributed real-time and embedded (DRE) system early in their lifecycle instead of waiting until system integrationto minimize the impact of rework needed to remedy QoS defects. Unfortunately, enterprise DRE system developers and testers often lack the necessary resources to support such testing efforts. This chapter discusses how test clouds (i.e., cloud-computing environments employed for testing) can provide the necessary testing resources. When combined with system execution modeling (SEM) tools, test clouds can provide the necessary toolsets to perform QoS testing earlier in the lifecycle. A case study of design and implementing resource management infrastructure from the domain of shipboard computing environments is used to show how SEM tools and test clouds can help identify defects in system QoS specifications and enforcement mechanisms before they become prohibitively expensive to fix.},
 duplicado = {false},
 inserir = {false},
 title = {Using Test Clouds to Enable Continuous Integration Testing of Distributed Real-Time and Embedded System Applications},
 year = {2013}
}

@article{970,
 abstract = {Testing-as-a-Service (TaaS) is a new quality assurance model addressing the challenges of software testing
in the cloud. The missing access to the hardware or different software configurations as well as the difficulties
of building a test environment are examples for common problems in the testing process. This paper addresses
such problems by proposing a TaaS-enabled framework offering testing services on as-needed basis. The
framework, called Testing as a Service Software Architecture (TASSA), supports testing of web service
compositions described with Business Process Execution Language for Web Services (WS-BPEL). Its core
functionality includes fault injection and dependencies isolation of the application under test. It is
implemented as web services deployed on cloud infrastructure. In addition, the TASSA Graphical User
Interface (GUI) for test case design and execution is implemented as a plugin for Eclipse IDE. It could be
accessed from a local computer or used for building a cloud test lab on a virtual machine. Sample business
process from wine industry is used for proving the feasibility of TASSA framework},
 duplicado = {false},
 inserir = {false},
 title = {TASSA: A Testing as a Service Framework for Web Service Compositions},
 year = {2016}
}

@article{971,
 abstract = {Abstract:
Cloud service testing ensures that services run properly and meet the Service Level Agreement (SLA) requirements. However, performance problems of a cloud service, such as the availability and reliability, are difficult to diagnose because these issues might be caused from different system components. To solve these problems, this study proposes an architecture for testing environment configuration and quality estimation for both of fault diagnosis and bottleneck detection. The proposed system has two components: offline testing and online management. In offline testing, target service are tested by using the proposed testing module and corresponding metrics are collected for further analysis. The analyzed results which are separated between fault diagnosis and bottleneck detection will be stored in knowledge databases. Finally, online management module will automatically suggest how to do when facing this kind of problem according to knowledge based diagnosis.},
 duplicado = {false},
 inserir = {false},
 title = {An Architecture for Cloud Service Testing and Real Time Management},
 year = {2015}
}

@article{974,
 abstract = {A
S software gradually becomes an important and necessary
facet in modern daily lives, software quality should
be treated as an utmost issue attentive by all parties involved.
Unfortunately, the inherent software quality problems are often
not carefully administered from the outset of the development
process. Inadequate and inappropriate testing is one common
shortfall that culminates defects to be accumulated over its life
cycle.
A software development model, namely, V-model presents
the relationships between each development and associated
testing phases in the waterfall model [1]. Four testing phases
are Unit Testing (UT), Integration Testing (IT), System Testing
(ST), and User Acceptance Testing (UAT). UT is operated
by the development team, IT and ST are performed by the
test team, and UAT is conducted by the user team. Some
organizations focus on IT and ST only while others pay
attention to all phases. The problems are lack of related testing
understanding and knowledge of developers and users [2, 3]
as far as test plan and test skill in UT and UAT are concerned.
Furthermore, the communication and coordination among the
teams are not effective. The impact from this issues is the gaps
between developers and testers, and between testers and users.
This is illustrated as a framework model in Fig. 1.
The software testing performance that we focus on this
study encompasses key performance factors such as duration,
effort, and quality [4, 5, 6, 7] can affect this gaps directly. This
study describes a significant improvement in key performance
factors with the concepts of testing service from test team for
development and user team in UT and UAT.
The paper is structured as follows. Section 2 presents
the survey results in software testing industry and related
researches. Service-based testing Support model (SbtS) is
presented in Section 3. Section 4 summarizes the practical
results of a case study with three pilot projects. A final
concluding remarks and future work are given in Section 5},
 duplicado = {false},
 inserir = {false},
 title = {Software Testing Process Performance Improvement using Service-Based Testing Support},
 year = {2012}
}

@article{977,
 abstract = {Abstract:
This study proposed testing/inspecting as a service (TaaS) for computing equipment and aimed to test objects in the context of combining software and hardware. We developed a dynamic feedback framework to enhance both the efficiency and the quality of the testing service. The framework consists of inner and outer feedback mechanisms, in which the updated industrial standards and client requirements are incorporated allowing the testing to be modified accordingly. A detailed description of the testing procedures and industrial metrology are provided in the study. The improvement gained from TaaS are demonstrated and discussed. The results show that by using infrastructure as a service (IaaS), the physical hardware improves in quality due to the fact that the quality was verified by Automated Optical Inspection (AOI) TaaS or cloud container data center (CDC) TaaS etc. Moreover, the OS in platform as a service (PaaS) can be ensured because the resources of the physical/virtual data in the IaaS were tested by the OS TaaS. As for the application (APP) TaaS, the stability and performance of the OS system can be maintained since the applications of the software on the PaaS have been tested.},
 duplicado = {false},
 inserir = {false},
 title = {The Verification and Validation of a Large-Scale System: Equipment TaaS as an Example},
 year = {2014}
}

@article{984,
 abstract = {This paper gives a general vision of the knowledge areas that compound the software engineering according to the IEEE SWEBOK (Software Engineering body of knowledge) guide, and starting at that point proposed a pedagogic strategy, to be applied as a cathedra complement at the Pedagogic and Technologic University of Colombia (UPTC), due the knowledge areas proposed at SWEBOK provide an appropriate structure that adapts itself to the teaching-learning process at the Systems and Computation engineering college. The strategy that square up the educational objectives of Bloom's Taxonomy cognitive domain is designed and later evaluated by an advisor and further applied to an study group belonging to the Software Engineering study line; taking the results as a base to define the viability of this propose to be extended to another knowledge areas proffered by SWEBOK guide.},
 duplicado = {false},
 inserir = {false},
 title = {Use of Learning Strategies of SWEBOK Guide Proposed Knowledge Areas},
 year = {2013}
}

@article{986,
 abstract = {Building software test cases is a costly and time-consuming process; hence, good-quality test cases are stored in libraries, for possible reuse. Existing methods for search and retrieval of reusable test cases are not flexible enough, since they do not consider semantics. We propose a test case reuse approach based on the semantic Web techniqueontology, which improves flexibility and reusability during test case generation. Moreover, the addition of semantics to test cases handles users queries better.},
 duplicado = {false},
 inserir = {true},
 title = {An Ontology-Based Approach for Test Case Reuse},
 year = {2015}
}

@article{987,
 abstract = {With the advent of unmanned systems and the movement toward greater complexity, the test and evaluation community faces new challenges. This paper presents an ontology for the unmanned and autonomous systems of systems test and evaluation (UASoS T&E) domain that is intended to help support addressing these challenges in T&E. The development of a UASoS T&E ontology provides a means for those interested in the field of UASoS T&E to understand the entities, relationships, and terminology within the domain. The ontology provides a common language and basis of understanding for UASoS T&E that can be leveraged as a foundation for other efforts.},
 duplicado = {false},
 inserir = {false},
 title = {An Ontology for Unmanned and Autonomous Systems of Systems Test and Evaluation},
 year = {2010}
}

@article{988,
 abstract = {Test automation is adopted by the majority of software and hardware producers since it speeds up the testing phase and allows to design and perform a large bunch of tests that would be hardly manageable in a manual way. When dealing with the testing of hardware instruments, different physical environments have to be created so that the instruments under test can be analyzed in different scenarios, involving disparate components and software configurations.

Creating a test case is a time consuming activity: test cases should be reused as much as possible. Unfortunately, when a physical test plant changes or a new one is created, understanding if existing test cases can be executed over the updated or new test plant is extremely difficult.

In this paper we present our approach for checking the compliance of a test case w.r.t. a physical test plant characterized by its devices and their current configuration. The compliance check, which is fully automated and exploits a logic-based approach, answers the query Can the test case A run over the physical configured test plant B?},
 duplicado = {false},
 inserir = {false},
 title = {Can My Test Case Run on Your Test Plant? A Logic-Based Compliance Check and Its Evaluation on Real Data},
 year = {2017}
}

@article{990,
 abstract = {????Reusing test cases from existing test case library is quite common in the software testing field. Testing practice tells us that there is a strong relationship between the granularity of a function unit under testing and that of the test case. A function unit with small granularity usually results in the test cases with the same small granularity. Therefore a test case defined as the function point,i. e.,the smallest size function unit,was provided for the first time.Though test cases with smaller granularity usually have better reusability,the cost of accurately reusing and integrating such test cases is also higher. In order to balance the test case reusability and the cost of test case reuse,a novel test case reuse model based on the function point was proposed in this paper. In this model,a reusable test case for specification-based testing was defined and some reuse strategies and three formal reuse methods were given. Finally,the complete automatic software process was realized by a reusing generation tool. The new method has improved reuse accuracy,while greatly enhances the software productivity.},
 duplicado = {false},
 inserir = {false},
 title = {Reusing Test Cases Based on the Function Point},
 year = {2014}
}

@article{991,
 abstract = {Using the Semantic Web (SW) technologies in Software Engineering (SE) is one way of overcoming current SE problems. In this article, we have proposed a semantic method for rapid software development, namely SRAD. From the SRAD point of view, static and dynamic models of software are described in domain ontology and application ontology, respectively. The domain ontology can be a strong substitute for data model and the application ontology contains formal description and implementation of the software functionalities. We have also proposed a method for implementing the software functionalities in the application ontology by using the SW technologies. To execute the software functionalities, any Java compatible rule engine can be used, but for efficiency reasons, we have developed a Running Engine (RE) that supports all types of SWRL atoms. By using the SRAD method, opposed to the current development methods that try to generate code from models, we can produce a formal runnable model of software which encompasses both business logic and program control logic.},
 duplicado = {false},
 inserir = {false},
 title = {SRAD: a semantic rapid method for software development},
 year = {2012}
}

@article{996,
 abstract = {Abstract:
In service computing, the behavior of a service may evolve. When an organization develops a service-oriented application in which certain services are provided by external partners, the organization should address the problem of uninformed behavior evolution of external services. This paper proposes an adaptive framework that bars problematic external services to be used in the service-oriented application of an organization. We use dynamic WSDL information in public service registries to approximate a snapshot of a network of services, and apply link analysis on the snapshot to identify services that are popularly used by different service consumers at the moment. As such, service composition can be strategically formed using the highly referenced services. We evaluate our proposal through a simulation study. The results show that, in terms of the number of failures experienced by service consumers, our proposal significantly outperforms the random approach in selecting reliable services to form service compositions.},
 duplicado = {false},
 inserir = {false},
 title = {An Adaptive Service Selection Approach to Service Composition},
 year = {2008}
}

@article{998,
 abstract = {Testing a group of software artifacts that implement the same specification is time consuming, especially when the test case repository is large. In the meantime, some of test cases may cover the same aspects in the software under test, and thus it is not necessary to apply all the test cases. This paper proposes a Model-based Adaptive Test (MAT) case selection and ranking technique to eliminate redundant test cases, and rank the test cases according to their potency and coverage. This technique can be applied in various domains where multiple versions of an application are available for testing, such as web service group testing, n-version applications, regression testing, and specification-based application testing. MAT is a statistical model based on earlier testing results, and the model can accurately determine the next sets of test cases to minimize the testing effort. It can be applied to testing of multi-versioned web services, and the results shows that MAT can reduce testing effort while still maintain the effectiveness of testing.},
 duplicado = {false},
 inserir = {false},
 title = {A Coverage Relationship Model for Test Case Selection and Ranking for Multi-version Software},
 year = {2009}
}

@article{999,
 abstract = {Abstract:
In recent years WS-BPEL has gain significant attention due to its ability to specify and orchestrate Web services participating in a business process. As the business process becomes complex, the XML-based WS-BPEL document used to describe the interactions among the Web services becomes difficult to understand and test. This paper proposes a structural testing approach for Web service compositions implemented with WS-BPEL. A test model is presented to capture the control flow of the WS-BPEL process. Specifically, the model takes into account the concurrency and synchronization characteristics of WS-BPEL while adapting the BPMN notation to depict the possible process execution flow. Based on the test model, test paths of a WS-BPEL process can be identified and test cases can be derived to validate the compositions of Web services.},
 duplicado = {false},
 inserir = {false},
 title = {A WS-BPEL Based Structural Testing Approach for Web Service Compositions},
 year = {2008}
}

