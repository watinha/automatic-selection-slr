@article{1273,
 abstract = {Context: Software Engineering (SE) research with a scientific foundation aims to influence SE practice to enable and sustain efficient delivery of high quality software. Goal: To improve the impact of SE research, one objective is to facilitate practitioners in choosing empirically vetted interventions. Method: Literature from evidence-based medicine, economic evaluations in SE and software economics is reviewed. Results: In empirical SE research, the emphasis has been on substantiating the claims about the benefits of proposed interventions. However, to support informed decision making by practitioners regarding technology adoption, we must present a business case for these interventions, which should comprise not just effectiveness, but also the evidence of cost-effectiveness. Conclusions: This paper highlights the need to investigate and report the resources required to adopt an intervention. It also provides some guidelines and examples to improve support for practitioners in decisions regarding technology adoption.},
 duplicado = {false},
 inserir = {false},
 title = {Is effectiveness sufficient to choose an intervention?: Considering resource use in empirical software engineering},
 year = {2016}
}

@article{1274,
 abstract = {Exploratory testing is neither black nor white, but rather a continuum of exploration exists. In this research we propose an approach for decision support helping practitioners to distribute time between different degrees of exploratory testing on that continuum. To make the continuum manageable, five levels have been defined: freestyle testing, high, medium and low degrees of exploration, and scripted testing. The decision support approach is based on the repertory grid technique. The approach has been used in one company. The method for data collection was focus groups. The results showed that the proposed approach aids practitioners in the reflection of what exploratory testing levels to use, and aligns their understanding for priorities of decision criteria and the performance of exploratory testing levels in their contexts. The findings also showed that the participating company, which is currently conducting mostly scripted testing, should spend more time on testing using higher degrees of exploration in comparison to scripted testing.},
 duplicado = {false},
 inserir = {false},
 title = {A Decision Support Method for Recommending Degrees of Exploration in Exploratory Testing},
 year = {2017}
}

@article{1275,
 abstract = {Ontology patterns have been pointed out as a promising approach for ontology engineering. The goal of this paper is to clarify concepts and the terminology used in Ontology Engineering to talk about the notion of ontology patterns taking into account already well-established notions of patterns in Software Engineering.},
 duplicado = {false},
 inserir = {false},
 title = {Ontology patterns: clarifying concepts and terminology},
 year = {2013}
}

@article{1276,
 abstract = {Abstract. This paper presents the version 2.0 of SABiO - a Systematic Approach
for Building Ontologies. SABiO focus on the development of domain
ontologies, and propose also support and management processes, which are
strongly linked to the development process. SABiO distinguishes between reference
and operational ontologies, providing activities that apply to the development
of both types of domain ontologies},
 duplicado = {false},
 inserir = {false},
 title = {SABiO: Systematic Approach for Building Ontologies},
 year = {2014}
}

@article{1277,
 abstract = {Abstract. Currently, most requirements documents are prepared using desktop
text editors. These documents are intended to be used by human readers. In this
paper, we discuss the use of semantic annotations in requirements documents,
in order to make information regarding links between requirements and other
software artifacts, such as other requirements, use cases, classes and test cases,
interpretable by computers. To do that, we extend a semantic document management
platform to the requirements domain, and explore the conceptualization
established by the Software Requirements Reference Ontology in order to
provide features to support some activities of the Requirement Engineering
Process, namely: prioritizing requirements, analyzing impacts from requirements
changes, tracing requirements through traceability matrices, and verifying
requirements using checklists.},
 duplicado = {false},
 inserir = {false},
 title = {Semantic Documentation in Requirements Engineering},
 year = {2014}
}

@article{1279,
 abstract = {Abstract
Software Engineering (SE) is a wide domain, where ontologies are useful instruments for dealing with Knowledge Management (KM) related problems. When SE ontologies are built and used in isolation, some problems remain, in particular those related to knowledge integration. The goal of this paper is to provide an integrated solution for better dealing with KM-related problems in SE by means of a Software Engineering Ontology Network (SEON). SEON is designed with mechanisms for easing the development and integration of SE domain ontologies. The current version of SEON includes core ontologies for software and software processes, as well as domain ontologies for the main technical software engineering subdomains, namely requirements, design, coding and testing. We discuss the development of SEON and some of its envisioned applications related to KM.},
 duplicado = {false},
 inserir = {false},
 title = {SEON: A Software Engineering Ontology Network},
 year = {2016}
}

@article{1280,
 abstract = {Abstract
Motivated by the assumption that Semantic Web technologies are not sufficiently leveraged in the Software Engineering discipline (SE) to provide support regarding the standardization of software development processes by means of international software standards, we investigate the existence of systematic literature reviews in this regard. We concluded that none of the available reviews is specifically focused on analysing international standard-based SE approaches, but on investigating SE approaches in a general way. In this paper, we present the details about all the stages in the conducting of a systematic literature review on the Semantic Web technologies-based support for the standardization of the SE discipline regarding software development processes; one of the major findings of the presented review is that nowadays there is a shortage of approaches providing support for the standardization of software development processes for small and very small software companies.},
 duplicado = {false},
 inserir = {false},
 title = {Towards Supporting International Standard-Based Software Engineering Approaches Using Semantic Web Technologies: A Systematic Literature Review},
 year = {2016}
}

@article{1282,
 abstract = {Abstract Software testing is an important activity in quality
assurance and it generates large amount of knowledge. Software
testers need to gather domain knowledge to be able to
successfully conduct a software testing activity. Not having a
proper knowledge base within its own context by software testing
environments cause software testers to query limited knowledge
available or consult peer software testers, which would greatly
impact on their decision-making process. Ontologies emerge as
one of the more appropriate knowledge management tools for
supporting knowledge representation, processing, storage and
retrieval. Given great importance to knowledge for software
testing, and the potential benefits of managing software testing
knowledge, using semantic web technologies, ontology based
knowledge management system is developed. A Software testing
knowledge sharing ontology is designed to describe software testing
domain knowledge. SPARQL is used as the query language
to retrieve software testing knowledge from the semantic storage.
Both Ontology experts and non-experts evaluated the developed
ontology. We believe our software testing ontology can support
other software organizations to improve the sharing of knowledge
and learning practices.},
 duplicado = {false},
 inserir = {true},
 title = {An Ontology-based Knowledge Management System for Software Testing},
 year = {2017}
}

@article{1293,
 abstract = {Abstract:
Despite all the efforts to reduce the cost of the testing phase in software development, it is still one of the most expensive phases. In order to continue to minimize those costs, in this paper, we propose a Domain-Specific Language (DSL), built on top of MetaEdit+ language workbench, to model performance testing for web applications. Our DSL, called Canopus, was developed in the context of a collaboration1 between our university and a Technology Development Laboratory (TDL) from an Information Technology (IT) company. We present, in this paper, the Canopus metamodels, its domain analysis, a process that integrates Canopus to Model-Based Performance Testing, and applied it to an industrial case study.},
 duplicado = {false},
 inserir = {false},
 title = {Canopus: A Domain-Specific Language for Modeling Performance Testing},
 year = {2016}
}

@article{1297,
 abstract = {Abstract. There are several areas of computer science whose main point of
study is the performance of computer systems, whether for the construction of
software with performance as to estimate the performance of systems. Examples
of such areas are: Performance Evaluation, Software Performance Engineering,
Software Performance Testing and Experimental Studies. However, there is no
work in the literature that unifies or even lists the concepts and methodologies
they raised. In this context, this paper describes how these areas can be related,
and this study will provide the basis for the definition of an ontology.},
 duplicado = {false},
 inserir = {false},
 title = {Relacionando Conceitos de Areas de Estudo de Desempenho da Ciencia da Computacao},
 year = {2015}
}

@article{1298,
 abstract = {Abstract Performance is a fundamental quality of software
systems. The focus of performance testing is to reveal bottlenecks
or lack of scalability of a system or an environment. However,
usually the software development cycle does not include this effort
on the early development phases, which leads to a weak elicitation
process of performance requirements. One way to mitigate that
is to include performance requirements in the system models.
This can be achieved by using Model-Based Testing (MBT) since
it enables to aggregate testing information in the system model
since the early stages of the software development cycle. This
also allows to automate the generation of test artifacts, such as
test cases or test scripts, and improves communication among
different teams. In this paper, we present a set of requirements
for developing a Domain-Specific Language (DSL) for modeling
performance testing of Web applications. In addition, we present
our design decisions in creating a solution that meets the specific
needs of a partner company. We believe that these decisions help
in building a body of knowledge that can be reused in different
settings that share similar requirements.},
 duplicado = {false},
 inserir = {false},
 title = {A Domain-Specific Language for Modeling Performance Testing},
 year = {2016}
}

@article{1301,
 abstract = {Large-scale system testing is challenging. It usually requires large number of test cases, substantial resources, and geographical distributed usage scenarios. It is expensive to build the test environment and to achieve certain level of test confidence. To address the challenges, test systems need to be scalable in a cost-effective manner. TaaS (Testing-as-a-Service) promotes a Cloud-based testing architecture to provide online testing services following a pay-per-use business model. The paper introduces the research and implementation of a TaaS system called Vee@Cloud. It serves as a scalable virtual test lab built upon Cloud infrastructure services. The resource manager allocates Virtual Machine instances and deploy test tasks, from a pool of available resources across different Clouds. The workload generator simulates various workload patterns, especially for system with new architecture styles like Web 2.0 and big data processing. Vee@Cloud promotes continuous monitoring and evaluating of online services. The monitor collects real-time performance data and analyzes the data against SLA (Service Level Agreement). A proof-of-concept prototype system is built and some early experiments are exercised using public Cloud services.},
 duplicado = {false},
 inserir = {false},
 title = {Vee@Cloud: the virtual test lab on the cloud},
 year = {2013}
}

@article{1302,
 abstract = {Cyber-physical systems like active safety systems in recent vehicles are significantly driven by software and rely predominantly on data that is perceived by cameras, laser scanners, and the like from the system's environment. For example, these sensor-based systems realize pedestrian protection function-alities, which cannot be tested under simplified conditions on proving grounds only or by arbitrary test-runs on public roads anymore. Instead, simulative environments are used nowadays, which provide the virtual surroundings for such a system where its real input sources are replaced with simplified sensor models. Thus, interactive and hazard-free system tests and automated system evaluations can be carried out easily. However, the simple strategy to run all available modeled traffic scenarios in the simulation on any change of the implementation would consume too much computation time to provide effective and fast feedback for developers. In this article, an improved strategy for selecting scenarios that shall be run in a simulation based on run-time control-flow analysis is proposed, which resulted from the in-depth analysis of the revision history of the source code and their accompanying simulations for two self-driving vehicles. The outlined strategy is evaluated on a self-driving miniature vehicle.},
 duplicado = {false},
 inserir = {false},
 title = {Improving scenario selection for simulations by run-time control-flow analysis},
 year = {2013}
}

@article{1304,
 abstract = {Abstract:
According to Capability Maturity Model Integration (CMMI), the purpose of Process and Product Quality Assurance (PPQA) is to provide staff and management with objective insights about processes and their associated work products. Such purpose is usually achieved through the implementation of software inspections. Although software inspection be a common practice, it is time-consuming and expensive, which turns the implementation in small and medium-sized teams infeasible. To improve the software inspection, this paper proposes a domain ontology for representing the concepts of quality assurance inspection, which is independent, extensive, shareable and semantically strong. Through the ontology it is possible to provide a formal structure to support the development of software engineering solutions with quality. To support the quality assurance inspections, we developed an agent-based prototype that encapsulates the ontology model. The prototype is able to generate inspection checklists and automatically allocate noncompliance issues. We validated the approach through a case study that shows an increase of inspection coverage and adherence of process.},
 duplicado = {false},
 inserir = {false},
 title = {OntoQAI: An Ontology to Support Quality Assurance Inspections},
 year = {2015}
}

@article{1305,
 abstract = {Abstract
Cloud computing introduces a new paradigm for software deployment, hosting, and service renting. Based on the XaaS architecture, a large number of users may share computing resources, platform services, and application software in a multi-tenancy approach. To ensure service availability, the system needs to support an advanced level of massive scalability so that it can provide necessary resources on demand following the pay-per-use pricing model. This chapter analyzes the unique requirements of cloud performance and scalability, compared with traditional distributed systems. Measurements are proposed with performance indicators, meters, and metrics identified from different perspectives. To support scalability testing in a dynamic environment, an agent-based testing framework is proposed to facilitate adaptive load generation and simulation using a two-layer control architecture.},
 duplicado = {false},
 inserir = {false},
 title = {Cloud scalability measurement and testing},
 year = {2015}
}

@article{1308,
 abstract = {Abstract:
Web services composition is the act of combining several Web services in order to create a new service. The composition process includes planning service sequence, discovering services, selecting services and executing the plan. During service selection, it is important that the selected service not only meets the functionality of the required tasks, the selected service must also match the behavior of its client. This paper proposes an approach to test the interaction or choreography of the Web service by leveraging on transition rules that represent the behavior of the services. A Finite State Machine representation of the choreography is extracted using elements of the choreography. Test cases are then generated based on the Finite State Machine model using existing test generation approach.},
 duplicado = {false},
 inserir = {false},
 title = {Generating Finite State Machine from WSMO choreography for testing Web services},
 year = {2015}
}

@article{1309,
 abstract = {Abstract:
Cloud Computing is a popular solution that has been used in recent years to cooperate and collaborate among distributed applications over networks. Effective testing to ensure cloud service quality becomes a key for the cloud computing adoption. In this paper, we propose an efficient framework that generates test cases for cloud services based on SOA (Service-Oriented Architecture). This approach uses source code comments to create WSDL-Ss (Web Service Semantics) and upload these WSDL-Ss to third parties, e.g. UDDI (Universal Description Discovery and Integration). In the third parties, the WSDL-Ss are translated to ESGs (Event Sequence Graphs) to model the cloud services. Finally, test cases are created from the ESGs to verify the cloud services' accuracy. A prototype system with a cloud banking is provided to illustrate this framework.},
 duplicado = {false},
 inserir = {false},
 title = {Automatic SaaS test cases generation based on SOA in the cloud service},
 year = {2012}
}

@article{1310,
 abstract = {Abstract:
With the rapid development of Web services, Web service discovery becomes a significant challenge in the matching precision and efficiency. In this paper, we present a novel method in the view of users' requirements for Web Service discovery. We set up the requirement model firstly, so as to present users' requirements in details, then cluster the Web Services due to users' common requirements, and obtain corresponding Web services' QoS values. Thus, users can find their really needed Web services in an ordered list. The case study indicates the effect of our method, namely, in the view of users' requirements, we sort the candidate Services due to their functions and their QoS attributes in descending order.},
 duplicado = {false},
 inserir = {false},
 title = {Web Service Discovery Based on User Requirements},
 year = {2011}
}

@article{1313,
 abstract = {Abstract
The ever-changing business context requires organisations to constantly adapt their motivation and service representations. While there has been work focusing on the relation between the motivation- and service level, very little work has been done in providing machinery for handling (propagating) changes at the motivation level and identifying the resulting impact on the service landscape. In this paper, we propose a novel framework which addresses this problem.},
 duplicado = {false},
 inserir = {false},
 title = {Maintaining Motivation Models (in BMM) in the Context of a (WSDL-S) Service Landscape},
 year = {2012}
}

@article{1314,
 abstract = {Abstract
The use of web services has been growing significantly, with increasingly large numbers of applications being implemented through the web. A difficulty associated with this development is the quality assurance of these services, specifically the challenges encountered when testing the applications - amongst other things, testers may not have access to the source code, and the correctness of the output may not be easily ascertained (known as the oracle problem). Metamorphic testing (MT) has been introduced as a technique to alleviate the oracle problem. MT makes use of properties of the software under test, known as metamorphic relations, and checks whether or not these relations are violated. Since MT does not require source code to generate the metamorphic relations, it is suitable for testing web services-based applications. We have designed an XML-based language representation to facilitate the formalisation of metamorphic relations, the generation of (follow-up) test cases, and the verification of the test results. Based on this, we have also developed a tool to support the automation of MT for web service applications. This tool has been used in an experiment to test web services, the evaluation of which is reported in this paper.},
 duplicado = {false},
 inserir = {false},
 title = {MT4WS: an automated metamorphic testing system for web services},
 year = {2016}
}

@article{1315,
 abstract = {Abstract
Selecting services from those available according to user preferences plays an important role due to the exploding number of services. Current solutions for services selection focus on selecting services based either only on non functional features, on context preferences or profile preferences. This paper discusses an improvement of existing services selection approaches by considering both user context and profile. The ultimate aim is to derive maximum profit from available profile and context information of the user by inferring the most relevant preferences w.r.t his/her contextual profile. Linguistic/fuzzy preference modeling and fuzzy inference based approach are used to achieve efficiently a selection process. Some experiments are conducted to validate our approach.},
 duplicado = {false},
 inserir = {false},
 title = {Multi Matchmaking Approach for Semantic Web Services Selection Based on Fuzzy Inference},
 year = {2014}
}

@article{1317,
 abstract = {A goal model is an important artificat in an organisational context. It encodes organisational intent and serves as a motivation for and justification of system requirements. However, the ever changing business environment requires organisations to constantly reposition and reinvent themselves. This involves adopting new goals or giving up goals that have beocme undesirable or infeasible to realize. In short, it reuqires the goal model to be maintained.
While there is a large body of literature on software maintenance, little has been done to support the maintenance of goal model artifacts. In particular, existing maintenance approaches ignore or do not suffciently take into account the structure of a goal model. This can result in erroneous modifcations. On the other hand, various issues exist when structured goals models, which are typically formalized as AND/OR graphs, need to be maintained. A maintenance approach that supports AND/OR graphs, but does not experience their defciencies, is therefore desirable.
Where goal models are manually maintained, erroneous and sub-optimal modifications are often unavoidable, simply because of human cognitive limits on how much of the space of alternative modifcations we can explore. Algorithmic maintenance functionality has the capability to explore this large space of alternatives for optimal modifcations. Algorithmic maintenance functionality is therefore desirable. In particular, gaps have remained in leveraging the structure of a goal model to provide effcient procedures to compute optimal change options.
As a goal model justifes system requirements, it must ultimately be ensured that goals in the goal model remain fulfilled after a modifcation. This requires the ability to algorithmically reason about the fulllment of goals with both functional and nonfunctional characteristics.
This thesis presents a framework that supports the algorithmic maintenance of formal and hierarchically structured goal models. In particular, a novel goal model formalization is introduced. It is based on AND/OR graphs, but overcomes their defficiencies in a maintenance setting. This thesis further demonstrates how a set of maintenance operators can be used to address a multitude of change requests and how the impact to the system requirements can be assessed. We then show how the key maintenance operator can be cast as a state space search problem and efficiently solved using A* search. Experiments with a prototype implementation demonstrate the feasibility of our approach.},
 duplicado = {false},
 inserir = {false},
 title = {A framework to support the maintenance of formal goal models},
 year = {2013}
}

@article{1324,
 abstract = {Abstract:
Cloud platform provides an infrastructure for resource sharing, software hosting and service delivering in a pay-per-use approach. To test the cloud-based software systems, techniques and tools are necessary to address unique quality concerns of the cloud infrastructure such as massive scalability and dynamic configuration. The tools can also be built on the cloud platform to benefit from virtualized platform and services, massive resources, and parallelized execution. The paper makes a survey of representative approaches and typical tools for cloud testing. It identifies the needs for cloud testing tools including multi-layer testing, SLA-based testing, large scale simulation, and on-demand test environment. To address the needs, it investigates the new architecture and techniques for designing testing tools for the cloud and in the cloud. Tool implementations are surveyed considering different approaches including migrated conventional tools, research tools, commercial tools and facilities like benchmark and testbed. Based on the analysis of state-of-the-art practices, the paper further investigates future trend of testing tool research and development from both capability and usability perspectives.},
 duplicado = {false},
 inserir = {false},
 title = {Cloud testing tools},
 year = {2011}
}

@article{1325,
 abstract = {Abstract:
Cloud computing has emerged as a new computing paradigm that impacts several different research fields, including software testing. Testing cloud applications has its own peculiarities that demand for novel testing methods and tools. On the other hand, cloud computing also facilitates and provides opportunities for the development of more effective and scalable software testing techniques. This paper reports on a systematic survey of published results attained by the synergy of these two research fields. We provide an overview regarding main contributions, trends, gaps, opportunities, challenges and possible research directions. We provide a review of software testing over the cloud literature and categorize the body of work in the field.},
 duplicado = {false},
 inserir = {false},
 title = {A Survey of Software Testing in the Cloud},
 year = {2012}
}

@article{1326,
 abstract = {Abstract
Software-as-a-service (SaaS) has received significant attention recently as one of three principal components of cloud computing, and it often deals with applications that run on top of a platform-as-a-service (PaaS) that in turn runs on top of infrastructure-as-a-service (IaaS). This paper provides an overview of SaaS including its architecture and major technical issues such as customization, multi-tenancy architecture, redundancy and recovery mechanisms, and scalability. Specifically, a SaaS system can have architecture relating to a database-oriented approach, middleware-oriented approach, service-oriented approach, or PaaS-oriented approach. Various SaaS customization strategies can be used from light customization with manual coding to heavy customization where the SaaS system and its underlying PaaS systems are customized together. Multi-tenancy architecture is an important feature of a SaaS and various trade-offs including security isolation, performance, and engineering effort need to be considered. It is important for a SaaS system to have multi-level redundancy and recovery mechanisms, and the SaaS system needs to coordinate these with the underlying PaaS system. Finally, SaaS scalability mechanisms include a multi-level architecture with load balancers, automated data migration, and software design strategies.},
 duplicado = {false},
 inserir = {false},
 title = {Software-as-a-service (SaaS): perspectives and challenges},
 year = {2014}
}

@article{1328,
 abstract = {Abstract:
Software-as-a-Service (SaaS) often adopts multi-tenancy architecture (MTA). However, building a MTA SaaS application requires significant effort, either from scratch or using existing platforms such as Force.com or Google App Engine. This paper introduces EasySaaS - a SaaS development framework designed to simplify SaaS development. EasySaaS provides two alternatives to build a SaaS application. First alternative allows tenants to publish their application specifications with their requirements, as well as test scripts and let the SaaS providers customize their SaaS solutions to meet tenants' requirements. The second alternative allows tenants to compose the application using templates provided in EasySaaS. This framework alleviates the workload of tenant developers, and provides an easy approach for customization according to tenants' requirements in a collaborative manner. Most services in the platform is domain independent as the domain knowledge are stored in ontology to support cross-domain development.},
 duplicado = {false},
 inserir = {false},
 title = {EasySaaS: A SaaS development framework},
 year = {2011}
}

@article{1329,
 abstract = {Abstract:
SaaS (Software-as-a-Service) often uses multi-tenancy architecture (MTA) where tenant developers compose their applications online using the components stored in the SaaS database. Tenant applications need to be tested, and combinatorial testing can be used. While numerous combinatorial testing techniques are available, most of them produce static sequences of test configurations and their goal is often to provide sufficient coverage such as 2-way interaction coverage. But the goal of SaaS testing is to identify those compositions that are faulty for tenant applications. This paper proposes an adaptive test configuration generation algorithm AR (Adaptive Reasoning) that can rapidly identify those faulty combinations so that those faulty combinations cannot be selected by tenant developers for composition. The AR algorithm has been evaluated by both simulation and real experimentation using a MTA SaaS sample running on GAE (Google App Engine). Both the simulation and experiment showed show that the AR algorithm can identify those faulty combinations rapidly. Whenever a new component is submitted to the SaaS database, the AR algorithm can be applied so that any faulty interactions with new components can be identified to continue to support future tenant applications.},
 duplicado = {false},
 inserir = {false},
 title = {Adaptive Fault Detection for Testing Tenant Applications in Multi-tenancy SaaS Systems},
 year = {2013}
}

@article{1330,
 abstract = {Abstract:
With the fast advance of cloud computing, developing software-as-a-service (SaaS) is becoming a hot topic in cloud computing and engineering. Due to new features of SaaS applications, SaaS testing becomes an interesting hot topic for both industry and research communities. Although there are a number of published papers discussing cloud testing, there is a lack of research papers addressing new issues, challenges, and needs in SaaS testing. This paper provides a tutorial to discuss SaaS testing, including its concepts, focuses and objective, test process, test environments, and requirements. Moreover, the paper discusses the special SaaS features, and examines the related issues and challenges and needs in SaaS testing.},
 duplicado = {false},
 inserir = {false},
 title = {SaaS Testing on Clouds - Issues, Challenges and Needs},
 year = {2013}
}

@article{1331,
 abstract = {Abstract:
Multi-tenancy architecture (MTA) is often used in Software-as-a-Service (SaaS) and the central idea is that multiple tenant applications can be developed using components stored in the SaaS infrastructure. Recently, MTA has been extended where a tenant application can have its own sub-tenants as the tenant application acts like a SaaS infrastructure. In other words, MTA is extended to STA (Sub-Tenancy Architecture). In STA, each tenant application not only needs to develop its own functionalities, but also needs to prepare an infrastructure to allow its sub-tenants to develop customized applications. This paper formulates eight models for STA, and discusses their trade-offs including their formal notations and application scenarios.},
 duplicado = {false},
 inserir = {false},
 title = {Multi-tenancy and Sub-tenancy Architecture in Software-as-a-Service (SaaS)},
 year = {2014}
}

@article{1333,
 abstract = {A system fetches consistent datasets in batches for a given period of time and provides the ability to retrieve each batch. Batches of data may be fetched for an interval of time. The present system may fetch new or changed data from different cloud/on-premise applications. It will store this data in the cloud or on-premise to build data history. As the system fetches new data, existing batches of data will not be overwritten. New batches of data are created as new versions so that change history is preserved. Past batches of data for a past time period may be provided to one or more tenants.},
 duplicado = {false},
 inserir = {false},
 title = {SAAS network-based backup system},
 year = {2015}
}

@article{1334,
 abstract = {A metadata management system receives metadata changes and automatically updates a metadata architecture which maps the data. The metadata changes may be received through a simple user interface by a user or administrator. Once received, the system may automatically update schemas and data transformation code to process data according to the new data mapping preference. The system may handle metadata updates in a multi-tenant system having one or more applications per tenant, and may update data for a single tenant and 1 or more tenant applications in a multitenancy.},
 duplicado = {false},
 inserir = {false},
 title = {Metadata manager for analytics system},
 year = {2016}
}

@article{1335,
 abstract = {Abstract:
SaaS (Software-as-a-Service) as a part of cloud computing is a new approach for software construction, evolution, and delivery. This paper proposes HLA-based SaaS-oriented simulation frameworks where simulation services will be organized into a SaaS framework running in a cloud environment. This SaaS-oriented frameworks can be applied to multiple application domains but illustrated by using HLA (High-Level Architecture). The framework will allow integration of a variety of modules, service-oriented design, flexible customization, multi-granularity simulation, high-performance computing, and system security. It has the potential to reduce system development time, and allows simulation to be run in a cloud environment taking advantages of resources offered by the cloud.},
 duplicado = {false},
 inserir = {false},
 title = {HLA-Based SaaS-Oriented Simulation Frameworks},
 year = {2014}
}

@article{1336,
 abstract = {Abstract:
Web software development and cloud computing based on Service-Oriented Architecture (SOA) and Service-Oriented Computing (SOC) represent the modern software engineering theories, practices, and technologies. As an architecture-driven computing paradigm, SOA and SOC are mature and are becoming the major paradigm for software development. SOA and SOC should be taught in all computer science and computer engineering programs. We do not suggest using SOC to replace the currently taught Object-Oriented Computing (OOC) paradigm. As SOC is based on OOC, we suggest to teaching SOC as the continuation and extension of OOC. At Arizona State University, SOA and SOC paradigm is incorporated into our Computing Science and Software Engineering programs since 2006. This paper presents the topics of the related courses and the open resources created for the courses, which are available for public accesses, including textbooks, lecture presentation slides, tests and assignments, software tools, a repository of sample services and applications, and tutorials of using tools and the cloud computing environment for service hosting and deployment.},
 duplicado = {false},
 inserir = {false},
 title = {Service-orientation in computing curriculum},
 year = {2011}
}

@article{1337,
 abstract = {An elastic parallel database system where metadata is specified out-of-band during database operations via a set of augmentation rules. The rules are used to augment or modify commands received, and indicate whether they are to be used for specific connections on which they are received, for all client connections, or some other scope.},
 duplicado = {false},
 inserir = {false},
 title = {Platform agnostic resource provisioning},
 year = {2014}
}

@article{1338,
 abstract = {Abstract:
We observed SaaS developed application by multi-tenancy to provide flexible customization and introduced many issues in software scalability and dynamic testing. Multi-tenancy demand to customizing the single instance according variability wishes among many customers. For that we proposed variable service process to customization multi-tenancy in runtime. It will be realize all benefits of variability concept for SaaS application. And proposed Feature meta-model to implement graphical editor and define all rules and linkages between elements of service process.},
 duplicado = {false},
 inserir = {false},
 title = {Variable service process by feature meta-model for SaaS application},
 year = {2012}
}

@article{1339,
 abstract = {An elastic parallel database system where metadata is specified out-of-band during database operations via a set of augmentation rules. The rules are used to augment or modify commands received, and indicate whether they are to be used for specific connections on which they are received, for all client connections, or some other scope.},
 duplicado = {false},
 inserir = {false},
 title = {Smart distributed transactions and multiply distributed tables},
 year = {2014}
}

@article{1340,
 abstract = {An elastic parallel database system where metadata is specified out-of-band during database operations via a set of augmentation rules. The rules are used to augment or modify commands received, and indicate whether they are to be used for specific connections on which they are received, for all client connections, or some other scope.},
 duplicado = {false},
 inserir = {false},
 title = {Dynamic rules based query rewriting engine},
 year = {2014}
}

@article{1341,
 abstract = {An elastic parallel database system where metadata is specified out-of-band during database operations via a set of augmentation rules. The rules are used to augment or modify commands received, and indicate whether they are to be used for specific connections on which they are received, for all client connections, or some other scope.},
 duplicado = {false},
 inserir = {false},
 title = {Out-of-band specification of metadata during database operations},
 year = {2014}
}

@article{1342,
 abstract = {An extract-transform-load (ETL) platform fetches consistent datasets in a batch for a given period of time and provides the ability to rollback that batch. The batch may be fetched for an interval of time, and the ETL platform may fetch new or changed data from different cloud/on-premise applications. It will store this data in the cloud or on-premise to build data history. As the ETL platform fetches new data, the system will not overwrite existing data, but rather will create new versions so that change history is preserved. For any reason, if businesses would like to rollback data, they could rollback to any previous batch.},
 duplicado = {false},
 inserir = {false},
 title = {Data consistency and rollback for cloud analytics},
 year = {2015}
}

@article{1343,
 abstract = {Most companies have adopted cloud solutions to deploy their application services for earning more profits or
reducing operating costs. Unfortunately, most buyers of cloud solutions have no ideas how to make the best decision
for purchasing a cloud solution. The intention of this paper is to propose some assessed criteria from the IaaS point
of view. We then translate the suggested criteria to practical issues from the functions and the performance points of
view. Finally, we compare the satisfactions and the performance of VMware, Microsoft Hyper-V and CitrixXen in
the issues. All buyers of cloud solutions can refer to our results.},
 duplicado = {false},
 inserir = {false},
 title = {ASSESSMENT CRITERIA OF IAAS SOLUTION IN CLOUD COMPUTING},
 year = {2013}
}

@article{1344,
 abstract = {An elastic parallel database system where metadata is specified out-of-band during database operations via a set of augmentation rules. The rules are used to augment or modify commands received, and indicate whether they are to be used for specific connections on which they are received, for all client connections, or some other scope.},
 duplicado = {false},
 inserir = {false},
 title = {Redistribution reduction in EPRDBMS},
 year = {2014}
}

@article{1345,
 abstract = {An elastic parallel database system where data distribution is container- and container-context based. Container Based Tables are defined and Container Member Tables achieve co-location of data as needed. A polymorphic key may also establish polymorphic key relationships between rows in one table and rows in many other possible tables.},
 duplicado = {false},
 inserir = {false},
 title = {Advancements in data distribution methods and referential integrity},
 year = {2015}
}

@article{1346,
 abstract = {Abstract
Cloud computing is an emerging technology that promises competitive advantages, cost savings, enhanced business processes and services, and various other benefits to enterprises. Despite the rapid technological advancement, the adoption of cloud computing is still growing slowly among small and mediumsized enterprises (SMEs). This paper presents a model to support the decisionmaking process, using a multi-criteria decision method PAPRIKA for the socio-technical aspects influencing SMEs cloud adoption decision. Due to the multifaceted nature of the cloud computing adoption process, the evaluation of various cloud services and deployment models have become a major challenge. This paper presents a systematic approach to evaluating cloud computing services and deployment models. Subsequently, we have conducted conjoint analysis activities with five SMEs decision makers as part of the distribution process of this decision modelling based on predetermined criteria. With the help of the proposed model, cloud services and deployment models can be ranked and selected.},
 duplicado = {false},
 inserir = {false},
 title = {Cloud computing adoption decision modelling for SMEs: a conjoint analysis},
 year = {2016}
}

@article{1347,
 abstract = {Abstract: To meet the required functional and nonfunctional requirements of different enterprise applications
it is important to model the possible design so that a feasible alternative can be defined. We observed SaaS
developed application by multi-tenancy to provide flexible customization and introduced many issues in
software scalability and dynamic testing. Multi-tenancy demand to customizing the single instance according
variability wishes among many customers. For that we proposed variable service process to customization
multi-tenancy in runtime. It will be realize all benefits of variability concept for SaaS application.},
 duplicado = {false},
 inserir = {false},
 title = {Variable Service Process for SaaS Application},
 year = {2012}
}

@article{1350,
 abstract = {Context: The use of a Service Oriented Architecture (SOA) offers a new and distinct approach to creating software based applications (SBAs) around the idea of integrating distributed autonomous computing resources. A widely available realisation of an SOA exists in the form of web services. However, to date no standard techniques have emerged for developing SBAs. There is also a lack of consistency in describing the concept itself, and the published literature offers little evidence derived from the experience of developing `real world examples.

Aims: The objective of the work described in this thesis was to conduct a series of studies to explore systematically the concept of what constitutes an SOA by using the published literature, to employ this to construct a proof of concept SOA design model based on a real world problem, and in doing so, to investigate how well existing
design notations are able to support this architectural style.

Method: The research described in this thesis has been conducted in an evolutionary manner by employing a range of empirical methods. A mapping study was performed to investigate how the concept of SOA is interpreted by the research community. Based upon this model of SOA, a participant-observer case study was employed to construct an SOA design model and a use case model for an energy engineering application to demonstrate use for a real world problem. Finally, expert knowledge was employed for evaluation of the case study through the use of walkthroughs.

Results: From the mapping study we created an integrated model of what constitutes an SOA for the use with the case study. The case study outcomes include a design for a renewable energy control system together with codified experience of constructing and recording the SOA design model. The experience of employing the walkthrough
method for evaluation, and the outcomes of the evaluation are also discussed.

Conclusion: From this research we conclude that the SOA research community needs to develop a clearer shared understanding and agreement on the model of what constitutes an SOA and the vocabulary used to describe the SOA concept. This will aid designers to communicate their mental models more effectively and will provide the
semantics needed for devising the new notations that this study implies are needed for SBA design. Further, some lessons about SBA design have been derived from the case study experiences.},
 duplicado = {false},
 inserir = {false},
 title = {Design Models for Service-based Software Application},
 year = {2013}
}

@article{1351,
 abstract = {Different from traditional enterprise applications that rely on the infrastructure
and services provided and controlled within an enterprise, cloud computing is
based on services that are hosted on providers over the Internet. Hereby, services
are fully managed by the provider, whereas consumers can acquire the required
amount of services on demand, use applications without installation and access
their personal files through any computer with internet access. Recently, a
growing interest in cloud computing can be observed thanks to the significant
developments in virtualization and distributed computing, as well as improved
access to high-speed Internet and the need for economical optimization of
resources.
An important category of cloud computing is the software as a service domain in
which software applications are provided over the cloud. In general when
describing SaaS, no specific application architecture is prescribed but rather the
general components and structure is defined. Based on the provided reference
SaaS architecture different application SaaS architectures can be derived each of
which will typically perform differently with respect to different quality factors.
An important quality factor in designing SaaS architectures is scalability.
Scalability is the ability of a system to handle a growing amount of work in a
capable manner or its ability to be enlarged to accommodate that growth. In this
thesis we provide a systematic modeling and design approach for designing
scalable SaaS architectures.
To identify the aspects that impact the scalability of SaaS based systems we have
conducted a systematic literature review in which we have identified and analyzed
the relevant primary studies that discuss scalability of SaaS systems. Our study
has yielded the aspects that need to be considered when designing scalable
systems. Our research has continued in two subsequent directions. Firstly, we
have defined a UML profile for supporting the modeling of scalable SaaS
architectures. The profile has been defined in accordance with the existing
practices on defining and documenting profiles. Secondly, we provide the socalled
architecture design perspective for designing scalable SaaS systems.
Architectural Perspectives are a collection of activities, tactics and guidelines to
modify a set of existing views, to document and analyze quality properties.
Architectural perspectives as such are basically guidelines that work on multiple
views together. So far architecture perspectives have been defined for several
quality factors such as for performance, reuse and security. However, an
architecture perspective dedicated for designing scalable SaaS systems has not
been defined explicitly. The architecture perspective that we have defined
considers the scalability aspects derived from the systematic literature review as
well as the architectural design tactics that represent important proved design rules
and practices. Further, the architecture perspective adopts the UML profile for
scalability that we have defined. The scalability perspective is illustrated for the
design of a SaaS architecture for a real industrial case study.},
 duplicado = {false},
 inserir = {false},
 title = {Analysis and design of scalable software as a service architecture},
 year = {2015}
}

@article{1352,
 abstract = {Abstract:
The EMFIS model allows companies to explicate a representation of the organization's current situation regarding continuous integration impediments, and visualizes what the organization must focus on in order to enable more frequent integration of software. The model is used to perform an assessment of twelve factors, where the ratings from participants representing the developers are summarized separately from ratings from participants representing the enablers (responsible for processes, development tools, test environments etc.). The EMFIS model is based on semi-structured interviews with 20 developers from two companies which develop large-scale software systems, and a literature review that included 74 research papers and four books. The model has been validated in workshops and interviews, which in total included 28 individuals in five different companies. The model was well received during the validation, and was appreciated for its simplicity and its ability to show differences in rating between developers and enablers.},
 duplicado = {false},
 inserir = {false},
 title = {The EMFIS Model Enable More Frequent Integration of Software},
 year = {2017}
}

@article{1354,
 abstract = {An elastic parallel database system where metadata is specified out-of-band during database operations via a set of augmentation rules. The rules are used to augment or modify commands received, and indicate whether they are to be used for specific connections on which they are received, for all client connections, or some other scope.},
 duplicado = {false},
 inserir = {false},
 title = {Extensions to generational data distribution methods},
 year = {2015}
}

@article{1355,
 abstract = {Abstract:
Context: Software has become more complicated, dynamic, and asynchronous than ever, making testing more challenging. With the increasing interest in the development of cloud computing, and increasing demand for cloud-based services, it has become essential to systematically review the research in the area of software testing in the context of cloud environments. Objective: The purpose of this systematic mapping study is to provide an overview of the empirical research in the area of software cloud-based testing, in order to build a classification scheme. We investigate functional and non-functional testing methods, the application of these methods, and the purpose of testing using these methods. Method: We searched for electronically available papers in order to find relevant literature and to extract and analyze data about the methods used. Result: We identified 69 primary studies reported in 75 research papers published in academic journals, conferences, and edited books. Conclusion: We found that only a minority of the studies combine rigorous statistical analysis with quantitative results. The majority of the considered studies present early results, using a single experiment to evaluate their proposed solution.},
 duplicado = {false},
 inserir = {false},
 title = {A Systematic Mapping Study of Empirical Studies on Software Cloud Testing Methods},
 year = {2017}
}

@article{1357,
 abstract = {The present system receives streaming raw data and inserts context into the raw data. The context raw data may be partitioned into sub-batches and transmitted to a data receiver and persister. The raw data may include context information as well as child-parent information to assist with persisting data. The context may be used to place the data in buckets without analysis of the data, thereby saving time and resources while storing the data batches.},
 duplicado = {false},
 inserir = {false},
 title = {Cloud-based streaming data receiver and persister},
 year = {2017}
}

@article{1358,
 abstract = {Abstract
Software-as-a-Service (SaaS) is a new research orientation for developing software, and has multi-tenancy architecture and customization features, which are very suitable for performance and benchmark test of OLTP transactions. And Bank Intermediary Business (BIB) is the most important business of Bank financial system. This paper focuses on establishing the SaaS-based BIB performance and benchmark architecture and proposes the SaaS-based BIB Database Model (SaaS-BIB-DM), the architecture layer (SaaS-BIB-AL), the data flow view (SaaS-BIB-DF) and the representative transaction model (SaaS-BIB-TM). The database is further extended with the SaaS hybrid two-layer partition methodology and the performance is proved to be better than that in three-tier C/S architecture. And the specific SaaS-based BIB architecture which we proposed is 4-level SaaS-based architecture. Based on the analysis the state-of-art of BIB and SaaS, the paper further investigates future trend of SaaS-based performance testing architecture and benchmark.},
 duplicado = {false},
 inserir = {false},
 title = {A SaaS-based software modeling for bank intermediary business},
 year = {2015}
}

@article{1359,
 abstract = {Abstract:
As a popular technique in cloud computing, multi-tenancy (MT) can significantly ease software maintenance, and improve resource utilization. To make use of the MT technique, an application may need to be transformed to be MT-enabled. This process involves finding and processing a special kind of data entities named global isolation points (GIPs). Practically, finding all GIPs of an application is challenging. Traditional method involves manually browsing the application code, requiring a great deal of human effort. To solve this problem, we introduce a toolkit named Auto-MT to help find and process GIPs of an application. Auto-MT is able to find new GIPs based on their relations to known GIPs. To characterize the relation, a novel graph called value flow graph (VFG) is introduced, which models the value flows of data entities. It can also be used in other scenarios, like taint analysis. We have implemented Auto-MT as an Eclipse Plug-in, and applied it to transform Roller, a widely used Java application. Experimental results show that Auto-MT saves substantial human effort, and accelerates the process of transforming applications to be MT-enabled.},
 duplicado = {false},
 inserir = {false},
 title = {A Semi-Automatic Approach of Transforming Applications to be Multi-Tenancy Enabled},
 year = {2014}
}

@article{1360,
 abstract = {Abstract
ENGELSK: One of the issues in the cloud today is the switching between different service providers.
Cloud providers often deliver their services through specific platforms(API), with specific
tools, customized and special file formats that could cause dependencies. The cloud customer
could potentially become dependent of the cloud provider. This is called a provider
"lock-in", and could potentially hinder the migration process, the further development
of the cloud technology, and the enabling of cloud computing technology for businesses
and organizations.
We are attempting to improve security, avoid the "lock-in" problem, and ensure compliance
in cloud computing and cloud environments through the improvement of business
resilience and business continuity. Security, compliance, business continuity and
business resilience are all aspects that influence trust between actors in a cloud computing
environment. The trust issue hinders a lot of different organizations and business to
take the step into the cloud environment. This issue of trust, along others, creates uncertainty
for organizations and businesses considering moving their storage, applications
or services into the cloud. This thesis aims to solve or improve this particular issue, and
with it, enable more secure cloud computing.
The thesis results and findings aims to help organizations and businesses with the
planning process, to help keeping their data secure, and successfully migrate/swap providers.
The results and findings are concluded upon two different case studies of two
cloud customers. The findings from these case studies are presented through security
checklists, lists of SLA/Sec-SLA metrics and through a modification of a conceptual
model.
A solution or improvement of these issues will enhance trust in cloud computing and
enable more organizations and businesses to enter the cloud, utilize its efficiency and
benefit from its low cost service on demand.},
 duplicado = {false},
 inserir = {false},
 title = {Towards a compliant and secure cloud: cloud migration, swapping providers and contractual aspects},
 year = {2012}
}

@article{1361,
 abstract = {Risk management is crucial for business software developers. In order to minimize the risk of choosing the wrong multitenancy implementation pattern for a software project, four case studies were conducted that resulted in four architectural patterns. Two of the patterns were derived from interviews while the other two from literature research. For each pattern, their advantages, their liabilities and possible solutions to mitigate them were determined. To help business software developers to choose the most suitable multitenancy implementation patterns, this work provides an overview of the four implementation patterns. The overview consists of a number of multitenancy specific problems and offers solutions on how the patterns can solve these problems, and it also consists of a comparison of the four implementation patterns by showing their consequences and linking them to possible mitigation solutions.},
 duplicado = {false},
 inserir = {false},
 title = {Enabling multitenant software solutions for online business software},
 year = {2012}
}

@article{1362,
 abstract = {Abstract Background: SaaS (Software as a Service) is a services
delivery model in Cloud Computing whose applications are remotely
hosted by the service provider and available to customers
on demand over the Internet. Multi-tenant Architecture (MTA) is
an organizational pattern for SaaS that enables a single instance
of an application to be hosted on the same hardware and accessed
by multiple customers, so-called tenants, with the aim of lowering
costs. Tenants are able to configure the system according to their
particular needs. Objective: This research aims at the obtaining
an overview of the challenges and research opportunities in MTA
context for SaaS through a Systematic Mapping Study. Results:
Eighty nine primary studies were selected for discussions on
advances and opportunities for further investigations. The results
showed the relevancy of MTA and pointed out the main research
trends for next years in this topic.},
 duplicado = {false},
 inserir = {false},
 title = {A Systematic Mapping Study on the Multi-tenant Architecture of SaaS Systems},
 year = {2016}
}

@article{1365,
 abstract = {Web service is a widely used implementation technique under the paradigm of Service-Oriented Architecture (SOA). A service-based system is subjected to continuous evolution and regression testing is required to check whether new faults have been introduced. Based on the current scientific work of web service regression testing, this survey aims to identify gaps in current research and suggests some promising areas for further study. To this end, we performed a broad automatic search on publications in the selected electronic databases published from 2000 to 2013. Through our careful review and manual screening, a total of 30 papers have been selected as primary studies for answering our research questions. We presented a qualitative analysis of the findings, including stakeholders, challenges, standards, techniques, and validations employed in these primary studies. Our main results include the following: (1) Service integrator is the key stakeholder that largely impacts how regression testing is performed. (2) Challenges of cost and autonomy issues have been studied heavily. However, more emphasis should be put on the other challenges, such as test timing, dynamics, privacy, quota constraints, and concurrency issues. (3) Orchestration-based services have been largely studied, while little attention has been paid to either choreography-based services or semantic-based services. (4) An appreciable amount of web service regression testing techniques have been proposed, including 48 test case prioritization techniques, 10 test selection techniques, two test suite minimization techniques, and another collaborative technique. (5) Many regression test techniques have not been theoretically proven or experimentally analyzed, which limits their application in large-scale systems. We believe that our survey has identified gaps in current research work and reveals new insights for the future work.},
 duplicado = {false},
 inserir = {false},
 title = {Regression Testing of Web Service: A Systematic Mapping Study},
 year = {2014}
}

@article{1366,
 abstract = {Abstract:
Cloud computing works on various service models like SaaS, PaaS, IaaS. The enterprises outsource data and computation to cloud and benefit from cloud computing unique attributes like abundant storage, network and compute. This paradigm also brings forth many challenges for data security and access control. Many organizations are choosing Cloud platforms for software development/testing, particularly teams based on agile software development need faster software integration. As cloud turns out to be cost-effective and performance intensive it remains as obvious choice for organizations. In this paper we provide an efficient model for software testing which leverages cloud environment and supports rigorous requirements of agile development model viz., continuous build integration and we provide an approach to run automation testcases easily (particularly User Interface automation testcases). The regression tests developed as part of software lifecycle can be executed either on-demand or selectively based on tags provided. The proposed model scales well to accommodate any number of testcases can be run flexibly. Surprisingly our model input is a single CSV (comma separated file) with test case information. The proposed model takes care of automatic segregation of the testcases into test suites and then executes them individually either parallel or serially based on configuration. We rely on Jenkins tool (an open source build management tool) for creating required jobs and scheduling. The proposed model of Tests execution is extremely useful in high demanding agile environments where feature development is many times faster than traditional water fall model. The approach also takes advantage of virtualized environment for tests execution compared to traditional hardware.},
 duplicado = {false},
 inserir = {false},
 title = {Highly scalable model for tests execution in cloud environments},
 year = {2012}
}

@article{1367,
 abstract = {Abstract
This paper presents the approach to functional test automation of services (black-box testing) and service architectures (grey-box testing) that has been developed within the MIDAS project and is accessible on the MIDAS SaaS. In particular, the algorithms and techniques adopted for addressing input and oracle generation, dynamic scheduling, and session planning issues supporting service functional test automation are illustrated. More specifically, the paper details: (i) the test input generation based on formal methods and temporal logic specifications, (ii) the test oracle generation based on service formal specifications, (iii) the dynamic scheduling of test cases based on probabilistic graphical reasoning, and (iv) the reactive, evidence-based planning of test sessions with on-the-fly generation of new test cases. Finally, the utilisation of the MIDAS prototype for the functional test of operational services and service architectures in the healthcare industry is reported and assessed. A planned evolution of the technology deals with the testing and troubleshooting of distributed systems that integrate connected objects.},
 duplicado = {false},
 inserir = {false},
 title = {Automation and intelligent scheduling of distributed system functional testing},
 year = {2017}
}

@article{1368,
 abstract = {Application of function characteristics generates test cases that bring internal information of functions. By reusing the generated test cases, integration testing can consider internal information of functions. KEY WORDS Complicated Medical Software, Required Interface, Abstraction of Function Behavior, Function Behavior Reduction, Test Case Reuse, Integration Testing. Integration testing usually loses internal information of components [3]. Unit testing called white-box testing is used to test a component and can examine execution path of a component in detail [4]. Integration testing called black-box testing does not treat an internal behavior of a component because test paths are increased in geometric progression if components are integrated [5]. Therefore, test cases cannot be defined to strictly test components used by other components. For example, suppose that a caller function invokes a callee function, where the callee function has already passed testing. When a developer tests the caller function, the developer ignores internal execution paths of the callee function and treats the callee function as one statement although the callee function has many execution paths inside. We propose a method to generate test cases considering internal characteristics of callee functions. Abstraction of callee functions generates test cases which reflect characteristics of the callee functions conforming with each of test coverage criteria. The generated test cases for the callee functions provide the characteristics of the callee functions with caller functions. Test cases for the caller functions can be generated considering the characteristics of the callee functions by reuse of the generated test cases for the callee functions. The rest of the paper is organized as follows. Test cases reflecting characteristics of callee functions are described by an example in Section 2. A method to characterize callee functions is presented in Section 3. Testing using characteristics of callee functions is described in Section 4. A case study is presented in Section 5. Finally, a conclusion and future work are given in Section 6. TEST CASE GENERATION FOR INTEGRATING MEDICAL SYSTEMS CONSIDERING FUNCTION CHARACTERISTICS Youngsul Shin, Muhammad I. Hossain, Woo Jin Lee Abstract A medical system is complicated because it has many sub-modules and is connected with many kinds of other systems. Because software faults tend to occur in interoperation among modules, a safety-critical medical system should be tested strictly in terms of integration. At integration testing phase, there are too many execution path of software to test, that makes testing difficult. To deal with the problem, existing test approaches ignore complicated internal information of modules at integration testing phase. However, the existing approach may not find faults of interoperation among modules because they cannot define test cases that cover many execution paths. Our paper proposes a method to test interoperation among modules by abstraction of function characteristics and reuse of test cases. Abstraction of function characteristics generates test cases that bring internal information of functions. By reusing the generated test cases, integration testing can consider internal information of functions.},
 duplicado = {false},
 inserir = {false},
 title = {TEST CASE GENERATION FOR INTEGRATING MEDICAL SYSTEMS CONSIDERING FUNCTION CHARACTERISTICS},
 year = {2012}
}

@article{1369,
 abstract = {Abstract:
Software development life cycle consists of various stages like requirements, design, implementation, testing and maintenance. In a typical enterprise level project number of testcases to be executed for each release would be in thousands. One known way of segregating the testcases is using component or module names and one can also classify them based on level of importance like unit testing, functional, sanity etc. With iterative development models like Agile, the release cycle is short and it is prudent to execute the testcase selectively majorly concentrating on impacting components. In this paper we provide a generic way to manage the tests and provide an efficient mechanism to run the tests selectively. In our approach we make use of well-known build management tool called Jenkins for running the tests on-demand. One unique benefit of our approach is that the input is a single comma-separated-value (CSV) file and it is very easy to add/modify existing tests. The tests are tagged using well known keywords (viz. database-layer, configuration, regression). When some particular tests need to be run, the user enters `Tag' in the job parameter and testsuite will be generated dynamically. In this paper we also describe an end-to-end test management system that supports running selective tests with help of jobs created in Jenkins environment. The proposed model of Tests execution is highly useful in high demanding environments like agile software development model, Test driven development model where feature development is many times faster than traditional water fall model. The approach suggested in this paper makes optimum use of cloud resources by distributing the jobs, so this can be utilized specially for testing under virtualized environment.},
 duplicado = {false},
 inserir = {false},
 title = {Mechanism for on demand Tag-Based software testing in virtualized environments},
 year = {2012}
}

@article{1370,
 abstract = {Abstract:
Continuous Integration (CI) is the most common practice among software developers where they integrate their work into a baseline frequently. The industry is facing huge challenges while developing Software (S/W)s at multiple sites and tested on multiple platforms. The best way to make CI faster and more efficient is to automate the build and testing process. Jenkins is a CI tool that helps in automating the complete process, reducing the work of a developer and check the development at each and every step of S/W evolution. In this paper, we discuss the implementation of Jenkins for software patch integration and release to client. We consider a real-life scenario, how the software development is carried out in corporate ventures and how Jenkins can save developers/integrators crucial work hours by automating the complete process. In this paper, Jenkins is implemented in a master/slave architecture where master node is the Jenkins server and slaves are the Jenkins clients. We discuss the usage of various plug-ins available that allow Jenkins to be used with any environment of software development.},
 duplicado = {false},
 inserir = {false},
 title = {ACI (automated Continuous Integration) using Jenkins: Key for successful embedded Software development},
 year = {2015}
}

@article{1371,
 abstract = {Services that represent sensor and actuator nodes, together with service orchestration, aid in overcoming the heterogeneous structure of the Internet of Things (IoT). Interconnecting different sensor and actuator nodes and exposing them as services is a complex topic which is even more demanding for testing. Further effort is needed to enable common and effcient methodologies for testing IoT-based services. IoT-based services differ from web services since they usually interact with the physical environment via sensor and actuator nodes. This changes how testing can be performed. An open research question is thereby how to apply Model-Based Testing (MBT) approaches for facilitating scalable and ef cient test automation. This thesis introduces a novel test framework to facilitate functional evaluation of IoT- based services based on MBT methodologies. The concept separates the service logic from connected sensor and actuator nodes in a sandbox environment. Furthermore, a new IoT service behaviour model is designed for representing relevant characteristics of IoT-based services and ensuring the automated emulation of sensor nodes. The IoT-behaviour model proves to be automatically transformable into executable Test Cases (TCs). As a proof of concept, the automated test approach is prototypically implemented as a novel test tool. The execution of the TCs reveals, that crucial failures, such as unexpected messages, data types, or data values, can be detected during test execution. Deriving tests from a test model typically result in huge number of TCs, which cannot be executed within a reasonable time and with limited resources. To enhance the diversity of executed TCs, similarity investigation algorithms are proposed and validated. The results show that the proposed Diversity-based Steady State Genetic algorithm can outperform existing solutions up to 11.6 % with less computation time. With regard to verifying the failure detection rate, experiments show that the proposed Group Greedy algorithm can enhance the rate up to 29 %.},
 duplicado = {false},
 inserir = {false},
 title = {Service testing for the internet of things},
 year = {2016}
}

@article{1378,
 abstract = {Abstract For the general case, the complete automation
of test data generation is an undecidable problem,
and many researches employ meta-heuristics trying
to find a reasonable partial solution. System testing
performed via graphical user interface (GUI) imposes
extra challenge for automation due to hundreds and
often thousands of possibilities of events that can be
generated. Objective: This work presents a study based
on systematic mapping aiming at identifying the state of
the art and the state of the practice on the automation
of system testing carried out via GUI. Method: We employed
the traditional protocol of mapping study to support
the data collection. Results: The work was carried
out from 6th February 2012 to 1st May 2013 resulting in
the selection of 39 out of 598 primary studies obtained
with the application of the search strings. Some of
these works used, besides functional testing criteria,
structural testing criteria to guide meta-heuristics. In
relation to meta-heuristics, the distribution of work
was more uniform, with a slight majority using Genetic
Algorithms for test data generation. Conclusion: There
are few research groups working on this subject. One
particular author is responsible for authoring more
than 30% of the selected primary studies and can be
considered a reference in the generation of test data
from GUI. Some research problems identified are 1) the
difficult to represent all the possible GUI interactions
without cause state explosion; 2) the need to evaluate
the techniques on large software products; and 3) the
complexity to automate the representation of the GUI
interactions by reducing the number of infeasible sequences
of actions.},
 duplicado = {false},
 inserir = {false},
 title = {Test Data Generation Based on GUI: A Systematic Mapping},
 year = {2014}
}

@article{1379,
 abstract = {The automated graphical user interface (GUI) testing has been
a challenging task. The manual testing done in the GUI
requires huge amount of time, so in order to reduce this time,
many automated GUI testing techniques have been proposed.
The most common way testing with any software is based on
the finite state machine (FSM). It has been used since we have
the software that doesn't have the GUI. So the various efforts
have done to use the same finite state model in the graphical
testing. In this paper, we have presented the survey of the
various GUI testing techniques, mostly that are mostly based
on finite state machines. The various approaches have
described that may range from the combinatorial to artificial
intelligence approach. There is still a gap that to be filled as
the tools are still the part of academic research but not the part
of industries. },
 duplicado = {false},
 inserir = {false},
 title = {A Literature Survey on Finite State Testing of Graphical User Interface},
 year = {2016}
}

@article{1381,
 abstract = {Abstract:
Testing is an important phase during the development cycle of software. Generating test cases is an important activity during this phase. TCG for GUI is a least focused area and the techniques which are being used in Command Line Interface do not fit here. One of the means to formally represent knowledge is through ontology. It represents domain information as a set of concepts and relationship among them. Ontology based GUI testing is a new branch of testing. To validate the framework for generating test case based on ontology an experiment has been conducted.},
 duplicado = {false},
 inserir = {true},
 title = {Validation of ontology based test case generation for graphical user interface},
 year = {2012}
}

@article{1386,
 abstract = {Abstract:
To solve the problem of modeling and formally verifying the composition of cloud manufacturing services, a new modeling and formal verification approach based on Calculus for Orchestration of Web Service (COWS) combined with Unified Modeling Language (UML) was proposed. First UML activity diagram for Cloud Manufacturing service composition was built with MagicDraw software, and saved as an XML file. The XML file was translated into a COWS file via UML4SOA, and formally verified with COWS Model Checking program (CMC). Furthermore, a composite service about part processing was given. Activity diagram of the service composition was built, including milling service, drilling service, exception handling, event handling and compensation mechanism. Finally, the composition verification result showed that the proposed approach was feasible.},
 duplicado = {false},
 inserir = {false},
 title = {Cloud manufacturing service composition modeling and formal verification based on Calculus for Orchestration of Web Service},
 year = {2013}
}

@article{1391,
 abstract = {Abstract
Air pollution has become a striking problem in recent years. When estimating the degree of human exposure to a particular air pollutant, time activity pattern is one of the most important factors, which is able to quantify the time people spend in different micro environments, such as indoor and outdoor. Traditional surveys use the method of questionnaires and telephone calls to explore the time activity pattern. In this paper, we propose a novel method to analyse the time activity pattern by utilising mobile web usage log. We test the method on two datasets covering about four million users. Experiments show that our method achieves an acceptable performance, and can truly measure the time activity pattern of human beings.},
 duplicado = {false},
 inserir = {false},
 title = {Time activity pattern observatory from mobile web logs},
 year = {2015}
}

@article{1392,
 abstract = {Abstract
Through research tools such as data warehouse and data mining in combination with abundant data peculiar to network users, this paper provides an insight into campus network user behaviour analysis and decision systems. Furthermore, the authors attempt to base webpage classification criteria on the open directory project (ODP) with a view to exploring specific patterns and rules found in network user behaviours. On the basis of those pioneering studies, the authors have put forward some ideas, procedures, and a framework concerning behaviour analysis and decision systems},
 duplicado = {false},
 inserir = {false},
 title = {An insight into campus network user behaviour analysis decision system},
 year = {2017}
}

@article{1393,
 abstract = {Abstract
With the increasing availability of connected health organisations, key medical information is expected to be accessible at the point of care. However, the high sensitivity of the clinical data and the large heterogeneity in health information systems pose a great interoperability challenge, including solutions that rely solely on the use of data exchange standards. Due to low adoption of these standards, such solutions will not sufficiently scale to achieve this objective. This chapter presents a service-based approach that utilises domain models combined with extensible problem models, enriched with domain terminology and knowledge services to enable autonomous data governance and semantic interoperability. The chapter addresses the resulting requirements, describes the proposed a solution and reports the results from the prototype of the approach.},
 duplicado = {false},
 inserir = {false},
 title = {Semantic Interoperability-Enabled Architecture for Connected Health Services},
 year = {2016}
}

@article{1394,
 abstract = {Abstract
In our earlier work, we defined criteria-based web service (CBWS) as a service which has 'criteria' associated with it. Criteria are a set of properties and are a part of non-functional properties (NFPs) of a service. Composite web services invoke one or more additional web services and combine their functionality to fulfil the user's requirement. In this paper, we propose that a composite service be able to invoke CBWS. To support this, our proposed approach is to extend the Business Process Execution Language (BPEL) to extended BPEL (X-BPEL) by introducing new activities which help to get the criteria information from the user before invoking CBWS. It is possible that more than one CBWS satisfies the criteria specified by the user. Upon finding these services, the user is given this list of web services to choose any one of them. This dialogue is supported by new activities in X-BPEL. The extensions are defined in the X-BPEL schema.},
 duplicado = {false},
 inserir = {false},
 title = {An extension to BPEL for criteria-based web service composition},
 year = {2016}
}

@article{1395,
 abstract = {Abstract
Currently, there are two mainstream process modelling paradigms: the traditional activity-centric approach and the recent artefact-centric approach. Several approaches have been proposed for configuration of traditional activity-centric business processes; however, to the best of our knowledge, few approaches have been developed for artefact-centric business processes. This paper fills this gap by proposing a novel configuration approach for artefact-centric business processes. The configuration of artefact-centric process modelling is achieved by the configuration of artefact lifecycle models for all the involved artefact classes. First, we apply deterministic finite automaton to formalise an artefact lifecycle model (ALM). To derive configurable ALM, we propose merger operation for ALMs. Also, an artefact lifecycle graph (ALG) is proposed to describe ALM as a state diagram, and the structural soundness of ALG is also presented. Then, we propose configurable artefact lifecycle graph (C-ALG) to describe the configurable artefact lifecycle model, in which arcs and sets of business rules can be defined to be configurable points. Finally, by configuring C-ALGs for each artefact class with our individualisation algorithm, structurally sound artefact lifecycle models are derived (including data models of artefacts, associated services and business rules), thus resulting a complete configured artefact-centric process model.},
 duplicado = {false},
 inserir = {false},
 title = {Artefact-centric business process configuration},
 year = {2016}
}

@article{1396,
 abstract = {Abstract
The derivation of business process execution language (BPEL) for web services from graph-oriented process models has attained wide focus in the literature. It is a challenging work owing to the fundamental differences between graph-oriented models and BPEL. In this paper, a transformation of activity diagrams (AD) into BPEL is presented, which concentrates on a specific kind of structure in graph-oriented process models called overlapped patterns (OPs). The structures of AD models containing OP are analysed, and an important subclass of OP, first-order OP, is defined. Then in the context of first-order OP, the applicable ranges of two existing transformation strategies of OP are discussed, and a new method is proposed for the cases that neither of them can handle.},
 duplicado = {false},
 inserir = {false},
 title = {Transformation from business process models to BPEL with overlapped patterns involved},
 year = {2016}
}

@article{1397,
 abstract = {Abstract
Service-oriented architecture (SOA) is an architectural style where the services are used instead of being physically integrated. The web services participating in a composition may evolve autonomously, making the service composition's behaviour uncertain. Moreover, the unavailability of source code in most of the situations makes prioritisation of the test cases of the web service composition more challenging. Traditional code-based prioritisation techniques are inappropriate. In this paper, a model-based test sequence prioritisation technique uses the coloured Petri nets as a model for web service composition design verification. The effectiveness of the prioritisation technique is validated using the average percentage fault detection (APFD) metric. The results show that the new proposed approach has high probability to outperform the random ordering on revealing regression faults in a modified web service composition code.},
 duplicado = {false},
 inserir = {false},
 title = {Requirements driven test prioritisation approach for web service composition},
 year = {2016}
}

@article{1398,
 abstract = {Abstract
This paper reports on a methodology to improve the approximation properties of the bivariate linear interpolation function when faced with the problem of sampling the signal at the intra-pixel misplacement ( x 0 , y 0 ). The energy measure, called intensity-curvature functional, is purposely designed yielding a polynomial system which zeros are the spatial set of points called sub-pixel efficacy region (SRE) of the interpolation function. The SRE uses empirically validated mathematical processes to determine the novel re-sampling locations ( x r 0 , y r 0 ) where the signal is sampled (calculated with the bivariate linear interpolation function) showing the optimisation of the interpolation error, which becomes smaller than the interpolation error obtained when sampling at the misplacement ( x 0 , y 0 ). The novel re-sampling locations have been sized to be a fraction of the pixel size whereas earlier they were sized to be a fraction of the misplacement (},
 duplicado = {false},
 inserir = {false},
 title = {The sub-pixel efficacy region of the bivariate linear interpolation function},
 year = {2014}
}

@article{1399,
 abstract = {Abstract
Research in this paper is to construct domain ontology called take-off ontology, based on which an application is developed to enable the user to retrieve model airplane and watch its take-off. Take-off ontology is constructed based on DOLCE+DnS, the famous upper ontology. Using DOLCE knowledge on model airplanes is constructed and using DnS and its extension, knowledge on situations (situation ontology) and related interpretations (description ontology) of take-off is constructed. The format that is used to represent take-off ontology is topic maps (XTM format) that can be easily processed by tm4j, the Java-based package. The application handles user-defined sequences of operations, which are constructed with glossary of DOLCE+DnS and contain varying operations on the user-selected model airplane during its take-off. After handling sequences of operations the application generates a scene reflecting (includes visually) behaviours of the model airplane during its take-off.},
 duplicado = {false},
 inserir = {false},
 title = {Exploration of take-off ontology of model airplane and its application},
 year = {2013}
}

@article{1400,
 abstract = {An emerging IT delivery model, Cloud Computing, can significantly reduce IT costs and complexities while improving workload optimisation and service delivery. More and more organisations are planning to migrate their existing systems into this internet-driven computing environment. This investigation is proposed for this purpose and will be undertaken with two main aims. The first aim is to establish a general framework and method to assist with the evolution of legacy systems into and within the Cloud environment. The second aim is to evaluate the proposed approach and demonstrate that such an approach can be more effective than developing Cloud services from scratch.
The underlying research procedure of this thesis consists of observation, proposition, test and conclusion. This thesis contributes a novel evolution approach in Cloud computing. A technical solution framework is proposed through a three-dimensional software evolution paradigm, which can cover the relationships of software models, software functions and software qualities in different Cloud paradigms. Finally, the evolved service will be run in the Cloud environments. The approach framework is implemented by three phases: 1) legacy system analysis and extraction, which proposes an analysis approach to decide the legacy system in the Cloud environment and to adopt the techniques of program slicing with improved algorithm and software clustering for extracting legacy components. 2) Cloud-oriented service migration including evolving software into and within Cloud. The process of evolving software INTO Cloud can be viewed mainly as changing software qualities on software models. The process of evolving software WITHIN Cloud can be viewed mainly as changing software functions on software models, the techniques of program and model transformation and software architecture engineering are applied. 3) Cloud service integration, which integrates and deploys the service in the Cloud environment.
The proposed approach is proved to be flexible and practical by the selected case study. Conclusions based on analysis and future research are discussed at the end of the thesis.},
 duplicado = {false},
 inserir = {false},
 title = {An Approach to Implementing Cloud Service Oriented Legacy Application Evolution},
 year = {2013}
}

@article{1404,
 abstract = {Abstract:
Over the last fifteen years, Web applications have evolved from the early simple and hyper-text based ones into the more complex, interactive, usable and adaptive applications of the new generations. New paradigms, architectures, and technologies for developing Web-based systems continuously emerge and transform this specific context. At the same time, new techniques and tools for effectively testing them have been proposed. This paper reports some relevant contributions about the Web application testing topic that appeared in the past editions of the Web Systems Evolution international symposium (WSE) and discusses some future trends for this specific field.},
 duplicado = {false},
 inserir = {false},
 title = {Web application testing in fifteen years of WSE},
 year = {2013}
}

@article{1406,
 abstract = {The International Symposium on Web Systems Evolution (WSE) is regarded as one of the premier conferences by the software evolution and maintenance community in its domain. As a symposium, an important focus of WSE is to enable stimulating discussions and interactions of researchers, ranging from the software maintenance and evolution community to the Web engineering and service-oriented computing community, from both academia and industry, while also providing a platform for presenting high-quality research papers. To give a visual impression of the topics covered by WSE from 2001 to 2011, Figure?1 shows a tag cloud constructed from the 170 paper titles published in the corresponding proceedings.},
 duplicado = {false},
 inserir = {false},
 title = {Special issue: selected papers from the 12th International Symposium on Web Systems Evolution (WSE 2010)},
 year = {2013}
}

@article{1408,
 abstract = {Abstract
A novel implementation is presented for NREO, a subject-specific ontology of the Nuclear or Radiological Emergency domain. The ontology design is driven by the requirements of ontology-based, multi-lingual language processing and retrieval use cases, but care is taken to architect the foundations in a way that can be extended to support other use cases in the domain. More specifically, NREO codifies and cross-references existing terminology glossaries and stakeholder lists into machine-processable terminological resources. At the interest of semantic interoperability, the proposed architecture is based on the Simple Knowledge Organization Scheme catalyzing the extensive cross-linking to different ontologies both within the nuclear technology domain and in related domains and disciplines. This and all other core design decisions are presented and discussed under the prism of their adequacy for our use cases and requirements. Both the ontology and terminological data have been made publicly available.},
 duplicado = {false},
 inserir = {false},
 title = {A conceptualization of a nuclear or radiological emergency},
 year = {2015}
}

@article{1409,
 abstract = {Abstract:
As Function Block Diagram (FBD) programs are used to implement safety-critical systems such as nuclear reactor protection systems, it is crucial to be able to generate effective test cases. The FBD is one of programming languages that are used for programmable logic controllers (PLCs). PLC programs are repeatedly run within a particular scan time for every execution. Among the constituents of FBD programs, function blocks and feedback variables use the inputs and outputs of the previous scan cycle on which to operate. Researchers have recently developed an automated test generation technique that satisfied several structural test coverage criteria for FBD programs using symbolic execution. However, their work could not generate test sequences for consecutive scan cycle but test inputs for one scan cycle. Test sequences are essential for testing FBD programs that have function blocks and feedback variables. This paper extends previous work in the field by generating test sequences for FBD programs with function blocks in a fully automated manner. The key technique involves explicitly unwinding FBD programs and solving test requirements using an SMT solver. We conduct experiments on increasing the coverage of test requirements by unwinding cycles and evaluating the effectiveness of the test set using mutation analysis with several subject programs, including a real-world reactor protection system. The experimental results show that the proposed approach is able to generate effective test sequences for FBD programs.},
 duplicado = {false},
 inserir = {false},
 title = {Automated Test Sequence Generation for Function Block Diagram Programs},
 year = {2016}
}

@article{1410,
 abstract = {Context

Empirical studies are essential in evaluating the effectiveness of Model-based Testing (MBT) research and should be reported properly to ensure their replication and to highlight the strengths and limitations of the MBT techniques being evaluated. Researchers have proposed guidelines detailing what information should be reported when presenting empirical studies and what should be the structure of such primary studies. There is a need to evaluate the reporting quality of the empirical studies in MBT literature.

Objective

To evaluate the reporting quality of empirical studies in the model based testing domain; identifying where the reported studies fail to follow the proposed guidelines and finding frequently omitted details. As an auxiliary goal we aim to quantify the percentage of empirical studies conducted in industrial context.

Method

We evaluate the reporting quality and the execution contexts of MBT empirical studies reported in literature. For our study we consider the MBT papers published in top ten software engineering journals over the last eighteen years. We evaluate the published primary studies using the empirical study reporting guidelines.

Results

We found 87 empirical in MBT that met our selection criteria. Initial results showed that the existing guidelines were not only too strict (for example they demand presence of specific sections rather than simply having the details present in the paper), they also did not adequately cover MBT specific details. Therefore, we propose modified the guidelines for reporting empirical studies in MBT and re-evaluated the selected studies. Results show that while only a few empirical studies follow the exact structure proposed by the guidelines, approximately half the papers contain at least 50% of the required details. Most of the papers omit details related to process and analysis leading to presented results. We found a positive trend of improving reporting quality of empirical studies in MBT over the last Eighteen years. Another important finding from the review is that few reported studies were conducted in real industrial context.

Conclusions

Model based testing community needs to be more aware of the reporting guidelines and more effort should be spent on reporting the necessary details. Furthermore, we found that only few studies that are conducted in industrial context and hence more focus should be given to empirical case studies in real industry context. However, the reporting quality of research papers presenting empirical evaluations is gradually improving.},
 duplicado = {false},
 inserir = {false},
 title = {Empirical studies omit reporting necessary details: A systematic literature review of reporting quality in model based testing},
 year = {2018}
}

@article{1411,
 abstract = {Abstract:
In safety-critical systems (e.g. Ship control system, avionics system), various reliability tactics are applied for guaranteeing the safe operation of the system. Therefore, much more attentions during the testing process are expected to be paid on these tactics. However, since developers cannot identify the reliability tactics from the system design effectively, the traditional testing approaches cannot obtain the test scenarios associated with reliability tactics directly. Based on our previous works about the aspect specifications of reliability tactics, we propose an approach for generating the test scenarios associated with reliability tactics from Sequence Diagram of Unified Modeling Language (UML). Owing to the separation of reliability tactics from the base model, the Label Transition Systems (LTSs) of the base and tactic aspect model can be obtained as intermediate models of generation process respectively. Then the action sequences of above LTSs are generated. Following the proposed principles, we can acquire the test scenarios associated with reliability tactics through integrating the action sequences of tactic aspect models with the ones of the base model. An avionics system is used to evaluate the availability of our approach. The results show that 1) our approach can effectively identify the test scenarios associated with reliability tactics from all possible test scenarios of system design, 2) the aspect specifications of reliability tactics can alleviate the input effort of test scenario generation for different functional fragments. We also exemplify the advantage of our approach for supporting the model changes.},
 duplicado = {false},
 inserir = {false},
 title = {Test Scenario Generation for Reliability Tactics from UML Sequence Diagram},
 year = {2014}
}

@article{1412,
 abstract = {Ensuring that an interactive application allows users to perform their activities and reach their goals is critical to the overall usability of the interactive application. Indeed, the effectiveness factor of usability directly refers to this capability. Assessing effectiveness is a real challenge for usability testing as usability tests only cover a very limited number of tasks and activities. This paper proposes an approach towards automated testing of effectiveness of interactive applications. To this end we resort to two main elements: an exhaustive description of users' activities and goals using task models, and the generation of scenarios (from the task models) to be tested over the application. However, the number of scenarios can be very high (beyond the computing capabilities of machines) and we might end up testing multiple similar scenarios. In order to overcome these problems, we propose strategies based on task models manipulations (e.g., manipulating task nodes, operator nodes, information...) resulting in a more intelligent test case generation approach. For each strategy, we investigate its relevance (both in terms of test case generation and in terms of validity compared to the original task models) and we illustrate it with a small example. Finally, the proposed strategies are applied on a real-size case study demonstrating their relevance and validity to test interactive applications.},
 duplicado = {false},
 inserir = {false},
 title = {A More Intelligent Test Case Generation Approach through Task Models Manipulation},
 year = {2017}
}

@article{1413,
 abstract = {Abstract
Testing safety-critical systems is crucial since a failure or malfunction may result in death or serious injuries to people, equipment, or environment. An important challenge in testing is the derivation of test cases that can identify the potential faults. Model-based testing adopts models of a system under test and/or its environment to derive test artifacts. This paper aims to provide a systematic mapping study to identify, analyze, and describe the state-of-the-art advances in model-based testing for software safety. The systematic mapping study is conducted as a multi-phase study selection process using the published literature in major software engineering journals and conference proceedings. We reviewed 751 papers and 36 of them have been selected as primary studies to answer our research questions. Based on the analysis of the data extraction process, we discuss the primary trends and approaches and present the identified obstacles. This study shows that model-based testing can provide important benefits for software safety testing. Several solution directions have been identified, but further research is critical for reliable model-based testing approach for safety.},
 duplicado = {false},
 inserir = {false},
 title = {Model-based testing for software safety: a systematic mapping study},
 year = {2018}
}

@article{1414,
 abstract = {Software testing is the pre-eminent part of the software development life cycle process.
Generating test cases is one of the challenging parts in software testing. Test cases are a set of
conditions where a tester will test whether the system or an application is working as it is
established to do. Test cases are often referred as test scripts, when they are collected into test
suite. Test automation defines that the system should work based on the established
functionalities that has been described. Genetic Algorithm is an adaptive search optimization
technique which performs implicitly parallel search in a large solution space and manipulates
simulation. The nature of GA is to perform testing of all competitive behavior of the solution
space until a good behavior is evolved. This paper presents a survey on automated test case
generation from UML sequence diagrams using GA (Genetic Algorithm) and other techniques. },
 duplicado = {false},
 inserir = {false},
 title = {A Systematic Survey on Automated Test Case Generation from UML Sequence Diagram Using GA and Other Approaches},
 year = {2015}
}

@article{1415,
 abstract = {Abstract
Scenario-based approaches have become attractive for designs through testing of embedded systems (ES). For verification, the system under development should be analyzed during its operation. Modeling of dynamic scenarios provides advantages of combining runtime verification with test generation and supports dynamic analysis of systems. This paper presents application of dynamic scenarios for test generation of a practical embedded signal processing system as case study.},
 duplicado = {false},
 inserir = {false},
 title = {Dynamic Scenarios in Embedded System Verification},
 year = {2015}
}

@article{1416,
 abstract = {ABSTRACT
The Software Engineering discipline "Software Testing" has not provided a resource for systematically testing a software
product with focus on the various aspects related to information security. This was one of the conclusions produced by a
literature review conducted in the second half of 2012; a systematic literature review is now under way aiming to provide
a more solid perspective on this subject. An approach based on adequately structuring the current knowledge on
information security may provide support for effective security testing.},
 duplicado = {false},
 inserir = {false},
 title = {SOFTWARE TESTING WITH EMPHASIS ON FINDING SECURITY DEFECTS},
 year = {2013}
}

@article{1417,
 abstract = {Software architecture models are specifications of the principal design decisions about a
software system that primarily govern its structure, behavior, and quality. Architecture
models provide a blueprint for how the system is implemented, serve as a basis for experimentation
with and rationalization of design decisions, and enable the automation
of software engineering tasks. Domain-specific languages (DSLs) are modeling languages
that are defined and customized for a particular family of problems or systems (the domain).
DSLs are able to concisely and intuitively express software architecture models
because they natively include the design abstractions that are most useful and natural
for the system under development.
However, because DSLs have non-standard semantics that cannot be known a priori,
leveraging architecture models specified in a DSL for automated quality analysis, code
generation, and other activities requires the implementation of specialized tools, called
model interpreters, that perform these functions. Implementation and maintenance of
domain-specific model interpreters are widely recognized as difficult and complex tasks
that incur a significant burden on software engineers and reduce the appeal of the DSL
approach.
In this dissertation, I describe XTEAM, an integrated set of processes, notations,
tools, and designs that enable the automated synthesis of model interpreters for DSLs.
The key elements of XTEAM are (1) semantics embedded in the DSL specification (called
a metamodel), (2) a metamodel interpreter that automatically synthesizes transformation
rules derived from those semantics, and (3) a reusable model interpreter framework that
applies those transformation rules in an efficient and structured way to domain-specific
models. The impact of XTEAM is that software engineers can use a DSL and perform
automated model analysis, code generation, and other types of model transformation
and manipulation without having to develop custom tools, greatly reducing the time,
effort, expense, and risk associated with domain-specific modeling. XTEAM has been
and continues to be used successfully on a number of software engineering projects, and
experimental evaluation of XTEAM indicates that, even when pessimistic assumptions
are made, XTEAM automates what would otherwise require multiple person-months of
effort.},
 duplicado = {false},
 inserir = {false},
 title = {Automated synthesis of domain-specific model interpreters},
 year = {2010}
}

@article{1418,
 abstract = {As two of the most popular defect removal activities, Inspection and Testing are
of the most labor-intensive activities in software development life cycle and consumes
between 30% and 50% of total development costs according to many studies. However,
most of the current defect removal strategies treat all instances of software artifacts as
equally important in a value-neutral way; this becomes more risky for high-value
software under limited funding and competitive pressures.
In order to save software inspection and testing effort to further improve
affordability and timeliness while achieving acceptable software quality, this research
introduces a value-based, dependency-aware inspection and test prioritization strategy for
improving the lifecycle cost-effectiveness of software defect removal options. This
allows various defect removal types, activities, and artifacts to be ranked by how well
they reduce risk exposure. Combining this with their relative costs enables them to be
prioritized in terms of Return On Investment (ROI) or Risk Reduction Leverage (RRL).
Furthermore, this strategy enables organizations to deal with two types of common
dependencies among items to be prioritized. This strategy will help project managers
determine how much software inspection/testing is enough? under time and budget
constraints. Besides, a new metric Average Percentage of Business Importance Earned
(APBIE) is proposed to measure how quickly testing can reduce the quality uncertainty
and earn the relative business importance of the System Under Test (SUT).
This Value-Based, Dependency-Aware Inspection and Testing strategy has been
empirically studied and successfully applied on a series of case studies within different
prioritization granularity levels: (1). Prioritizing artifacts to be reviewed in 21 graduate
level, real-client software engineering course projects; (2). Prioritizing testing scenarios
to be applied in an industrial project at the acceptance testing phase in Galorath, Inc.; (3).
Prioritizing software features to be functionally tested in an industrial project in the
China-NFS company; (4). Prioritizing test cases to be executed in 18 course projects. All
the comparative statistics analysis from the four case studies show positive results from
applying the Value-Based, Dependency-Aware strategy. },
 duplicado = {false},
 inserir = {false},
 title = {Value-based, dependency-aware inspection and test prioritization},
 year = {2012}
}

@article{1419,
 abstract = {Abstract
Complex systems such as autonomous robots and networks of unmanned vehicles are being
developed to perform sophisticated tasks and collaborative missions in uncertain operational
environments. These systems may involve significant levels of autonomous behavior, may rely
on network centric operations, or may constitute a system of systems (SoS) under the control
of multiple independent stakeholders. Testing such complex systems presents unique
challenges compared to traditional systems, especially given emergent behaviors and
uncertainties surrounding anticipated missions and operations. In this paper, we discuss these
challenges and their impact on a spectrum of test decisions, including test design, planning,
execution, and long term strategy. We then show how real options methods can be applied to
support decision making under uncertainty for testing complex systems.},
 duplicado = {false},
 inserir = {false},
 title = {A Real Options Approach to Testing},
 year = {2010}
}

@article{1421,
 abstract = {Abstract
The Autonomous Ground Resupply (AGR) Program is a U.S. Army Tank Automotive Research Development and Engineering Center (TARDEC) effort to reduce the number of soldiers required for ground resupply. One objective of this program involves providing vehicles with the capability to operate unmanned in a variety of circumstances. Prior to fielding, this system is undergoing an assessment to identify possible vulnerabilities. We use this system to illustrate a general approach to the assessment of the autonomous decision making, describing a process which could be used to identify vulnerabilities in the autonomy of military systems without describing any actual vulnerabilities discovered in the AGR system.},
 duplicado = {false},
 inserir = {false},
 title = {Assessing autonomy vulnerabilities in military vehicles},
 year = {2017}
}

@article{1422,
 abstract = {Abstract

Systems engineering process simulators can help users to explore risks and evaluate the consequences of various project, process, and organization decisions. These types of simulators can incorporate the complex system dynamics associated with various concerns and include critical factors, their associated relationships, and corresponding effects, allowing one to reason and explore various decision options. Simulators can also help educate current and new engineers to understand the multi-faceted issues related to systems engineering projects and processes. In support of the development of these types of simulators, this paper discusses the need for an architecture framework for systems engineering simulators. An architecture framework is proposed as an enabler to assist in the development of systems engineering process simulators.},
 duplicado = {false},
 inserir = {false},
 title = {The Development of an Architecture Framework for Systems Engineering Process Simulators},
 year = {2011}
}

@article{1423,
 abstract = {Abstract:
Service-Oriented Architecture (SOA) is a new architectural style for developing distributed business applications. Nowadays, those applications are realized through web services, which are later grouped as web service compositions. Web service compositions language, like the BPELWS 2.0 standard, are extensions of imperative programming languages. Additionally, it presents a challenge for traditional white-box testing, due to its inclusion of specific instructions, concurrency, fault compensation and dynamic service discovery and invocation. In fact, there is a lack of unit testing approaches and tools, which has resulted in inefficient practices in testing and debugging of automated business processes. Therefore, we performed a systematic review study to analyze 27 different studies for unit testing approaches for BPEL. This paper aims to focus on a comprehensive review to identify a categorization, a description of test case generation approaches, empirical evidence, current trends in BPEL studies, and finally to end with future work for other researchers.},
 duplicado = {false},
 inserir = {false},
 title = {Unit Testing Approaches for BPEL: A Systematic Review},
 year = {2009}
}

@article{1424,
 abstract = {Abstract This paper proposes Web Service based network
management. The Web Service based network management
system is analyzed. It consists of network management layer,
collaborative management implementation layer, and
management function layer mainly. The complex
management network tasks can be accomplished
respectively by more than one Web Service distributed on
Internet and the Web Services interchange information
based on XML message. The SNMP/XML gateway and the
translation between GDMO/ASN.1 and XML/Schema are
designed and implemented to implement the integration
between the legacy network management systems and the
network management developed by Web Service
technologies. The service management in Web Service based
network management is discussed. Service composition/recomposition
in Web Service based network management is
analyzed based on the QoS requirements negotiation
between the network management requirements and the
statement of Web Service and network, OWL-S being used
to described the network management requirements to
discover the suitable Web Service, BPEL being used to
describe the Web Service composition},
 duplicado = {false},
 inserir = {false},
 title = {The Research on Web Service based Network Management},
 year = {2010}
}

@article{1425,
 abstract = {Abstract In traditional automatic test solutions, a test
engine usually encompasses all functions in its kernel,
including compiling test program, generating test event
chain, scheduling test process and executing test events. This
makes the engine tightly coupled with test language and the
system under test, so that it is difficult to maintain, optimize
and extend the test engine. In order to solve these problems,
a micro-kernel test engine is designed and implemented
based on the service oriented architecture. This microkernel
approach decouples function modules to make the
test engine kernel independent of the system under test and
the test language. This also makes the test engine more
modularized, so that the debugging process and
maintenance work of the engine can be much easier. With
new compiling component and test adapters, the engine
kernel can be extended for new test methods or reused in
new test applications. The application example and
extensibility analysis discussed in section 6 show the
feasibility of this micro-kernel test engine},
 duplicado = {false},
 inserir = {false},
 title = {A Micro-Kernel Test Engine for Automatic Test System},
 year = {2011}
}

@article{1426,
 abstract = {Abstract:
In this paper architecture of real time messaging server is proposed to support two-way client-server interactions. Here, the duplex web service acts as a service broker between the clients and the server. It selects dynamically the Web service based on the client profile, Web service registry and runtime service status. The Real Time Messaging Framework will dynamically configure into a Client Proxy to maintain communication, which can be real time text, audio or video. Due to the duplex web services, overhead will be reduced at the server side, at the same time push back service, which is a part of the two-way interaction of the web service, will pass the message to other client in the network. In the whole process, emphasis will be on increasing the data transfer speed and efficiency. Thus, use of a web service in a chat server will make it work better and faster in distributed environment.},
 duplicado = {false},
 inserir = {false},
 title = {Architecture of Real Time Messaging server using duplex web services},
 year = {2014}
}

@article{1427,
 abstract = {Abstract:
Web Services are applied widely in the field of information technology, and performance testing for Web Services applications has become an important problem. This paper introduces the method of performance testing, and proposes a framework of agent-based performance testing on Web Services, which includes TestFlow Generator, Scenario Creator, Test Manager, Load Generation Agent and Test Analyzer. And the implementation of kernel modules in the framework is introduced especially. To realize the load allocation to distributed Load Generation Agents from Test Manager, a queue-based allocation strategy is given.},
 duplicado = {false},
 inserir = {false},
 title = {Distributed agent-based performance testing framework on Web Services},
 year = {2010}
}

@article{1428,
 abstract = {Abstract:
Presently, the web services are commonly used in web program development. These service agents may contain a nested call to other agents in order to complete their tasks. Therefore, the processing time of these agents may consume the processing time according to the external process of others. Moreover, there might be some errors within the external web agents. Therefore, the web service flows must be determined before real implementation. As a consequence of such requirement, the Web Services Composition (WSC) must be clearly define the flows and web descriptions to support the verification process of the web services agents. In this paper a new architecture, modified from the existing EMTI, is proposed using the extending framework, EEMTI, with extending Web Services Definition Language (WSDL) of the nested schema. Every element in the required agents is completely monitored. Consequently, the hidden false can be unclosed and corrected before the agent is called.},
 duplicado = {false},
 inserir = {false},
 title = {EEMTI: An extending framework for nested web service verification},
 year = {2012}
}

@article{1429,
 abstract = {Abstract:
Structured Modeling is a common modeling method. Modeling analysis in Structured Modeling is complex, difficult and time-consuming. This paper proposed the service architecture under Grid analysis environment for SM that is ideally suited to Semantic Web applications. Based on WSDL, OWL-S, FIPA, and OGSA, extending existing Grid paradigms to harness emerging Web service technologies such as SOAP, WSDL and BPEL, a concise overview of Grid middleware for SM is provided. The functionality model, the Web service-based grid architecture for SM, and the logic function architecture of SM is proposed by this paper.},
 duplicado = {false},
 inserir = {false},
 title = {Grid Analysis Environment Service Architecture for Structured Modeling},
 year = {2009}
}

@article{1430,
 abstract = {Abstract
The coordination between agent service and Web service is the key factor for intelligent Web service management in the multi-agent based Web service framework. In view of the drawbacks of existing coordination approaches for agent service and Web service, this paper proposed a multi-agent and workflow-based Web service management model. Through analyzing the interaction relations between agent service and Web service in the logical action-based task environment, a uniform task view for intelligent Web service is built. And based on such task view, a workflow towards special task is designed to realize intelligent Web service discovery and cooperation and composition. This model provides a more flexible Web service management.},
 duplicado = {false},
 inserir = {false},
 title = {Multi-agent and Workflow-Based Web Service Management Model},
 year = {2010}
}

@article{1431,
 abstract = {In this paper of real time messaging server is
proposed to support two-way client-server
interactions. Here, the role of service broker
between the clients and the server is played by the
duplex web services. Depending on the client
profile, web service registry and run time service
status, web service is dynamically selected. The Real
Time Messaging Framework will dynamically
configure into a Client Proxy to maintain
communication, which can be real time text, audio
or video. Because of the duplex web services,
overhead will be reduced at the server side by using
push back services at the same time. This service is
the part of two way interaction of the web service
that passes the message to the other client in the
network. In the whole process, emphasis will be on
increasing the data transfer speed and efficiency.
Thus, use of a web service in a chat server will
make it work better and faster in distributed
environment.},
 duplicado = {false},
 inserir = {false},
 title = {Real Time Messaging Server Using Duplex Web Services},
 year = {2014}
}

@article{1433,
 abstract = {ABSTRACT
This architecture supports two-way client-server interactions.
The Proxy Framework acts as a service broker between the
clients and the CSTA Web services. It dynamically selects the
Web service based on client profile, Web service registry and
runtime service status. The Proxy framework separates service
independent functions, such as session management, event
subscriptions, and type conversion, from the service
dependent logic, such as message composition and parsing, so
that different services can be plugged into the framework
easily. The Proxy Framework can be dynamically configured
into a Client Proxy to facilitate development of CSTA
applications in programming languages, including Visual
C++, Visual Basic and Java. Due to the duplex web services
interaction created, load will be reduced at the server end, at
the same time push back service which is a part of the twoway
interaction of the web service will pass the message to
other client in the network. Emphasis will be on increasing the
data transfer speed, at the same time use of a web service in a
chat server will make it work better and faster in distributed
environment.},
 duplicado = {false},
 inserir = {false},
 title = {Implementation of Real Time Messaging Server using Duplex Web Services},
 year = {2015}
}

@article{1434,
 abstract = {Abstract:
This paper proposes the information collaboration computation based on multi-agent. The process structure of information collaboration computation is discussed. Based on the process structure, the multi-agent information collaboration task process is analyzed according to the task life cycle. The multi-agent based information collaboration computation framework includes task modeling, task management, and multi agent implementation is proposed and analyzed. The prototype system has been applied in information collaboration computation and proven effectively.},
 duplicado = {false},
 inserir = {false},
 title = {Multi-agent Based Information Collaboration Computation},
 year = {2014}
}

@article{1435,
 abstract = {Producing high quality software systems has been one of
the most important software development concerns. Software
testing is recognized as a fundamental activity for assuring
software quality; however, it is an expensive, errorprone,
and time consuming activity. For this reason, a diversity
of testing tools has been developed, however, they
have been almost always designed without an adequate attention
to their evolution, maintenance, and reuse. In this
paper, we propose an aspect-based software architecture,
named RefTEST (Reference Architecture for Software Testing
Tools), that comprises the knowledge to develop testing
tools. This architecture is strongly based on separation of
concerns and aspects, aiming at evolving, maintaining and
reusing efforts to develop these tools. Our experimental results
have pointed out that RefTEST can contribute to the
development and reengineering of testing tools.},
 duplicado = {false},
 inserir = {false},
 title = {Towards a Reference Architecture for Software Testing Tools},
 year = {2007}
}

@article{1436,
 abstract = {Software architectures have played a significant role in determining the success of software systems. In spite of impact of the architectures to the software development and, as a consequence, to the software quality, there is not yet a consensus about which mechanisms work better to describe these architectures. In addition, despite the relevance of reference architectures as an artifact that comprises knowledge of a given domain and supports development of systems for that domain, issues related to their representation have not also had enough attention. In this perspective, this work intends to contribute with an experience of representing reference architectures aiming at easily sharing and reusing knowledge in order to develop software systems. A case study on software testing is presented illustrating our experience.},
 duplicado = {false},
 inserir = {false},
 title = {Reference architecture knowledge representation: an experience},
 year = {2008}
}

@article{1437,
 abstract = {Abstract:
Web services are an emerging Service-Oriented Architecture technology to integrate applications using open standards based on XML. Software Engineering tools integration is a promising area since companies adopt different software processes and need different tools on each activity. Software engineers could take advantage of software engineering tools available as web services and create their own workflow for integrating the required tools. In this paper, we propose the development of testing tools designed as web services and discuss the pros and cons of this idea. We developed a web service for structural testing of Java programs called JaBUTiService, which is based on the stand-alone tool JaBUTi. We also present an usage example of this service with the support of a desktop front-end and pre prepared scripts. A set of 62 classes of the library Apache-Commons-BeanUtils was used for this test and the results are discussed.},
 duplicado = {false},
 inserir = {false},
 title = {JaBUTiService: A Web Service for Structural Testing of Java Programs},
 year = {2009}
}

@article{1438,
 abstract = {Abstract
Software testing is recognized as a fundamental activity for assuring software quality. Aiming at supporting this activity, a diversity of testing tools has been developed, including tools based on SOA (Service-Oriented Architecture). In another perspective, reference architectures have played a significant role in aggregating knowledge of a given domain, contributing to the success in the development of systems for that domain. However, there exists no reference architecture for the testing domain that contribute to the development of testing tools based on SOA. Thus, the main contribution of this paper is to present a service-oriented reference architecture, named RefTEST-SOA (Reference Architecture for Software Testing Tools based on SOA), that comprises knowledge and experience about how to structure testing tools organized as services and pursues a better integration, scalability, and reuse provided by SOA to such tools. Results of our case studies have showed that RefTEST-SOA is a viable and reusable element to the development of service-oriented testing tools.},
 duplicado = {false},
 inserir = {false},
 title = {A Service-Oriented Reference Architecture for Software Testing Tools},
 year = {2011}
}

@article{1441,
 abstract = {Abstract:
Software architectures have played a significant role in determining the success of software systems. In particular, reference architectures have emerged, achieving well-recognized understanding in a specific domain, promoting reuse of design expertise and facilitating the development of systems. In another perspective, ontologies have been widely investigated aiming at representing, communicating and reusing knowledge. In spite of their relevance on directly dealing with domain knowledge, reference architectures and ontologies have been separately treated. In this paper we investigate the impact in using ontologies to the establishment of reference architectures. We illustrate our idea using an ontology of software testing to build a reference architecture for the testing domain. Preliminary results indicate that ontologies are an important and viable mechanism aiming at building reference architectures.},
 duplicado = {false},
 inserir = {false},
 title = {Exploring ontologies to support the establishment of reference architectures: An example on software testing},
 year = {2009}
}

@article{1442,
 abstract = {Abstract:
With more and more attention on the software quality, the test case reuse has become a focus in current research. By integrating knowledge management and software reuse theory, several design guidelines for reusable test cases are identified according to the characteristics of software components. A reusable test case knowledge management model is proposed to support the knowledge reuse based on the ontology representation of reusable test cases in this paper. With the ontology and knowledge management model, test engineers can retrieve and reuse test cases flexibly.},
 duplicado = {false},
 inserir = {true},
 title = {Ontology-Based Testing Platform for Reusing},
 year = {2012}
}

@article{1448,
 abstract = {Abstract:
Systems-of-Systems are a class of systems composed of diverse, independent constituent systems. Together, these constituents can accomplish missions that otherwise could not be performed by any of them separately. In another perspective, knowledge representation approaches can assist in the establishment of a common understanding in this field by formalizing and standardizing the main terms and concepts adopted. In spite of the relevance of SoS, a consolidated terminology which could support the community working with such systems is still missing. Furthermore, the multiplicity of stakeholders, technologies, and expertise involved in an SoS makes the need of a common understanding even more imperative. In this study, we report on the main findings of a systematic literature review covering knowledge representation approaches in the SoS field. With this study, we are able to present a comprehensive panorama of the knowledge representation approaches that are currently adopted. Even though a consolidated terminology is not available yet, such panorama can be helpful for devising a common, comprehensive terminology for the SoS field. Therefore, we conclude this paper with directions for future work.},
 duplicado = {false},
 inserir = {false},
 title = {A Systematic Literature Review on Knowledge Representation Approaches for Systems-of-Systems},
 year = {2015}
}

@article{1457,
 abstract = {The graphical user interfaces (GUIs) are playing a major role
in the popularity of software systems. Recognizing the
importance of GUI, software teams feel an immense pressure
in delivering the interface according to expectations of the
customer. Beside heavy focus and due attention towards GUI,
software engineers are still looking for practices to ensure the
thorough testing of such applications involving GUI's. One
major breakthrough to automate this manual effort of GUI
testing is to map GUI events with some models and graphs.
Event-flow graph is relatively a fresh and useful addition to
cope up with automation of GUI testing. We have used event
driven nature of GUI for testing in some of previous studies.
In this paper we are presenting an idea of generating test cases
for GUI based on search based algorithm and manipulating
ontology for GUI testing. This ontology is supposed to work on
the basis of semantics of possible events and than annotations
will use to generate the test cases and work as an oracle for
verification of the output of testing effort. },
 duplicado = {false},
 inserir = {true},
 title = {Using Search Based SE for GUI Test Data Generation},
 year = {2012}
}

@article{1463,
 abstract = {Abstract Multiple views of data sets can provide assistance
on discovering unforeseen associations among elements contained
on these sets. Thus, users are able to explore data
in distinct perspectives. Ontologies are formal representations
which describe data relations explicitly. Representing the underlying
data into ontology, the exploratory visualization can
benefit from a semantic representation to create the mappings,
and might be helpful to establish relations on multiple views.
In this way, we propose the application of ontology to support
the mapping in the coordination process, defining how the data
elements are related. Also, we present two case studies applying
ontologies on exploring text collections. After these presentation,
the results are compared with traditional coordination
techniques.},
 duplicado = {false},
 inserir = {false},
 title = {Ontologies to coordinate multiple views: exploring document collections},
 year = {2014}
}

@article{1464,
 abstract = {Abstract
Software R&D teams require proper forms of representing knowledge at carrying out software engineering processes and researches. In this context, transfer of knowledge becomes a dynamic process because team members participating in the process acquire, communicate and integrate knowledge from different sources. In this paper, a behavior tree-based model is presented for representing knowledge generated from research and development activities. Through structured nodes representing pieces of knowledge, it is possible to identify key points of new challenges, concerns, issues, gaps, etc., and shed lights on new insights and knowledge of importance to team members, contributing to improve and provide solutions to the domain analyzed.},
 duplicado = {false},
 inserir = {false},
 title = {A Behavior Tree-Based Model for Supporting the Analysis of Knowledge Transferred in Software R&D Teams},
 year = {2016}
}

@article{1469,
 abstract = {Abstract:
This paper examines the use of Design by Contract for web service descriptions, and explores the issues and solutions of automatic test case generation and test oracle generation in the context of WS testing based on contracts. In our approach, the traditional concept of contracts (pre-condition, post-condition, and invariant) is extended to contain richer information, such as process control, to support automatic test generation. Contracts are used to specify the relation between a component and its clients as a formal agreement, expressing each party's rights and obligations. Contracts can be expressed in the OWL-S process model. By checking whether the web service respects its contracts, we can ascertain its validity. Therefore, contracts provide the basis for the automation of the testing process.},
 duplicado = {false},
 inserir = {false},
 title = {Contract-Based Testing for Web Services},
 year = {2007}
}

@article{1471,
 abstract = {Abstract:
Web Ontology Language for Services (OWL-S) is a standard XML-based language for specifying workflows and integration semantics among Web services (WS), which form composite WS. This paper analyzes the fault patterns of OWL-S specified composite WS and their workflows, proposes an ontology-based mutation analysis method, and applies specification-based mutation techniques for composite WS simulation and testing. Four categories of OWL-S mutant operators are specified, including data mutation, condition mutation, control flow mutation, and data flow mutation. Finally, the paper studies the ontology-based input mutation technique using a BookFinder service as a case study, which shows that ontology-based mutation provides viable test adequacy criteria for testing OWL-S specified composite WS.},
 duplicado = {false},
 inserir = {true},
 title = {Automatic Mutation Testing and Simulation on OWL-S Specified Web Services},
 year = {2008}
}

@article{1473,
 abstract = {Abstract:
In this paper, we describe an approach to discover the control flow graph of web services for web services analysis, verification, and testing. For this purpose, three novel methods are proposed. First, we introduce a domain independent RDF Schemas for concise resource oriented functional specification of web services operations. Secondly, we describe the use of RDF entailment to accurately derive the control flow from the functional specifications. We developed a transformation from RDF graph to SPARQL query to facilitate the RDF entailment which offers flexibility and extensibility over the direct graph matching approach. The third is a linkage based web services modeling and analysis framework, within which we apply an improved Google PageRank algorithm to efficiently calculate test coverage potential using the derived control flow. We justify that the proposed linkage based web services modeling and analysis framework is particularly suitable for testing web services. A prototype of the proposed methods has been implemented and tested on some standard based web services. Experimental results show that the control flow analysis is quite efficient and accurate, and the coverage based test results of the proposed approach are very promising.},
 duplicado = {false},
 inserir = {false},
 title = {Control Flow Analysis and Coverage Driven Testing for Web Services},
 year = {2008}
}

@article{1474,
 abstract = {Abstract:
Web service composition is an emerging paradigm for enabling application integration within and across organizational boundaries. Model checking is a promising technique for the verification and validation of software systems. In this paper, we present a model checking framework to specifying and verifying the compositions of Web services workflow based on BPEL4WS (Business Process Execution Language for Web Services). By using annotation layers, a BPEL4WS model can be extended with constraints (properties) information. An underlying BPEL4WS model and one or more constraint annotation layers compose a complete specification of a business process imposed specific constraints. By transforming the annotated BPEL4WS model to an extended TPPN(Timed Predicate Petri-net) model, a business process can be automatically verified and analyzed. The method allows us to add conveniently constraints information to a business process model, and state whether a process satisfies given properties without actual execution based on its specification.},
 duplicado = {false},
 inserir = {false},
 title = {A Framework for Model Checking Web Service Compositions Based on BPEL4WS},
 year = {2007}
}

@article{1476,
 abstract = {Abstract
Recent years have seen the utilisation of Semantic Web Service descriptions for automating a wide range of service-related activities, with a primary focus on service discovery, composition, execution and mediation. An important area which so far has received less attention is service validation, whereby advertised services are proven to conform to required behavioural specifications. This paper proposes a method for validation of service-oriented systems through automated functional testing. The method leverages ontology-based and rule-based descriptions of service inputs, outputs, preconditions and effects (IOPE) for constructing a stateful EFSM specification. The specification is subsequently utilised for functional testing and validation using the proven Stream X-machine (SXM) testing methodology. Complete functional test sets are generated automatically at an abstract level and are then applied to concrete Web services, using test drivers created from the Web service descriptions. The testing method comes with completeness guarantees and provides a strong method for validating the behaviour of Web services.},
 duplicado = {false},
 inserir = {false},
 title = {Leveraging Semantic Web Service Descriptions for Validation by Automated Functional Testing},
 year = {2009}
}

@article{1477,
 abstract = {Abstract:
Web services have become popular in the modern infrastructure of the World Wide Web. They aim to provide automatic discovery, selection, and invocation of required applications (services) across the internet. However, the quality assurance aspects of web services remain a challenge. Recently, the semantic web has been introduced as an emerging technology which emphasizes presenting the meaning of the web content to achieve a machine processable automation. In this paper, we explore the synergy of applying specification based software testing techniques to semantic web services. Our approach investigates the possibility of deriving concrete test cases from the goal specification of a semantic web service in order to determine the correctness of a service implementation. Furthermore, we also propose coverage criteria to evaluate the generated test cases at both the goal and the service description levels. We demonstrate the generation and evaluation of the test cases from a goal specification with the help of a simplified discount example.},
 duplicado = {false},
 inserir = {false},
 title = {Towards Specification Based Testing for Semantic Web Services},
 year = {2009}
}

@article{1479,
 abstract = {Abstract:
Software testing is one of the most important techniques used to assure the quality of Web Services at present. Test-data generation is an important topic in Web Services testing. The quality of test data will influence the efficiency and cost when testing Web Services. Based on the contract-based mutation testing technique, this paper presents a method of automated test-data generation for Web Services. First, according to the description information and contracts in WSDL documents of Web Services, initial test data are generated automatically by the random method. Then the test data are selected using contract mutation testing. This method can generate a test suite meeting a certain contract mutation score, which indicates the quality and efficiency of testing. Finally, we have developed a prototype on the Microsoft .NET platform, and carried out some experiments. The results have shown that the proposed method is effective in automated test-data generation for Web Services.},
 duplicado = {false},
 inserir = {false},
 title = {Test-Data Generation for Web Services Based on Contract Mutation},
 year = {2009}
}

@article{1480,
 abstract = {More and more Web based systems are being developed by composing other single or even composite services. This is due to the fact that not all available services are able to satisfy the needs of a user. The process of composing Web services involves discovering the appropriate services, selecting the best services, combining those services together, and finally executing them. Although much research efforts have been dedicated to the discovery, selection, and composition of services, the process of testing the Web service composition has not been given the same attention. This paper discusses the importance of Web services composition testing, provides a classification of the most prominent approaches in that area, presents several criteria for comparison of those approaches, and conducts a comparative evaluation of the approaches. The results of the paper give an essential perspective to do research work on Web services composition testing.},
 duplicado = {false},
 inserir = {false},
 title = {A comparative evaluation of state-of-the-art web service composition testing approaches},
 year = {2011}
}

@article{1481,
 abstract = {Abstract:
Web Services (WS), which are based on standard XML protocols, such as, WSDL, SOAP and UDDI, are the building blocks of Service Oriented Architecture (SOA). The aim of SOA is to automate web service tasks, such as, web service discovery, selection, composition and execution. Since XML is a syntax-based language, the automation of these tasks is still a challenge. To overcome this, web services can be described semantically using an ontology description language, e.g., Web Ontology Language (OWL), giving rise to semantic web services (SWS). Because semantic web services are relatively new, there has been little research into testing and quality assurance aspects. In this paper, we propose a novel approach for generating test cases based on user requirements for testing semantic web services. In SWS frameworks, such as, Web Service Modelling Ontology (WSMO), the user requirements are presented as a goal specification in terms of a state model. We use a model checking approach to generate test cases from this state model. To achieve this, we represent a set of rules for translation from a goal specification to a formal B abstract state machine. The B representation of the goal specification is given as input to the model checker to generate concrete test cases using the assertion violation property of the model checker. Finally, the proposed framework is evaluated using a real world case study based on, the Amazon E-commerce Service.},
 duplicado = {false},
 inserir = {false},
 title = {A Framework for Testing Semantic Web Services Using Model Checking},
 year = {2009}
}

@article{1482,
 abstract = {Abstract:
Test case is one of the most important part of software testing. Reusing available test cases is an effective means to reduce the cost and improve the efficiency of software testing. In order to efficiently reuse test cases, a unified and standard format to describe test cases is needed. Therefore, this paper presents a reusing method based on ontology. The test case ontology is created by Protege Ontology Editor and described by OWL (Web Ontology Language). At last, an example to illustrate the application of this method is to be discussed.},
 duplicado = {false},
 inserir = {true},
 title = {An application of ontology to test case reuse},
 year = {2011}
}

@article{1483,
 abstract = {Abstract
In this paper, we present a Petri net-based approach for modeling the choreography of semantic Web services which are described following the OWL-S specification. In our approach, each control construct of the OWL-S choreography is represented through a Petri net pattern that captures formally its operational semantics. The main difference between our work and the main proposals that model the semantics of OWL-S services choreography is that, although both approaches represent the service choreography with Petri nets, our proposal is also concerned with the practical execution of the Petri nets by the client. Therefore we also represent the flow of data, the outputs transformations, the effects in the environment, in addition to the structures that control the choreography of the services in our Petri net models. The implementation of the OWL-S choreography is performed in a Petri net ontology-based engine. This is another difference with traditional approaches that only use Petri nets for the analysis of the service properties. Furthermore, the use of an underlying ontology engine for supporting both the domain models of OWL-S services and the Petri net models provides several advantages in terms of reasoning, extension, and reuse.},
 duplicado = {false},
 inserir = {false},
 title = {Toward the use of Petri nets for the formalization of OWL-S choreographies},
 year = {2012}
}

@article{1484,
 abstract = {Abstract
In this paper, OPENET, an engine for the execution of high-level Petri nets (HLPNs) is presented. OPENET is based on an ontology that represents the knowledge of the ISO/IEC 15909-1 standard and, respectively, describes semantically and declaratively both the static structure and the dynamic behavior of HLPNs. Thus the ontology is composed of (i) a taxonomy that describes the main components of a net, capturing the vocabulary and semantics specified in the standard; and (ii) a set of axioms and rules that constrain how the instances of the taxonomy are created, restricting the range and domain of the relations, and the values of the attributes. These axioms guarantee that a HLPN is correctly constructed, and restrict how it should be executed; and (iii) a set of rules which contain the knowledge needed to execute HLPNs and thus infer new instances of the concepts that describe the dynamic model of the HLPN. The OPENET engine has been implemented in F-Logic with the FLORA-2 reasoner, and is being used in several domains: the execution of courses in E-learning, the modeling and execution of workflows in Industry, and the execution of web service choreographies.},
 duplicado = {false},
 inserir = {false},
 title = {OPENET: Ontology-based engine for high-level Petri nets},
 year = {2010}
}

@article{1487,
 abstract = {Abstract: Resource-oriented Services recently become an enabling technology to integrate and
configure information from different heterogeneous systems so as to meet ever-changing
environment which not only need the concepts for entities but also require the semantics for
operations. By the aim of combining structural and operational semantics agilely, a Semantic
Resource Service Model (SRSM) is proposed. Firstly, SRSM describes Entity-Oriented and
Transition-Oriented Resource by semantic meta-model which contains data structures and
operation semantics. Secondly, by describing structural semantics for Entity-Oriented
Resource, heterogonous inputs/outputs of a service can be automatically matched. Thirdly, by
describing operational semantics for Transition-Oriented Resource, the service composition
sequence can be inferred after ontology reasoning. Then, both Entity-Oriented and TransitionOriented
Resources are encapsulated into composited RESTful service. At last, a case study
and several comparisons are applied in a prototype system. The result shows that the proposed
approach provides a flexible way for resource-oriented service composition. },
 duplicado = {false},
 inserir = {false},
 title = {Ontology Combined Structural and Operational Semantics for Resource-Oriented Service Composition},
 year = {2013}
}

@article{1490,
 abstract = {Abstract:
Web service composition is an emerging paradigm for enabling application integration within and across organizational boundaries. Since most business processes exist in temporal context in real world, and the candidate partners in Web service compositions have complex interactions, timing constraints satisfiability verification for Web service compositions becomes increasingly important to build efficient and effective business processes based on Web services. In this paper, we present an approach to specify and verify timing constraints satisfiability for the compositions of Web services workflow based onBPEL4WS(Business Process Execution Language for Web Services). By using timing constraints annotation layers, a BPEL4WS model can be extended with timing constraint information. The pair of an underlying BPEL4WS model and a timing constraint annotation layer can be transformed to corresponding TPPN (Timed Predicate Petri-net) model to verify automatically timing constraint satisfiability for a business process. The method allows us to add conveniently timing constraints information to a business process model,and find whether a process is schedulable and which activities in the process are not schedulable under the consideration of timing constraints without actual execution based on its specification. The paper describes how to specify timing constraints for compositions of Web services workflow, and how to determine whether the specification satisfies its timing constraints.},
 duplicado = {false},
 inserir = {false},
 title = {Timing Constraints Specification and Verification for Web Service Compositions},
 year = {2008}
}

@article{1492,
 abstract = {Abstract:
Semantic annotation of web services has been proposed as a solution to the problem of discovering services to fit a particular need and reusing them appropriately. While there exist tools that assist human users in the annotation task, e.g., Radiant and Meteor-S, no semantic annotation proposal considers the problem of verifying the accuracy of the resulting annotations. Early evidence from workflow compatibility checking suggests that the proportion of annotations that contain some form of inaccuracy is high, and yet no tools exist to help annotators to test the results of their work systematically before they are deployed for public use. In this paper, we adapt techniques from conventional software testing to the verification of semantic annotations for web service input and output parameters. We present an algorithm for the testing process and discuss ways in which manual effort from the annotator during testing can be reduced. We also present two adequacy criteria for specifying test cases used as input for the testing process. These criteria are based on structural coverage of the domain ontology used for annotation. The results of an evaluation exercise, based on a collection of annotations for bioinformatics web services, show that defects can be successfully detected by the technique.},
 duplicado = {false},
 inserir = {false},
 title = {Verification of Semantic Web Service Annotations Using Ontology-Based Partitioning},
 year = {2013}
}

@article{1494,
 abstract = {Abstract
As we described in Chapter 1, Services are often provisioned within short-term, volatile and highly dynamic (business) processes. These processes are designed in an abstract manner and when instantiated can involve service providers not known of during the design time of the service-based application. Thus, different from traditional software systems, service-based applications require the composition and coordination of services within highly distributed environments, cutting across the administrative boundaries of various organizations.

This chapter provides a review of quality contracts, or more generally, those parts of Service Level Agreements (SLAs) which deal with statements about the services quality levels on which the service requestor and the providers have reached an agreement. Aspects of the contracts, such as the identification of parties, legal obligations and penalties for contract violation, are not covered by this chapter.},
 duplicado = {false},
 inserir = {false},
 title = {Analytical quality assurance},
 year = {2004}
}

@article{1496,
 abstract = {Abstract:
Verification for interaction protocol of Web services is crucial to both implementation and composition of Web services. The verification process can prove important and desirable properties of the control flow and data flow of Web services. Model checking is a promising technique for verification and validation of software systems. In this paper, we present a model checking framework to specifying and verifying Web service flow based on annotated OWL-S. By using annotation layers, an OWL-S Process model can be extended with constraints (like time properties) information. An underlying OWL-S model and one or more constraint annotation layers compose a complete specification of a Web process imposed specific constraints. By transforming the annotated OWL-S model to a TCPN (Time Constraints Petri net) model, Web service flow can be analyzed and verified. The method allows us to add conveniently constraints information to a Web process model, and state whether a process satisfies given properties without actual execution based on its specification.},
 duplicado = {false},
 inserir = {false},
 title = {Model Checking for Web Service Flow Based on Annotated OWL-S},
 year = {2008}
}

@article{1497,
 abstract = {Abstract:
Design patterns provide experience reusability and increase quality of object oriented designs. Knowing which design patterns are implemented in a software is important in comprehending, maintaining and refactoring its design. However, despite the interest in using design patterns, traditionally, their usage is not explicitly documented. Therefore, a method is required to reveal this information from some artifacts of the systems (e.g. source codes, models, and executables). In this paper, an approach is proposed which uses the Semantic Web technologies for automatically detecting design patterns from Java source code. It is based on the semantic data model as the internal representation, and on SPARQL query execution as the analysis mechanism. Experimental evaluations demonstrate that this approach is both feasible and effective, and it reduces the complexity of detecting design patterns to creating a set of SPARQL queries.},
 duplicado = {false},
 inserir = {false},
 title = {A Semantic Web based approach for design pattern detection from source code},
 year = {2012}
}

@article{1499,
 abstract = {In web service times, the techniques for composing services are based on service reuse and automatic integration. A new web service will be generated by composing some existing web services. These web services cooperate with each other to provide a new more complex function. It is necessary and very important to test the interaction behavior between any two web services during composition. In this paper, a kind of enhanced hierarchical color petri-net (or EH-CPN) is introduced to generate test cases for testing the interaction, where EH-CPN is transformed from OWL-S document, and both control flow and data flow information in EH-CPN are analyzed and used to generate an executable test sequence, and further test cases are created by combining the test sequence and test data in an XML file.},
 duplicado = {false},
 inserir = {true},
 title = {GENERATING TEST CASES OF COMPOSITE SERVICES BASED ON OWL-S AND EH-CPN},
 year = {2010}
}

@article{1500,
 abstract = {Abstract:
Software testing in a service oriented and/or cloud computing environment is made challenging by its dynamic and loosely coupled nature. In this work, we propose and demonstrate a methodology for capturing changes and reconfiguring test data for an atomic service based on changes in the WSDL service description. Based on our results we believe that our technique can be extended further to capture changes in composite services and services that run on the Software as a Service (SaaS) platform in cloud computing.},
 duplicado = {false},
 inserir = {false},
 title = {Test Reconfiguration for Service Oriented Applications},
 year = {2011}
}

@article{1506,
 abstract = {Abstract:
The Web service technology is becoming the mainstream for implementing the distributed applications based on architecture oriented service. Simple Web service couldn't deal with the requirements of complicated distributed system. Some relevant Web services have to be integrated to a composite service to overcome it. Now, more attentions are paid to the quality of service of composite Web service. The OWL-S is an important specification to construct composite Web service for realizing the goals of discovering, composing and executing Web services automatically, but it doesn't provide the ways to verify its correctness. Petri net is a mature and visual formal method to verifying the distributed systems based on rigid mathematical ground. This paper focuses on verifying the correctness of composite Web services defined by OWL-S. The rules are presented for translating the composite Web service defined by OWL-S into corresponsive Petri Net described by xml style, and a prototype is implemented based on these rules. With an open source Petri net analyzing tool, the correctness and other properties could be verified and analyzed. An example shows that the presented method is feasible.},
 duplicado = {false},
 inserir = {false},
 title = {Verifying web services of OWL-S with Petri net},
 year = {2010}
}

@article{1507,
 abstract = {Abstract:
The paper presents a timing constraint modeling and analyzing method for Web processes based on annotated OWL-S. The method allows us to find whether Web processes are schedulable and which processes in Web processes are not schedulable without actual execution based on its specification. This also suggests replacement of corresponding components or relaxing timing constraints. The contributions of the paper mainly include two aspects: 1) The paper presents a method for extending OWL-S model based on annotation layers. By using annotation layers, the OWL-S model is extended with auxiliary information such as time constraints. This method can be extended to verify other properties of the Web processes by using different annotation layers. 2) The paper describes how to model Web services based systems and determine whether the specification satisfies time consistency. First, we model a Web process in extended OWL-S with timing constraints. Then, we transform the system specification along with its imposed timing constraints into a TPPN (time predicate Petri net) model. Final, TPPN analyzer can automatically simulate and analyze to verify whether the system specification is schedulable under the imposed timing constraints.},
 duplicado = {false},
 inserir = {false},
 title = {A Framework for Time Consistency Verification for Web Processes Based on Annotated OWL-S},
 year = {2007}
}

@article{1510,
 abstract = {Abstract
Due to their high level of abstraction, web services blur the traditional distinction between model and code. Conse-quently, executable artifacts can now be written to a level of abstraction that more closely approximates that of mod-els used for model checking. Unlike traditional localized white-box software artifacts, the distributed black-box na-ture of web services demand exhaustive verification. Re-cent usage trends for model checkers coupled with the rise of web services, suggest a different approach to this com-putationally intensive form of verification. This paper out-lines these trends and describes a web-enabled approach to verification of web services. We outline how activities as-sociated with model checking may be migrated into a web services framework, enabling practitioners to more easily incorporate model checking into their solutions.},
 duplicado = {false},
 inserir = {false},
 title = {Toward Model Checking Web Services over the Web},
 year = {2008}
}

@article{1511,
 abstract = {Abstract

Industrial cyber-physical systems require complex software to orchestrate heterogeneous mechatronic components and control physical processes. This software is typically developed and refined iteratively in a model-driven fashion. Testing such multi-dimensional systems is extremely difficult as subsequent refinements may not correspond accurately with previous system models.

We propose a framework to generate test-cases from functional requirements at all stages in the model-driven engineering process. A requirements ontology initially created during requirements engineering is iteratively refined such that test-cases can be generated automatically. An industrial water process system case study illustrates the strengths of the proposed formalism. We also present an automatic test-case generation and execution tool called REBATE (REquirements Based Automatic Testing Engine).},
 duplicado = {false},
 inserir = {false},
 title = {Automatic test case generation from requirements for industrial cyber-physical systems},
 year = {2016}
}

@article{1512,
 abstract = {Abstract:
Timing constraints verification (such as time consistency) for service flow has become indispensable since many services for e-business are distributed in different places with temporal context. Unfortunately, current researches mainly focus on the modeling, analyzing and verifying of service processes, while ignoring temporal factors. To address the challenges of temporal properties verification for service, the paper presents a method for verifying time constraints consistency of service flow. By building time ontology based on DAML_Time, OWL_S is extended with time constraints, which expresses time information of service flow clearly and roundly. Then the annotated OWL_S is transformed to some kinds of formal models and verification algorithm is presented. A prototype system MC4WST is also designed to prove feasibility of the verification process.},
 duplicado = {false},
 inserir = {false},
 title = {A Verification Method for Temporal Consistency of Service Flow},
 year = {2008}
}

@article{1514,
 abstract = {Abstract:
The adequate testing of black-box components is an important basis before they will be reused in the approach of Component Based Software Development. The test-data generation and test adequacy ensuring are difficult issues for the unavailability of the source code of black-box components. In this paper, an extended component interface specification model is proposed to support the component understanding, testing and reuse. Then the function of different kinds of specification elements in testing is defined. Based on the syntactic and semantic specifications, the proposed test-data generation method can produce test suite meeting a certain mutation score, which is viewed as a kind of effective test adequacy criterion. Finally, some experiments were carried and the results have shown that the different kinds of specification can support the testing of black-box components.},
 duplicado = {false},
 inserir = {false},
 title = {The Support of Interface Specifications in Black-box Components Testing},
 year = {2010}
}

@article{1515,
 abstract = {The testing of service-based applications is an important but challenging activity. Especially the integration testing is a difficult task that needs to cope with the message-based communication in the service-oriented world. In this thesis, a model-based approach to service integration and integration testing is proposed. The necessary research work to realize its phases is the main contribution of the dissertation. First, MCM, a domain-specific language for service choreography modeling, is introduced together with a precise semantics that makes it suitable for integration testing. Then, a framework for generating service integration tests is presented, incorporating three different model-based test generation techniques that can be chosen according to the test context. Further, it is explained how the generated test cases are transformed into concrete test scripts, thus enabling their execution on an enterprise service-based application. Finally, the conducted case study of the MCM-based approach in an industrial setting is described.},
 duplicado = {false},
 inserir = {false},
 title = {Modeling and Model-based Testing of Service Choreographies},
 year = {2010}
}

@article{1516,
 abstract = {Abstract
As testing has been playing an important role in guaranteeing quality of Web service in respect of E-learning, the traditional testing of Web services presents its inability such as time-consuming and flawed testing procedures as opposed to desired measures in quickly and thoroughly scanning Web services for detects. To reduce testing cost and enhance testing efficiency, our testing measure for E-learning Web services is to describe services with an annotation mixed with the Object Constrain Language (OCL) and Semantic Annotations for WSDL and XML Schema (SAWSDL), parse OCL pre-conditions and OCL post-conditions, and generate test cases with class division and boundary values. An experimental case indicates the feasibility of the proposed approach.},
 duplicado = {false},
 inserir = {false},
 title = {OCL-Based Testing for E-Learning Web Service},
 year = {2010}
}

@article{1518,
 abstract = {Preventing bad things from happening to engineered systems, demands improvements
to how we model their operation with regard to safety. Safety-critical and
fiscally-critical systems both demand automated and exhaustive verification, which
is only possible if the models of these systems, along with the number of scenarios
spawned from these models, are tractably finite. To this end, this dissertation addresses
problems of a model's tractability and usefulness. It addresses the state space
minimization problem by initially considering tradeoffs between state space size and
level of detail or fidelity. It then considers the problem of human interpretation in
model capture from system artifacts, by seeking to automate model capture. It introduces
human control over level of detail and hence state space size during model
capture. Rendering that model in a manner that can guide human decision making
is also addressed, as is an automated assessment of system timeliness. Finally, it addresses
state compression and abstraction using logical fault models like fault trees,
which enable exhaustive verification of larger systems by subsequent use of transition
fault models like Petri nets, timed automata, and process algebraic expressions. To
illustrate these ideas, this dissertation considers two very different applications - web
service compositions and submerged ocean machinery. },
 duplicado = {false},
 inserir = {false},
 title = {FINITE SAFETY MODELS FOR HIGH-ASSURANCE SYSTEMS},
 year = {2010}
}

@article{1525,
 abstract = {Abstract:
Industrial cyber-physical systems require complex distributed software to orchestrate many heterogeneous mechatronic components and control multiple physical processes. Industrial automation software is typically developed in a model-driven fashion where abstractions of physical processes called plant models are co-developed and iteratively refined along with the control code. Testing such multi-dimensional systems is extremely difficult because often models might not be accurate, do not correspond accurately with subsequent refinements, and the software must eventually be tested on the real plant, especially in safety-critical systems like nuclear plants. This paper proposes a framework wherein high-level functional requirements are used to automatically generate test cases for designs at all abstraction levels in the model-driven engineering process. Requirements are initially specified in natural language and then analyzed and specified using a formalized ontology. The requirements ontology is then refined along with controller and plant models during design and development stages such that test cases can be generated automatically at any stage. A representative industrial water process system case study illustrates the strengths of the proposed formalism. The requirements meta-model proposed by the CESAR European project is used for requirements engineering while IEC 61131-3 and model-driven concepts are used in the design and development phases. A tool resulting from the proposed framework called REBATE (Requirements Based Automatic Testing Engine) is used to generate and execute test cases for increasingly concrete controller and plant models.},
 duplicado = {false},
 inserir = {false},
 title = {Requirements-Aided Automatic Test Case Generation for Industrial Cyber-physical Systems},
 year = {2015}
}

@article{1527,
 abstract = {Abstract:
Service requirements documentation plays a crucial role on the quality of service-oriented systems to be developed. A large amount of service requirements are documented in the form of natural language, which are usually human-centric and therefore error-prone and inaccurate. In order to improve the quality of service requirements documents, we propose a service requirements modeling and validation method using workflow patterns. First, it extracts the process information using natural language processing tools. Then it formalizes the process information with a requirements modeling language - Workflow-Patterns-based Process Language (WPPL). Finally, the defects existed in service requirements are checked against a set of checking rules by matching with workflow patterns. A financial service example - Trade Order - was used to illustrate our approach},
 duplicado = {false},
 inserir = {false},
 title = {Using Workflow Patterns to Model and Validate Service Requirements},
 year = {2016}
}

@article{1531,
 abstract = {Traditional test oracles have two problems. Firstly, several test oracles are needed for a single software
program to perform different functions and maintaining a large number of test oracles is tedious and
might be prone to errors. Secondly, testers usually test only the important criteria of a web application,
since its time consuming to check with all the possible criteria.

Ontologies have been used in a wide variety of domains and they have also been used in software
testing. However, they have not been used for test oracle automation. The main idea of this thesis is to
define a procedure for how ontology-based test oracle automation can be achieved for testing web
applications and minimize the problems of traditional test oracles. The proposed procedure consists of
the following steps: first, the expected results are stored in ontology A by running previous working
version of the web application; second, the actual results are stored in ontology B by running the web
application under test at runtime; and finally, the results of both ontology A and B are compared. This
results in an automated test oracle comparator. Evaluation includes how the proposed procedure
minimizes the traditional test oracle problems and by identifying the benefits of the defined procedure.},
 duplicado = {false},
 inserir = {true},
 title = {An Ontology-based Automated Test Oracle Comparator for Testing Web Applications},
 year = {2011}
}

@article{1536,
 abstract = {Abstract:
Formal verification of Web service composition is an important means for ensuring the quality of Web service. The Web service composition modeling is a key step of the validation of Web services interaction. Based on the semantic consistency of the Web services description semantic OWL-S and Coloured Petri net, this dissertation proposes a modeling method of Web services interaction based on the OWL-S and Coloured Petri Net and the description rules of OWL-S service operation semantics by CPN. And the analysis and verification of this modeling method has also been done.},
 duplicado = {false},
 inserir = {false},
 title = {Modeling method of ontology web service interaction based on Coloured Petri Net},
 year = {2014}
}

@article{1539,
 abstract = {Abstract
Ontologies are an essential component of semantic knowledge bases and applications, and nowadays they are used in a plethora of domains. Despite the maturity of ontology languages, support tools and engineering techniques, the testing and validation of ontologies is a field which still lacks consolidated approaches and tools. This paper attempts at partly bridging that gap, taking a first step towards the extension of some traditional software testing techniques to ontologies expressed in a widely-used format. Mutation testing and coverage testing, revisited in the light of the peculiar features of the ontology language and structure, can can assist in designing better test suites to validate them, and overall help in the engineering and refinement of ontologies and software based on them.},
 duplicado = {false},
 inserir = {true},
 title = {Software Testing Techniques Revisited for OWL Ontologies},
 year = {2016}
}

@article{1540,
 abstract = {As Web Services draw modules within and across enterprises, dynamically and belligerently testing Web Services has become crucial. Comprehensive Functional, Concert, Interoperability and Susceptibility Testing form the Pillars of Web Services Testing. Only by adopting a comprehensive testing department, enterprises can safeguard that their Web Services is robust, scalable, interoperable, and secure. Overall functionality of web services would be informal towards test. But, only if we methodically trust the applications components (services) before we combine them to complete the application. In current scenario web service technology comprehends various testing apparatuses for manipulating and generating the test cases. But these tools and approaches were negotiating security and execution time and consume more resources. The existing methodologies will generate test cases for the low end web services and limited number of requests, due to these constraints we built new testing framework. In this paper we introduced the new basis with testing of actions, scripts and link for web services by the use of test cases. For this approach we used SOAP web services with SOA. The test case generation and testing reports will gives the accurate testing results and test cases. These test cases are generated using Java JUnit testing tool. We implemented our approach in a java based platform for efficient and secure manner.},
 duplicado = {false},
 inserir = {false},
 title = {The Framework for Testing of Web Services through Actions in Addition to Scripts},
 year = {2014}
}

@article{1545,
 abstract = {
Organizational workflows are now automated by incorporating the dynamic composite web services. As more and more web service is evolving now, enforcing the Quality of service (QoS) factors in web service is in need today. Among the various QoS factor, reliability play a vital role. Reliability is defined by using the functional and non functional metrics. Functional metrics are correctness, fault tolerance and testability of the web service. Non functional metrics are timeliness and Interoperability. In this paper, a model is proposed to verify and enhance the correctness of the web service in the composite web service environment.},
 duplicado = {false},
 inserir = {false},
 title = {Verifying and Enhancing the Correctness of the Dynamic Composite Web Service},
 year = {2011}
}

@article{1546,
 abstract = {The reliability of web services is important for both users and software developers. In order to guarantee the
reliability of the web services that are invoked and integrated at runtime, mechanisms for automatic testing
of web services are needed. A basic issue in web service testing is to be able to generate appropriate input
values for web services and to estimate whether the output obtained is proper for the functionality. In this
work, we propose a method for automatic web service testing that uses semantics dependency-based and data
mutation-based techniques to generate different test cases and to analyze web services. We check whether the
services function properly under the input values generated and enriched from various data sources and we
check robustness of web services by generating random and erronous data inputs. Experimental evaluation
with real web services show that proposed mechanisms provide promising results for automatic testing of web
services.},
 duplicado = {false},
 inserir = {false},
 title = {Testing Discovered Web Services Automatically},
 year = {2014}
}

@article{1547,
 abstract = {Knowledge representation is a subarea of artificial intelligence concerned with using formal symbols to represent a set of facts within a knowledge domain. Two popular knowledge representation languages, namely Petri net and ontology, are promising knowledge sharing and reusing methods in knowledge engineering. The combination of Petri net and ontology can facilitate achieving complementary advantages. Currently, many efforts have been done on knowledge sharing between Petri nets and ontologies. To investigate these issues and more importantly serve as identifying the direction of knowledge sharing between Petri nets and ontologies, in this paper we give a comprehensive literature overview of knowledge sharing between Petri net models and ontology models to satisfy the obvious need. In detail, we discuss the knowledge sharing from two aspects: the different knowledge representation approaches of ontology to represent and reason Petri net and issues of constructing Petri net from ontology. In addition, other important issues on applications and directions for future research are discussed in detail.},
 duplicado = {false},
 inserir = {false},
 title = {A literature overview of knowledge sharing between Petri nets and ontologies},
 year = {2016}
}

@article{1549,
 abstract = {Abstract: Now a day's many businesses publish their applications utilities on the web. The desideratum for supporting the
classification and semantic annotation of services constitutes an important challenge for service centric software
engineering. Such a semantic annotation may require, in turn, to be made in acceptance to a specific ontology. Also, a
service description needs to felicitously relate with other similar services. For a particular service request to relate an implicit
service description, this paper overcomes the issues in web service discovery using semantics. The service request along with
web service composition is performed using categorization of semantic based service and enhancement in semantics in an
ontology frame work. },
 duplicado = {false},
 inserir = {false},
 title = {Ontology based personalized information from semantic Web},
 year = {2014}
}

@article{1550,
 abstract = {During the execution of a test plan, a test manager may decide to drop a test case if its result can be inferred from already executed test cases. We show that it is possible to automatically generate a test plan to exploit the potential to justifiably drop a test case and thus reduce the number of test cases. Our approach uses Boolean formulas to model the mutual dependencies between test results. The algorithm to generate a test plan comes with the formal guarantee of optimality with regards to the inference of the result of a test case from already executed test cases.},
 duplicado = {false},
 inserir = {false},
 title = {A Logical Approach to Generating Test Plans},
 year = {1612}
}

@article{1555,
 abstract = {A major bottleneck for a wider deployment and use of ontologies and knowledge engineering techniques
is the lack of established conventions along with cumbersome and inefficient support for vocabulary and
ontology authoring. We argue, that the pragmatic development by convention paradigm well-accepted within
software engineering, can be successfully applied for ontology engineering, too. However, the definition of
a valid set of conventions requires broadly-accepted best-practices. In this regard, we empirically analyzed a
number of popular vocabularies and ontology development efforts with respect to their use of guidelines and
common practices. Based on this analysis, we identified the following main aspects of common practices:
documentation, internationalization, naming, structure, reuse, validation and authoring. In this paper, these
aspects are presented and discussed in detail. We propose a set of practices for each aspect and evaluate their
relevance in a study with vocabulary developers. The overall goal is to pave the way for a new paradigm
of vocabulary development similar to Software Development by Convention, which we name Vocabulary
Development by Convention.},
 duplicado = {false},
 inserir = {false},
 title = {Towards Vocabulary Development by Convention},
 year = {2015}
}

@article{1556,
 abstract = {Abstract : In model-based engineering (MBE), the abstraction power of models is used to deal with the ever increasing complexity of modern software systems. As models play a central role in MBE-based development processes, for the adoption of MBE in practical projects it becomes indispensable to introduce rigorous methods for ensuring the correctness of the models. Consequently, much effort has been spent on developing and applying validation and verification (V&V) techniques for models. However, there are still many open challenges. In this paper, we shortly review the status quo of V&V techniques in MBE and derive a catalogue of open questions whose answers would contribute to successfully putting MBE into practice.},
 duplicado = {false},
 inserir = {false},
 title = {Research Questions for Validation and Verification in the Context of Model-Based Engineering},
 year = {2013}
}

@article{1558,
 abstract = {In using Modeling and Simulation for the system Verification & Validation activities, often the
difficulty is finding and implementing consistent abstractions to model the system being simulated
with respect to the simulation requirements. A proposition for the unified design and
implementation of modeling abstractions consistent with the simulation objectives based on the
computer science, control and system engineering concepts is presented. It addresses two
fundamental problems of fidelity in simulation, namely, for a given system specification and some
properties of interest, how to extract modeling abstractions to define a simulation product
architecture and how far does the behaviour of the simulation model represents the system
specification. A general notion of this simulation fidelity, both architectural and behavioural, in
system verification and validation is explained in the established notions of the experimental frame
and discussed in the context of modeling abstractions and inclusion relations. A semi-formal
ontology based domain model approach to build and define the simulation product architecture is
proposed with a real industrial scale study. A formal approach based on game theoretic quantitative
system refinement notions is proposed for different class of system and simulation models with a
prototype tool development and case studies. Challenges in research and implementation of this
formal and semi-formal fidelity framework especially in an industrial context are discussed.},
 duplicado = {false},
 inserir = {false},
 title = {Simulation product fidelity: A qualitative & quantitative system engineering approach},
 year = {2010}
}

@article{1559,
 abstract = {Despite the existence of number of well-known conceptualization in e-Business and e-Commerce, there have been no efforts so far to develop a detailed, comprehensive conceptualization for business model. Current business literature is replete with fragmented conceptualizations, which only partially describe aspects of a business model. In addition, the existing conceptualizations do not explicitly support the emerging phenomenon of open government data -- an increasingly valuable economic and strategic resource. Consequently, no comprehensive, formal, executable open government data business model ontology exists, that could be directly leveraged to facilitate the design, development of an operational open data business model. This paper bridges this gap by providing a parsimonious yet sufficiently detailed, conceptualization and formal ontology of open government data business model for open data-driven organizations. Following the design science approach, we developed the ontology as a 'design artefact' and validate the ontology by using it to describe an open data business model of an open data-driven organization.},
 duplicado = {false},
 inserir = {false},
 title = {An Ontology for Open Government Data Business Model},
 year = {2017}
}

@article{1564,
 abstract = {Abstract Software systems need to be constantly tested, either
to verify changes or to check conformance to requirements. The
current leading approaches to automate GUI tests are coding
and the use of Capture & Replay (C&R) tools. Coding is
usually associated with (even if ad hoc) reuse strategies, but
requires from the developer specialized knowledge about the
adopted framework. On the other hand, even though C&R is
able to promote faster automation, it raises maintainability and
scalability issues in the long term due to scripts scattering and
rework for each new test case, because usually there is no
associated reuse strategy. In order to combine the benefits of
both approaches, we propose: an abstract and framework-free
representation of test actions captured during testing activities; a
text-based strategy that matches a new test case with previously
recorded test actions; and a C&R tool that implements these
concepts in the mobile context. We developed and evaluated our
strategy in the context of a partnership with Motorola Mobility,
achieving a reuse ratio up to 71% with time gains similar to
traditional C&R approaches when compared to coding.
Keywords-test automation; capture and replay; reuse; mobile
applications; natural language processing},
 duplicado = {false},
 inserir = {false},
 title = {Capture & Replay with Text-Based Reuse and Framework Agnosticism},
 year = {1996}
}

@article{1566,
 abstract = {Abstract - Up-to-date software design documentation is
valuable for maintenance engineers, testers and developers
joining a project at a later stage; however, UML Models, e.g.
class diagrams, are often poorly kept up-to-date during
development and maintenance. Reverse engineering,
therefore, has become a popular method to recover an up-todate
design from the underlying source code. However current
techniques yet produce a detailed representation of the
underlying source code that would reduce the
understandability. In order for the understandability to
enhance, condensation of the reverse engineered diagrams
has been proposed as a solution. However, current state-ofthe-art
approaches in this area still demand a need for
improving or alternative approaches. Consistently, in this
paper we are putting forward a bridging idea of using the
semantic web technologies for improving the condensation
process. Two contributions are proposed that support each
other. Firstly, the V-Ontmodel is suggested for updating
software documentation over the software evolution. Secondly,
a general architecture is proposed enabling us to reduce
sophisticated analysis tasks for the condensation process of
class diagrams to a few queries in SPARQL or its extensions
like SPARQL-ML. To discuss the feasibility of the approach
we focus on the condensation of reverse engineered class
diagrams in the whole paper. Finally, an illustration example
is presented in which helper classes are excluded from the
class diagrams in the condensation process regarding the
structural patterns extracted from the software metadata.},
 duplicado = {false},
 inserir = {false},
 title = {Condensation of Reverse Engineered UML Diagrams by Using the Semantic Web Technologies},
 year = {2015}
}

@article{1567,
 abstract = {Test oracle methods have changed significantly over time, which has resulted in
clear shifts in the research literature. Over the years, the testing techniques, strategies,
and criteria utilized by researchers went through technical developments due to the
improvement of technologies and programming languages. Software testing designers,
known as testers, currently have several resources to increase their confidence in the
software under test correctness. All of these Software Testing resources are supposed
to include a mechanism to decide whether a particular execution is considered a failure
or not. In Software Testing environments, this decision is the responsibility of the test
oracle. Despite the evolution and adaptation of testing techniques over more than
30 years, test oracles remain a particular and relevant issue. In this chapter, using
literary evidence from a pool of about 300 studies directly related to test oracles, we
present a classification of test oracles based on a taxonomy that considers their source
of information and notations. Based on this classification, we perform a quantitative
analysis to highlight the shifts in (evolution of) research on test oracles. Exploring
geographical and quantitative information, we analyzed the maturity of this field using
co-authorship networks among studies published between 1978 and 2013. Further, we
determine the most prolific authors and their countries, main conferences and journals,
supporting tools, academic efforts, and use a comparative analysis between academia and
industry. Finally, from these analyzes we draw an analytic reflection about contemporary
test oracle approaches and a criticism about oracle trends.},
 duplicado = {false},
 inserir = {false},
 title = {Automated Test Oracles: State of the Art, Taxonomies and Trends},
 year = {2014}
}

