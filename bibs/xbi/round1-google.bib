@article{318,
 abstract = {Abstract:
As Web applications are more widely used and the browser and operating system becomes more diversiform, cross-browser incompatibility becomes more and more serious which may lead to visual faults or malfunction. The existing tools are difficult to pinpoint incompatibility. We propose a model comparison-based compatibility testing method. CRAWLJAX, an open source crawler tool, is used to build the behavioral model of the Web application under testing. Two models are generated by crawling the application in the reference browser and the target browser respectively, and then they are compared to find and locate incompatibility issues. It reduces the workload of the test and repair. Finally, we use a simple example to illustrate our approach.},
 duplicado = {false},
 inserir = {true},
 title = {Cross-Browser Compatibility Testing Based on Model Comparison},
 year = {2015}
}

@article{320,
 abstract = {Abstract The benefits of using a software testing tool in order
to achieve significant reductions in cost and time to market,
and, at the same time, increasing the quality has encouraged
the adoption of testing tools both in single systems and product
lines. In this context, this study focuses on the following goals:
analyze how the available tools are supporting the Software
Product Lines (SPL) Testing Process, investigate the state-of-theart
on single system and SPL testing tools, synthesize available
evidence, and identify gaps among the tools, available in the
literature. A mapping study was undertaken to analyze important
aspects that should be considered when adopting testing tools. A
set of four research questions were defined in which 33 studies,
dated from 1999 to 2011, were evaluated. From the total of 33
studies considered, 24 of them described single system testing
tools and the other 9 described SPL testing tools. However, there
is insufficient information about publications describing tools
used in the industry. There is no tool suitable to all testing levels
of a SPL, researchers need to consider the feasibility of adapting
existing tools or constructing new tools.},
 duplicado = {false},
 inserir = {false},
 title = {A Mapping Study on Software Product Lines Testing Tools},
 year = {2012}
}

@article{321,
 abstract = {Abstract:
Web application have gained increased acceptance over the years in companies and organization as the world move to a global village. Software developers have also grown interest in developing web applications compared to stand-alone application because of the immense benefits it offers such as ubiquity, platform dependence, low cost of support and maintenance, better speed and performance, piracy proof etc. As mobile application emerged in the last decade, attention has been focused on mobile applications by organizations and businesses in order to maximize their profits as much as possible. There has been a rapid increase of software release in the mobile applications store. As the growth of both web application and mobile application increase, the question of quality assurance remains a concern. A comparative study of software testing techniques can be performed to improve the standard of testing of both web and mobile application. This paper therefore reviews the similarity and difference in the testing mechanism.},
 duplicado = {false},
 inserir = {false},
 title = {A Comparative Study of Web Application Testing and Mobile Application Testing},
 year = {2015}
}

@article{323,
 abstract = {Abstract Much work has been done on automating regression
testing for Web applications, but most of them focus on test
data generation or test execution. Little work has been done
on automatically determining if a test passed or failed; testers
would need to visually confirm the result which can be a tedious
task. The difficulty is compounded by the fact that parts of a
Web page (such as advertisements) may change each time the
Web application is executed even though it has no bearing on the
Web application function itself. We thus propose a test oracle for
automatically determining the result of regression testing a Web
application. The key point of our approach is the identification
of parts that may change, which we call variable region. We
first generate the expected result, by executing the original (premodification)
Web application multiple times so that variable
regions can be identified. Then, after the Web application is
modified, regression testing is conducted by comparing the output
of the modified Web application against the expected output. An
evaluation confirmed the usefulness of our approach.},
 duplicado = {false},
 inserir = {false},
 title = {An Oracle based on Image Comparison for Regression Testing of Web Applications},
 year = {2015}
}

@article{324,
 abstract = {Cross-Browser Incompatibilities (XBIs) represent inconsistencies in Web Application when introduced in different browsers. The growing number of implementation of browsers (Internet Explorer, Microsoft Edge, Mozilla Firefox, Google Chrome) and the constant evolution of the specifications of Web technologies provided differences in the way that the browsers behave and render the web pages. The web applications must behave consistently among browsers. Therefore, the web developers should overcome the differences that happen during the rendering in different environments by detecting and avoiding XBIs during the development process. Many web developers depend on manual inspection of web pages in several environments to detect the XBIs, independently of the cost and time that the manual tests represent to the process of development. The tools for the automatic detection of the XBIs accelerate the inspection process in the web pages, but the current tools have little precision, and their evaluations report a large percentage of false positives. This search aims to evaluate the use of Artificial Neural Networks for reducing the numbers of false positives in the automatic detection of the XBIs through the CSS (Cascading Style Sheets) and the relative comparison of the element in the web page.},
 duplicado = {false},
 inserir = {true},
 title = {Deteccao Automatica de Incompatibilidades Cross-Browser utilizando Redes Neurais Artificiais},
 year = {2016}
}

@article{325,
 abstract = {Abstract Modern software applications are very complex and they need fre-quent changes
as per the changes in user requirements. These applications are developed using the
combination of various different programming languages. They consist of a multi-tiered},
 duplicado = {false},
 inserir = {false},
 title = {Automated Techniques to Detect Faults Early in Large Software Applications},
 year = {2014}
}

@article{326,
 abstract = {The current application is directed to methods and systems for designing and configuring web-site testing and analysis. In certain implementations, a testing service collects customer page-access and conversion information on behalf of a web site. The testing service is straightforwardly accessed and configured, through a web-site-based graphical user interface, and is virtually incorporated into the web site.},
 duplicado = {false},
 inserir = {false},
 title = {Graphical-user-interface-based method and system for designing and configuring web-site testing and analysis},
 year = {2016}
}

@article{327,
 abstract = {Modern software applications need to run on a variety of web and mobile platforms with diverse software and hardware-level features. Thus, developers of such software need to duplicate the testing and maintenance effort on a wide range of platforms. Often developers are not able to cope with this increasing demand and release software that is broken on certain platforms, thereby affecting a class of customers using such platforms. Hence, there is a need for automating such duplicate activities to assist the developer in coping with the ever increasing demand. The goal of my work is to improve the testing and maintenance of cross-platform web and mobile applications by developing automated techniques for comparing and matching the behavior of such applications across different platforms. To achieve this goal, I have identified three problems that are relevant in the context of cross-platform testing and maintenance: 1) automated identification of inconsistencies in the same application's behavior across multiple platforms, 2) detecting features that are present in the application on one platform, but missing on another platform version of the same application, and, 3) automated migration of test suites and possibly other software artifacts across platforms. I present three different scenarios for the development of {cross-platform} web and mobile applications, and formulate each of the three problems in the scenario where it is most relevant. To address and mitigate these problems in their corresponding scenarios, I present the principled design, development and evaluation of the two techniques, and a third preliminary technique to highlight the research challenges of test migration. The first technique, X-pert identifies inconsistencies in a web application running on multiple web browsers. The second technique, FMAP matches features between the desktop and mobile versions of a web application and reports any features found missing on either of the platform versions. The final technique, MigraTest attempts to automatically migrate test cases from a mobile application on one platform to its counterpart on another platform. To evaluate these techniques, I implemented them as prototype tools and ran these tools on real-world subject applications. The empirical evaluation of X-pert shows that it is accurate and effective in detecting real-world inconsistencies in web applications. In the case of FMAP, the results of my evaluation show that it was able to correctly identify missing features between desktop and mobile versions of the web applications considered, as confirmed by my analysis of user reports and software fixes for these applications. The third technique, MigraTest was able to efficiently migrate test cases between two mobile platform versions of the subject applications.},
 duplicado = {false},
 inserir = {true},
 title = {Cross-platform testing and maintenance of web and mobile applications},
 year = {2014}
}

@article{328,
 abstract = {JavaScript has become one of the most prevalent programming languages. Unfortunately, some of the unique
properties that contribute to this popularity also make JavaScript programs prone to errors and difficult for
program analyses to reason about. These properties include the highly dynamic nature of the language, a set
of unusual language features, a lack of encapsulation mechanisms, and the no crash philosophy. This paper
surveys dynamic program analysis and test generation techniques for JavaScript targeted at improving the
correctness, reliability, performance, security, and privacy of JavaScript-based software.
},
 duplicado = {false},
 inserir = {false},
 title = {A Survey of Dynamic Analysis and Test Generation for JavaScript},
 year = {2017}
}

@article{330,
 abstract = {Abstract:
Many APIs enable cross-platform system development by abstracting over the details of a platform, allowing application developers to write one implementation that will run on a wide variety of platforms. Unfortunately, subtle differences in the behavior of the underlying platforms make cross-platform behavior difficult to achieve. As a result, applications using these APIs can be plagued by bugs difficult to observe before deployment. These portability bugs can be particularly difficult to diagnose and fix because they arise from the API implementation, the operating system, or hardware, rather than application code. This paper describes CheckAPI, a technique for detecting violations of cross-platform portability. CheckAPI compares an application's interactions with the API implementation to its interactions with a partial specification-based API implementation, and does so efficiently enough to be used in real production systems and at runtime. CheckAPI finds latent errors that escape pre-release testing. This paper discusses the subtleties of different kinds of API calls and strategies for effectively producing the partial implementations. Validating CheckAPI on JavaScript, the Seattle project's Repy VM, and POSIX detects dozens of violations that are confirmed bugs in widely-used software.},
 duplicado = {false},
 inserir = {false},
 title = {Detecting latent cross-platform API violations},
 year = {2015}
}

@article{331,
 abstract = {Abstract The advancement in web technology and popularity of web applications amplifies the inconsistencies
between various web browsers. These inconsistencies augment cross browser incompatibilities that constitute different
look on different browsers for a particular web application. In some cases, Cross-Browser Inconsistencies (XBIs)
consists of acceptable difference whereas these may entirely prevent users from accessing part of a web applications
functionality in other cases. Therefore, testing process of a web application must be performed comprehensively on
multiple browsers to achieve consistency. Available tools and techniques require a considerable manual effort to
recognize such issues and provide limited support for fixing the cause of the issues. In this paper, we propose a
technique for detecting cross-browser issues without human intervention.},
 duplicado = {false},
 inserir = {true},
 title = {Detection of Cross Browser Inconsistency by Comparing Extracted Attributes},
 year = {2017}
}

@article{332,
 abstract = {Abstract The growing diversity of web-client platform
configurations causes websites to vary unpredictably, creating a
myriad of challenges during software development life cycle
(SDLC). This eventually affects websites user experience (UX).
Exploratory heuristic evaluation (EHE) and lab-based usability
testing (LBUT) are popular usability evaluation methods (UEM)
that could be used to measure and improve the websites UX.
Hence, the objective of this study is to derive and validate the
EHE and LBUT process flow for website cross browser
compatibility. In addition to finding compatibility defects using
EHE, it is important to determine the reliability of these defects
using LBUT. When it comes to improving websites UX, root
cause analysis (RCA) is performed and recommendation are
provided to the design and development team. From the results,
it can be concluded that the compatibility guideline and process
flow developed can improve the productivity and reliability of the
EHE and LBUT methodologies.
},
 duplicado = {false},
 inserir = {false},
 title = {Measuring and Improving Website User Experience using UX Methodologies: A Case Study on Cross Browser Compatibility Heuristic},
 year = {2012}
}

@article{333,
 abstract = {Abstract:
Presentation failures in a website can negatively impact end users' perception of the quality of the website, the services it delivers, and the branding a company is trying to achieve. Presentation failures can occur easily in modern web applications because of the highly complex and dynamic nature of the HTML, CSS, and JavaScript that define a web page's visual appearance. Debugging such failures manually is time consuming and error-prone, and existing techniques do not provide an automated debugging solution. In this paper, we present our tool, WebSee, that provides a fully automated debugging solution for presentation failures in web applications. When run on real-world web applications, WebSee was able to accurately and quickly identify faulty HTML elements.},
 duplicado = {false},
 inserir = {false},
 title = {WebSee: A Tool for Debugging HTML Presentation Failures},
 year = {2015}
}

@article{334,
 abstract = {Abstract:
Presentation failures in a website can undermine its success by giving users a negative perception of the trustworthiness of the site and the quality of the services it delivers. Unfortunately, existing techniques for debugging presentation failures do not provide developers with automated and broadly applicable solutions for finding the site's faulty HTML elements and CSS properties. To address this limitation, we propose a novel automated approach for debugging web sites that is based on image processing and probabilistic techniques. Our approach first builds a model that links observable changes in the web site's appearance to faulty elements and styling properties. Then using this model, our approach predicts the elements and styling properties most likely to cause the observed failure for the page under test and reports these to the developer. In evaluation, our approach was more accurate and faster than prior techniques for identifying faulty elements in a website.},
 duplicado = {false},
 inserir = {false},
 title = {Using Visual Symptoms for Debugging Presentation Failures in Web Applications},
 year = {2016}
}

@article{335,
 abstract = {Abstract:
Web applications can be easily made available to an international audience by leveraging frameworks and tools for automatic translation and localization. However, these automated changes can distort the appearance of web applications since it is challenging for developers to design their websites to accommodate the expansion and contraction of text after it is translated to another language. Existing web testing techniques do not support developers in checking for these types of problems and manually checking every page in every language can be a labor intensive and error prone task. To address this problem, we introduce an automated technique for detecting when a web page's appearance has been distorted due to internationalization efforts and identifying the HTML elements or text responsible for the observed problem. In evaluation, our approach was able to detect internationalization problems in a set of 54 web applications with high precision and recall and was able to accurately identify the underlying elements in the web pages that led to the observed problem.},
 duplicado = {false},
 inserir = {false},
 title = {Detecting and Localizing Internationalization Presentation Failures in Web Applications},
 year = {2016}
}

@article{336,
 abstract = {Abstract:
Client-side JavaScript is widely used in web applications to improve user-interactivity and minimize client-server communications. Unfortunately, JavaScript is known to be error-prone. While prior studies have demonstrated the prevalence of JavaScript faults, no attempts have been made to determine their causes and consequences. The goal of our study is to understand the root causes and impact of JavaScript faults and how the results can impact JavaScript programmers, testers and tool developers. We perform an empirical study of 502 bug reports from 19 bug repositories. The bug reports are thoroughly examined to classify and extract information about each bug' cause (the error) and consequence (the failure and impact). Our results show that the majority (68 percent) of JavaScript faults are DOM-related, meaning they are caused by faulty interactions of the JavaScript code with the Document Object Model (DOM). Further, 80 percent of the highest impact JavaScript faults are DOM-related. Finally, most JavaScript faults originate from programmer mistakes committed in the JavaScript code itself, as opposed to other web application components. These results indicate that JavaScript programmers and testers need tools that can help them reason about the DOM. Additionally, developers can use the error patterns we found to design more powerful static analysis tools for JavaScript.},
 duplicado = {false},
 inserir = {false},
 title = {A Study of Causes and Consequences of Client-Side JavaScript Bugs},
 year = {2016}
}

@article{337,
 abstract = {Abstract
Cross-browser compatibility testing aims at verifying that a web page is rendered as intended by its developers across multiple browsers and platforms. Browserbite is a tool for cross-browser testing based on comparison of screenshots with the aim of identifying differences that a user may perceive as incompatibilities. Browserbite is based on segmentation and image comparison techniques adapted from the field of computer vision. The key idea is to first extract web page regions via segmentation and then to match and compare these regions pairwise based on geometry and pixel density distribution. Additional accuracy is achieved by post-processing the output of the region comparison step via supervised machine learning techniques. In this way, compatibility checking is performed based purely on screenshots rather than relying on the Document Object Model (DOM), an alternative that often leads to missed incompatibilities. Detected incompatibilities in Browserbite are overlaid on top of screenshots in order to assist users during cross-browser testing.},
 duplicado = {false},
 inserir = {true},
 title = {Cross-Browser Testing in Browserbite},
 year = {2014}
}

@article{338,
 abstract = {Differences in the rendering of a website across different browsers can cause inconsistencies in its appearance and usability, resulting in Layout Cross Browser Issues (XBIs). Such XBIs can negatively impact the functionality of a website as well as usersimpressions of its trustworthiness and reliability. Existing techniques can only detect XBIs, and therefore require developers to manually perform the labor intensive task of repair. In this demo paper we introduce our tool, XFix, that automatically repairs layout XBIs in web applications. To the best of our knowledge, XFix is the first automated technique for generating XBI repairs.},
 duplicado = {false},
 inserir = {true},
 title = {XFix: an automated tool for the repair of layout cross browser issues},
 year = {2017}
}

@article{339,
 abstract = {A consistent cross-browser user experience is crucial for the success of a website. Layout Cross Browser Issues (XBIs) can severely undermine a websites success by causing web pages to render incorrectly in certain browsers, thereby negatively impacting users impression of the quality and services that the web page delivers. Existing Cross Browser Testing (XBT) techniques can only detect XBIs in websites. Repairing them is, hitherto, a manual task that is labor intensive and requires significant expertise. Addressing this concern, our paper proposes a technique for automatically repairing layout XBIs in websites using guided search-based techniques. Our empirical evaluation showed that our approach was able to successfully fix 86% of layout XBIs reported for 15 different web pages studied, thereby improving their cross-browser consistency.},
 duplicado = {false},
 inserir = {false},
 title = {Automated repair of layout cross browser issues using search-based techniques},
 year = {2017}
}

@article{341,
 abstract = {Cross Browser Incompatibilities (XBIs) stands for compatibility issues which can be observed while rendering the same Web application in different browsers. The increasing number of browser implementations and the continuous evolving characteristic of Web technologies, lead to differences in how browsers behave and render Web applications. Every element of a Web application should be correctly rendered and present the same behavior, despite the Web browser implementation, version, or OS which is used by users [1]. In order to overcome this issue during the Web Engineering process, developers must detect and fix XBIs before deploying Web applications, regardless of the effort and cost required to conduct these inspections. This research has the goal of elaborating an approach for automatically detecting XBIs in Web applications, using machine learning (supervised learning algorithms) and screenshot similarity to detect XBIs. Next section presents the related work of this research.},
 duplicado = {false},
 inserir = {true},
 title = {Automatic detection of cross-browser incompatibilities using machine learning and screenshot similarity: student research abstract},
 year = {2017}
}

@article{342,
 abstract = {Cross-browser compatibility testing is concerned with identifying perceptible differences in the way a Web page is rendered across different browsers or configurations thereof. Existing automated cross-browser compatibility testing methods are generally based on document object model (DOM) analysis, or in some cases, a combination of DOM analysis with screenshot capture and image processing. DOM analysis, however, may miss incompatibilities that arise not during DOM construction but rather during rendering. Conversely, DOM analysis produces false alarms because different DOMs may lead to identical or sufficiently similar renderings. This paper presents a novel method for cross-browser testing based purely on image processing. The method relies on image segmentation to extract regions from a Web page and computer vision techniques to extract a set of characteristic features from each region. Regions extracted from a screenshot taken on a baseline browser are compared against regions extracted from the browser under test based on characteristic features. A machine learning classifier is used to determine if differences between two matched regions should be classified as an incompatibility. An evaluation involving 140 pages shows that the proposed method achieves an F-score exceeding 90%, outperforming a state-of-the-art cross-browser testing tool based on DOM analysis.},
 duplicado = {false},
 inserir = {true},
 title = {Browserbite: cross-browser testing via image processing},
 year = {2016}
}

@article{345,
 abstract = {Abstract
Testing applications with a graphical user interface (GUI) is an important, though challenging and time consuming task. The state of the art in the industry are still capture and replay tools, which may simplify the recording and execution of input sequences, but do not support the tester in finding fault-sensitive test cases and leads to a huge overhead on maintenance of the test cases when the GUI changes. In earlier works the authors presented the TESTAR tool, an automated approach to testing applications at the GUI level whose objective is to solve part of the maintenance problem by automatically generating test cases based on a structure that is automatically derived from the GUI. In this paper they report on their experiences obtained when transferring TESTAR in three different industrial contexts with decreasing involvement of the TESTAR developers and increasing participation of the companies when deploying and using TESTAR during testing. The studies were successful in that they reached practice impact, research impact and give insight into ways to do innovation transfer and defines a possible strategy for taking automated testing tools into the market.},
 duplicado = {false},
 inserir = {true},
 title = {TESTAR: Tool Support for Test Automation at the User Interface Level},
 year = {2015}
}

@article{346,
 abstract = {Abstract:
Due to the exponential increase in the number ofmobile devices being used to access the World Wide Web, it iscrucial that Web sites are functional and user-friendly across awide range of Web-enabled devices. This necessity has resulted in the introduction of responsive Web design (RWD), which usescomplex cascading style sheets (CSS) to fluidly modify a Web site's appearance depending on the viewport width of the device in use. Although existing tools may support the testing of responsive Web sites, they are time consuming and error-prone to use because theyrequire manual screenshot inspection at specified viewport widths. Addressing these concerns, this paper presents a method thatcan automatically detect potential layout faults in responsively designed Web sites. To experimentally evaluate this approach, weimplemented it as a tool, called ReDeCheck, and applied itto 5 real-world web sites that vary in both their approach toresponsive design and their complexity. The experiments revealthat ReDeCheck finds 91% of the inserted layout faults.},
 duplicado = {false},
 inserir = {false},
 title = {Automatic Detection of Potential Layout Faults Following Changes to Responsive Web Pages (N)},
 year = {2015}
}

@article{347,
 abstract = {Abstract:
To automatically test web applications, crawling-based techniques are usually adopted to mine the behavior models, explore the state spaces or detect the violated invariants of the applications. However, their broad use is limited by the required manual configurations for input value selection, GUI state comparison and clickable detection. In existing crawlers, the configurations are usually string-matching based rules looking for tags or attributes of DOM elements, and often application-specific. Moreover, in input topic identification, it can be difficult to determine which rule suggests a better match when several rules match an input field to more than one topic. This paper presents a natural-language approach based on semantic similarity to address the above issues. The proposed approach represents DOM elements as vectors in a vector space formed by the words used in the elements. The topics of encountered input fields during crawling can then be inferred by their similarities with ones in a labeled corpus. Semantic similarity can also be applied to suggest if a GUI state is newly discovered and a DOM element is clickable under an unsupervised learning paradigm. We evaluated the proposed approach in input topic identification with 100 real-world forms and GUI state comparison with real data from industry. Our evaluation shows that the proposed approach has comparable or better performance to the conventional techniques. Experiments in input topic identification also show that the accuracy of the rule-based approach can be improved by up to 22% when integrated with our approach.},
 duplicado = {false},
 inserir = {false},
 title = {Using Semantic Similarity in Crawling-Based Web Application Testing},
 year = {2017}
}

@article{348,
 abstract = {Abstract
Testing is an important part of every software development process on which companies devote considerable time and effort. The burgeoning web applications and their proliferating economic significance in the society made the area of web application testing an area of acute importance. The web applications generally tend to take faster and quicker release cycles making their testing very challenging. The main issues in testing are cost efficiency and bug detection efficiency. Coverage-based testing is the process of ensuring exercise of specific program elements. Coverage measurement helps determine the thoroughness of testing achieved. An avalanche of tools, techniques, frameworks came into existence to ascertain the quality of web applications. A comparative study of some of the prominent tools, techniques and models for web application testing is presented. This work highlights the current research directions of some of the web application testing techniques.},
 duplicado = {false},
 inserir = {false},
 title = {A Review on Web Application Testing and its Current Research Directions},
 year = {2017}
}

@article{349,
 abstract = {To automatically test web applications, crawling-based techniques are usually adopted to mine the behavior models, explore the state spaces or detect the violated invariants of the applications. However, in existing crawlers, rules for identifying the topics of input text fields, such as login ids, passwords, emails, dates and phone numbers, have to be manually configured. Moreover, the rules for one application are very often not suitable for another. In addition, when several rules conflict and match an input text field to more than one topics, it can be difficult to determine which rule suggests a better match. This paper presents a natural-language approach to automatically identify the topics of encountered input fields during crawling by semantically comparing their similarities with the input fields in labeled corpus. In our evaluation with 100 real-world forms, the proposed approach demonstrated comparable performance to the rule-based one. Our experiments also show that the accuracy of the rule-based approach can be improved by up to 19% when integrated with our approach.},
 duplicado = {false},
 inserir = {false},
 title = {Using Semantic Similarity for Input Topic Identification in Crawling-based Web Application Testing},
 year = {1608}
}

@article{351,
 abstract = {Abstract. Bigger and more complex software systems demand quality
practices that are seldom carried out in real industry. A common
practice is to provide a post-release maintenance service of products to
correct defects reported by the end user. In previous work we presented
TESTAR, a technology-agnostic tool for automated testing of applications
from their GUI. Here we introduce state-transition graph models
derived from TESTAR test results as a tool for visualisation of what has
been tested, to which extent and which software defects were found. We
discuss how such models enable to perform quality assessment of software
products by inspecting and debugging the system behaviour from
the GUI perspective. This constitutes a step forward in aid of software
developers and testers, since the User Interface is commonly the means
end-users encounter potential software defects.},
 duplicado = {false},
 inserir = {false},
 title = {Visualization of automated test results obtained by the TESTAR tool},
 year = {2016}
}

@article{352,
 abstract = {The security implications of social bots are evident in consideration of the fact that data sharing and propagation functionality are well integrated with social media sites. Existing social bots primarily use Really Simple Syndication and OSN (online social network) application program interface to communicate with OSN servers. Researchers have profiled their behaviors well and have proposed various mechanisms to defend against them. We predict that a web test automation rootkit (WTAR) is a prospective approach for designing malicious social bots. In this paper, we first present the principles of designing WTAR-based social bots. Second, we implement three WTAR-based bot prototypes on Facebook, Twitter, and Weibo. Third, we validate this new threat by analyzing behaviors of the prototypes in a lab environment and on the Internet, and analyzing reports from widely-used antivirus software. Our analyses show that WTAR-based social bots have the following features: (i) they do not connect to OSN directly, and therefore produce few network flows; (ii) they can log in to OSNs easily and perform a variety of social activities; (iii) they can mimic the behaviors of a human user on an OSN. Finally, we propose several possible mechanisms in order to defend against WTAR-based social bots.},
 duplicado = {false},
 inserir = {false},
 title = {Understanding a prospective approach to designing malicious social bots},
 year = {2016}
}

@article{354,
 abstract = {Abstract

Research in black-box testing has produced impressive results in the past 40 years, addressing many aspects of the problem that span from integration with the development process, to test case generation and execution. In the past few years, the research in this area has focused mostly on the automation of black-box approaches to improve applicability and scalability. This chapter surveys the recent advances in automatic black-box testing, covering contributions from 2010 to 2014, presenting the main research results and discussing the research trends.},
 duplicado = {false},
 inserir = {false},
 title = {Chapter Four - Recent Advances in Automatic Black-Box Testing},
 year = {2015}
}

@article{355,
 abstract = {Abstract
Web      technologies   of   inconsistency      and      Web
standards  it    to    be    consistent
with    different  web  technology
evolution,    Web  application    developers    has    been  broadly  rang
of  face  on  the  different  certain
problem  .  Web    applications
become  difficult  part  for  them  to  keep  track  problem    of    their
web  correctly  rendered across  broad  range  of  browsers  and
platforms.      It   kind   of   cross-browser   inconsistency   (XBI),
developers      are  keep  checking
that  document  and  platform
produced  by  the  web  application  is  appropriate  pass  the  across
every useful browser-platform so
many combinations. It take the
requires  more  than  execution  time  and      error  results  in  web
application.   This   web      cross-browser      inconsistency   existing
testing    tools    for  speed    up  the    process  of    automating  and  the
rendering  of  a  document  in    cross    browsers    and  different
platforms,    and    using    either
image  analysis,  and    Document
Object    Model    (DOM)    analysis    to  the  feature  of  cross-browser
inconsistency. This dissertation are comparisons of  the  problem
of  cross-browser    inconsistency    testing    with  the  modern    web
applications  and their  functionality check  the accuracy  of web
applications  behavior    with    different  different    web  browsers
and    their  present    a    solution.  In  the  reasons  of  cross-browser
inconsistency issues and their so many solutions to them are been
presented     feature     different     cross-browser     inconsistency.
Proposed  solution  has  been  used  the  concept  of  Automation
Testing  using  Behavioral  methodol
ogy  of  the  web  application  on
different browsers such as Firefox, Internet Explorer and  Google
Chrome.  For  the  test  bed  selenium  and  TestNG  variation  has
been  integrated  onto  the  eclipse  java  environment.  Obtained
results  has  proven  that  the  pr
oposed  XBI  validation  method
outperform  and  provides  the  cost  effective  solution  instead  of
paying  high  cost  to  other  testing  service  provider  lie  Browser-
stack},
 duplicado = {false},
 inserir = {true},
 title = {Analysis and Identification of Cross Browser Inconsistency Issues in Web Application using  Automation Testing},
 year = {2017}
}

@article{357,
 abstract = {Due to the spread of the internet and the ever increasing number of web applications, the issue of compatibility across browsers has become very important. This compatibility issue is also referred as Cross Browser Inconsistency (XBI) wherein same website looks or behaves differently in different web browsers. In this paper our aim is to address this issue of compatibility and propose an automated approach of detecting XBIs. Cross Browser Inconsistencies can either be in the content, structure or behavior of the webpage. In order to get a grasp of the above mentioned types of inconsistencies, we surveyed some random websites and analyzed them in different browsers. We also studied the basic working of browser, in order to establish its connection with the occurrences of XBIs. Each browser has its own rendering mechanisms, which sometimes differs from standards. Hence, the execution of these websites is different in different browsers. Finally we have proposed an automated approach for XBI detection.},
 duplicado = {false},
 inserir = {true},
 title = {An Automated Approach for Cross-Browser Inconsistency (XBI) Detection},
 year = {2016}
}

@article{359,
 abstract = {Abstract
Event-driven applications, such as, web applications and Android mobile ap-
plications, may be tested by selecting an interesting input (i.e. a sequence of
events), and deciding if a failure occurs when the selected input is applied to
the event-driven application under test. Automated testing promises to re-
duce the workload for developers by automatically selecting interesting inputs
and detect failures. However, it is non-trivial to conduct automated testing
of event-driven applications because of, for example, infinite input spaces and
the absence of specifications of correct application behavior.
In this PhD dissertation, we identify a number of specific challenges when
conducting automated testing of event-driven applications, and we present
novel techniques for solving these challenges.
First, we present an algorithm for stateless model-checking of event-driven
applications with partial-order reduction, and we show how this algorithm
may be used to systematically test web applications for timing related fail-
ures.  Next, we present an algorithm for generating inputs to event-driven
applications in a targeted manner, combining existing techniques using UI
models and concolic testing in a novel way. Finally, we show how server inter-
face descriptions can be used to simplify the process of automated testing of
web applications that depend on client-server communication, and we present
a learning algorithm for inferring such server interface descriptions from con-
crete observations.
We implement tools for web applications and Android mobile applications
using the above algorithms and techniques, and we experimentally evaluate
the effectiveness of the proposed solutions on real-world applications. Based
on our experiments, we conclude that our proposed solutions are useful when
automatically testing event-driven applications, and that our proposed solu-
tions pushes the state-of-the-art within this area.},
 duplicado = {false},
 inserir = {false},
 title = {Automated Testing of Event-Driven Applications},
 year = {2015}
}

@article{360,
 abstract = {Information and technology has taken new changes in the field of communication. This new world is the world of digital with the large amount of data is available on the internet. Lot of peoples is accessing the social sites like facebook, government sites also having the data in huge variety. Banks now maintain all their data related to customer and employees on internet. We required finding new ways and technologies through which we can search the closer observations on this huge data and generate knowledge, i.e. why mining technologies comes in  to existence. Mining techniques  basically
used
some
automated
tools
to
achieve
business
intelligence.
Using
these
tools,
we
will
able
In
this
paper,
we
will
talk
about
mining
techniques,
their
implementations
and
we
will
also
talk
about
how
these
mining
techniques
will
be
useful
to
achieve
Business
Intelligence.},
 duplicado = {false},
 inserir = {false},
 title = {A SURVEY OF BUSINESS INTILLIGENCE USING DATA MINING, WEB MINING AND TEXT MINING},
 year = {2015}
}

@article{361,
 abstract = {QVMMA: A Short Term and Long Term Layer 3 DDoS Detector and Mitigator
{tag}                                                                           {/tag}
IJCA Proceedings on International Conference
on Communication, Computing and Virtualization 2016 by IJCA Journal
ICCCV 2016 - Number 1
Year of Publication: 2016
Authors:
Sonia Laskar
Dhirendra Mishra
{bibtex}icccv20163.bib{/bibtex}
Abstract
Distributed Denial of Service (DDoS) attacks continue to harm servers using intense wars
against popular ecommerce and content websites.  The short term and long term types of
popular DDoS attacks can be detected, prevented and mitigated using the proposed novel
Qualified Vector Match and Merge Algorithm (QVMMA) in real time.  14 feature components are
used to generate an attack signature in real time and stored in dynamically updated DDoS
Captured Attack Pattern (DCAP)30database.  It is effective in detecting new and old attacks.
Persistent DDoS attacks cause financial damage or reputation loss by loss of the
company&apos;s valuable clients.  The server&apos;s availability is heavily compromised.
1 / 5
QVMMA: A Short Term and Long Term Layer 3 DDoS Detector and Mitigator
Popular websites Github and BBC UK faced DDoS attacks in 2015.  Long term DDoS attack
directed on Github continued for over 118 hours34,35.  Short term DDoS attack experienced by
BBC36 website caused its patchy response.  The main crux of the problem is the absence of a
way to differentiate between attack records and legitimate records while the attack is occurring
in real time.  Several methods1-31,37-42,43 are listed in brief in the paper.  Post mortem
solutions are not applicable in real time.  Available real time solutions are slow.  QVMMA is an
ideal faster real time solution to prevent DDoS attacks using Statistical Feature Vector
Generation.  Matlab is used for DDoS real time simulation where the topologies (bus, star,
abilene network) are created using OMNET++33.  QVMMA generates and uses Statistical
Feature Vector for Attack Signature Generation, Matching and Identification only for qualifier
satisfied records.  The web server&apos;s log files used as input to QVMMA are according to
W3C log format standard34.  Experimentation is completed with exhaustive 336 cases.  Four
networks are tested with 5, 8, 10, 13 nodes.  Performance evaluation of QVMMA concludes
EER is 11. 8% when threshold is 1. 6.  Using model of FAR and FAR, the trendline provides
threshold at 1 with EER at 10%.  Abilene network achieves best result.  As the number of
attackers, nodes and intermediate routers increase, detection time increases.  If threshold is
increased, the accuracy reduces.  If the number of nodes increases, accuracy increases.  Thus
it is concluded that QVMMA can be used for effective layer 3 DDoS Prevention and Mitigation in
real time based on results generated in Matlab simulation.  Extended results are provided.  A
model is provided in this paper to predict the detection time for any number of attackers.  Other
models are provided based on data collected through experimentation to formulate a relation
between detection time, accuracy, Actual Attack Traffic Passed Rate (A_ATPR) with respect to
the number of attackers.  The corresponding correlation coefficient and regression coefficient
are calculated to identify and conclude the strong relationships.  This paper focuses on results
and discussion on studying the effects and trend observed based on increasing the number of
attackers during a DDoS attack.   Thus QVMMA is fast enough to be used in real time to detect
and mitigate short term or long term layer 3 Denial of Service(DoS) and more complex DDoS
attacks. },
 duplicado = {false},
 inserir = {false},
 title = {QVMMA: A Short Term and Long Term Layer 3 DDoS Detector and Mitigator},
 year = {2016}
}

@article{362,
 abstract = {JavaScript has become one of the most prevalent programming languages. Unfortunately, some of the unique
properties that contribute to this popularity also make JavaScript programs prone to errors and difficult for
program analyses to reason about. These properties include the highly dynamic nature of the language, a set
of unusual language features, a lack of encapsulation mechanisms, and the no crash philosophy. This paper
surveys dynamic program analysis and test generation techniques for JavaScript targeted at improving the
correctness, reliability, performance, security, and privacy of JavaScript-based software.},
 duplicado = {false},
 inserir = {false},
 title = {00 A Survey of Dynamic Analysis and Test Generation for JavaScript},
 year = {2017}
}

@article{363,
 abstract = { Abstract:
Metamorphic testing (MT) can enhance security testing by providing an alternative to using a test oracle, which is often unavailable or impractical. The authors report how MT detected previously unknown bugs in real-world critical applications such as code obfuscators, giving evidence that software testing requires diverse perspectives to achieve greater cybersecurity.},
 duplicado = {false},
 inserir = {false},
 title = {Metamorphic Testing for Cybersecurity},
 year = {2016}
}

@article{366,
 abstract = {Abstract [en]

Context. Software testing is becoming more and more important in software development life-cycle especially for web testing. Selenium is one of the most widely used property-based Graph-User-Interface(GUI) web testing tools. Nevertheless, it also has some limitations. For instance, Selenium cannot test the web components in some specific plugins or HTML5 videos frame. But it is important for testers to verify the functionality of plugins or videos on the websites. Recently, the theory of the image recognition-based GUI testing is introduced which can locate and interact with the components to be tested on the websites by image recognition. There are only a few papers do research on comparing property-based GUI web testing and image recognition-based GUI testing. Hence, we formulated our research objectives based on this main gap.

Objectives. We want to compare these two different techniques with EyeSel which is the tool represents the image recognition-based GUI testing and Selenium which is the tool represents the property-based GUI testing. We will evaluate and compare the strengths and drawbacks of these two tools by formulating specific JUnit testing scripts. Besides, we will analyze the comparative result and then evaluate if EyeSel can solve some of the limitations associated with Selenium. Therefore, we can conclude the benefits and drawbacks of property-based GUI web testing and image recognition-based GUI testing.

Methods. We conduct an experiment to develop test cases based on websites components both by Selenium and EyeSel. The experiment is conducted in an educational environment and we select 50 diverse websites as the subjects of the experiment. The test scripts are written in JAVA and ran by Eclipse.  The experiment data is collected for comparing and analyzing these two tools.

Results. We use quantitative analysis and qualitative analysis to analyze our results. First of all, we use quantitative analysis to evaluate the effectiveness and efficiency of two GUI web testing tools. The effectiveness is measured by the number of components that can be tested by these two tools while the efficiency is measured by the measurements of test cases development time and execution time. The results are as follows (1) EyeSel can test more number of components in web testing than Selenium (2) Testers need more time to develop test cases by Selenium than by EyeSel (3) Selenium executes the test cases faster than EyeSel. (4) Results (1) indicates the effectiveness of EyeSel is better than Selenium while Results (2)(3) indicate the efficiency of EyeSel is better than Selenium. Secondly, we use qualitative analysis to evaluate four quality characteristics (learnability, robustness, portability, functionality) of two GUI web testing tools. The results show that portability and functionality of Selenium are better than EyeSel while the learnability of EyeSel is better than Selenium. And both of them have good robustness in web testing.

Conclusions. After analyzing the results of comparison between Selenium and EyeSel, we conclude that (1) Image recognition-based GUI testing is more effectiveness than property-based GUI web testing (2) Image recognition-based GUI testing is more efficiency than property-based GUI web testing (3) The portability and functionality of property-based GUI web testing is better than Image recognition-based GUI testing (4) The learnability of image recognition-based GUI testing is better than property-based GUI web testing. (5) Both of them are good at different aspects of robustness},
 duplicado = {false},
 inserir = {false},
 title = {Comparison of Different Techniques of Web GUI-based Testing with the Representative Tools Selenium and EyeSel},
 year = {2017}
}

@article{367,
 abstract = {Abstract

Test oracles differentiate between the correct and incorrect system behavior. Hence, test oracle automation is essential to achieve overall test automation. Otherwise, testers have to manually check the system behavior for all test cases. A common test oracle automation approach for testing systems with visual output is based on exact matching between a snapshot of the observed output and a previously taken reference image. However, images can be subject to scaling and translation variations. These variations lead to a high number of false positives, where an error is reported due to a mismatch between the compared images although an error does not exist. To address this problem, we introduce an automated test oracle, named VISOR, that employs a fast image processing pipeline. This pipeline includes a series of image filters that align the compared images and remove noise to eliminate differences caused by scaling and translation. We evaluated our approach in the context of an industrial case study for regression testing of Digital TVs. Results show that VISOR can avoid 90% of false positive cases after training the system for 4 h. Following this one-time training, VISOR can compare thousands of image pairs within seconds on a laptop computer.},
 duplicado = {false},
 inserir = {false},
 title = {VISOR: A fast image processing pipeline with scaling and translation invariance for test oracle automation of visual output systems},
 year = {2018}
}

@article{370,
 abstract = {Abstract

Various tools are available to help developers detect cross-browser incompatibilities (XBIs) by testing the documents generated by their code. We propose an approach that enables XBIs to be detected earlier in the development cycle by providing support in the IDE as the code is being written. This has the additional advantage of making it clear to the developers where the sources of the problems are and how to fix them. We present wIDE which is an extension to an IDE designed specifically to support web developers. wIDE uses a compatibility knowledge base to scan the source code for XBIs. The knowledge base is extracted automatically from online resources and periodically updated to ensure that the compatibility information is always up-to-date. In addition, developers can query documentation from within the IDE to access descriptions and usage examples of code statements. We report on a qualitative user study where developers provided positive feedback about the approach, but raised some issues to address in future work.},
 duplicado = {false},
 inserir = {true},
 title = {Improved Developer Support for the Detection of Cross-Browser Incompatibilities},
 year = {2017}
}

@article{372,
 abstract = {Due to the increasing popularity of web applications, and the number of browsers and platforms on which such applications can be executed, cross-browser incompatibilities (XBIs) are becoming a serious concern for organizations that develop web-based software. Most of the techniques for XBI detection developed to date are either manual, and thus costly and error-prone, or partial and imprecise, and thus prone to generating both false positives and false negatives. To address these limitations of existing techniques, we developed X-PERT, a new automated, precise, and comprehensive approach for XBI detection. X-PERT combines several new and existing differencing techniques and is based on our findings from an extensive study of XBIs in real-world web applications. The key strength of our approach is that it handles each aspects of a web application using the differencing technique that is best suited to accurately detect XBIs related to that aspect. Our empirical evaluation shows that X-PERT is effective in detecting real-world XBIs, improves on the state of the art, and can provide useful support to developers for the diagnosis and (eventually) elimination of XBIs.},
 duplicado = {false},
 inserir = {true},
 title = {X-PERT: accurate identification of cross-browser issues in web applications},
 year = {2013}
}

@article{373,
 abstract = {Research on software evolution is very active, but evolutionary principles, models and theories that properly explain why and how software systems evolve over time are still lacking. Similarly, more empirical research is needed to understand how different software projects co-exist and co-evolve, and how contributors collaborate within their encompassing software ecosystem.

In this chapter, we explore the differences and analogies between natural ecosystems and biological evolution on the one hand, and software ecosystems and software evolution on the other hand. The aim is to learn from research in ecology to advance the understanding of evolving software ecosystems. Ultimately, we wish to use such knowledge to derive diagnostic tools aiming to predict survival of software projects within their ecosystem, to analyse and optimise the fitness of software projects in their environment, and to help software project communities in managing their projects better.},
 duplicado = {false},
 inserir = {false},
 title = {Studying Evolving Software Ecosystems based on Ecological Models},
 year = {2014}
}

@article{374,
 abstract = {One of the consequences of the continuous and rapid evolution of web technologies is the amount of inconsistencies between web browsers implementations. Such inconsistencies can result in cross-browser incompatibilities (XBIs)-situations in which the same web application can behave differently when run on different browsers. In some cases, XBIs consist of tolerable cosmetic differences. In other cases, however, they may completely prevent users from accessing part of a web application's functionality. Despite the prevalence of XBIs, there are hardly any tools that can help web developers detect and correct such issues. In fact, most existing approaches against XBIs involve a considerable amount of manual effort and are consequently extremely time consuming and error prone. In recent work, we have presented two complementary approaches, WEBDIFF and CROSST, for automatically detecting and reporting XBIs. In this paper, we present CROSSCHECK, a more powerful and comprehensive technique and tool for XBI detection that combines and adapts these two approaches in a way that leverages their respective strengths. The paper also presents an empirical evaluation of CROSSCHECK on a set of real-world web applications. The results of our experiments show that CROSSCHECK is both effective and efficient in detecting XBIs, and that it can outperform existing techniques.},
 duplicado = {false},
 inserir = {true},
 title = {CrossCheck: Combining Crawling and Differencing to Better Detect Cross-browser Incompatibilities in Web Applications},
 year = {2012}
}

@article{375,
 abstract = {Abstract:
CSS is a widely used language for describing the presentation semantics of HTML elements on the web. The language has a number of characteristics, such as inheritance and cascading order, which makes maintaining CSS code a challenging task for web developers. As a result, it is common for unused rules to be accumulated over time. Despite these challenges, CSS analysis has not received much attention from the research community. We propose an automated technique to support styling code maintenance, which (1) analyzes the runtime relationship between the CSS rules and DOM elements of a given web application (2) detects unmatched and ineffective selectors, overridden declaration properties, and undefined class values. Our technique, implemented in an open source tool called Cilla, has a high precision and recall rate. The results of our case study, conducted on fifteen open source and industrial web-based systems, show an average of 60% unused CSS selectors in deployed applications, which points to the ubiquity of the problem.},
 duplicado = {false},
 inserir = {false},
 title = {Automated analysis of CSS rules to support style maintenance},
 year = {2012}
}

@article{376,
 abstract = {Context

The web has had a significant impact on all aspects of our society. As our society relies more and more on the web, the dependability of web applications has become increasingly important. To make these applications more dependable, for the past decade researchers have proposed various techniques for testing web-based software applications. Our literature search for related studies retrieved 193 papers in the area of web application testing, which have appeared between 2000 and 2013.

Objective

As this research area matures and the number of related papers increases, it is important to systematically identify, analyze, and classify the publications and provide an overview of the trends and empirical evidence in this specialized field.

Methods

We systematically review the body of knowledge related to functional testing of web application through a systematic literature review (SLR) study. This SLR is a follow-up and complimentary study to a recent systematic mapping (SM) study that we conducted in this area. As part of this study, we pose three sets of research questions, define selection and exclusion criteria, and synthesize the empirical evidence in this area.

Results

Our pool of studies includes a set of 95 papers (from the 193 retrieved papers) published in the area of web application testing between 2000 and 2013. The data extracted during our SLR study is available through a publicly-accessible online repository. Among our results are the followings: (1) the list of test tools in this area and their capabilities, (2) the types of test models and fault models proposed in this domain, (3) the way the empirical studies in this area have been designed and reported, and (4) the state of empirical evidence and industrial relevance.

Conclusion

We discuss the emerging trends in web application testing, and discuss the implications for researchers and practitioners in this area. The results of our SLR can help researchers to obtain an overview of existing web application testing approaches, fault models, tools, metrics and empirical evidence, and subsequently identify areas in the field that require more attention from the research community.},
 duplicado = {false},
 inserir = {false},
 title = {Web application testing: A systematic literature review},
 year = {2014}
}

@article{377,
 abstract = {Quality assurance of Web applications is a challenge, due to the large number and variance of involved components. In particular, rich Web 2.0 applications based on JavaScript pose new challenges for testing, as a simple crawling through links covers only a small part of the functionality. The WEBMATE approach automatically explores and navigates through arbitrary Web 2.0 applications. WEBMATE addresses challenges such as interactive elements, state abstraction, and non-determinism in large applications; we demonstrate its usage for regular application testing as well as for cross-browser testing.},
 duplicado = {false},
 inserir = {true},
 title = {WebMate: a tool for testing web 2.0 applications},
 year = {2012}
}

@article{378,
 abstract = {Abstract. Web applications are everywhere well tested web applications
however are in short supply. The mixture of JavaScript, HTML and
CSS in a variety of different browsers makes it virtually impossible to apply
static analysis techniques. In this setting, systematic testing becomes
a real challenge. We present a technique to automatically generate tests
for Web 2.0 applications. Our approach systematically explores and tests
all distinct functions of a web application. Our prototype implementation
WEBMATE handles interfaces as complex as Facebook and is able to
cover up to 7 times as much code as existing tools. The only requirements
to use WEBMATE are the address of the application and, if necessary,
user name and password},
 duplicado = {false},
 inserir = {true},
 title = {WebMate: Generating Test Cases for Web 2.0},
 year = {2013}
}

@article{379,
 abstract = {Despite the seemingly obvious advantage of test automation, significant skepticism exists in the industry regarding its cost-benefit tradeoffs. Test scripts for web applications are fragile: even small changes in the page layout can break a number of tests, requiring the expense of re-automating them. Moreover, a test script created for one browser cannot be relied upon to run on a different web browser: it requires duplicate effort to create and maintain versions of tests for a variety of browsers. Because of these hidden costs, organizations often fall back to manual testing.

We present a fresh solution to the problem of test-script fragility. Often, the root cause of test-script fragility is that, to identify UI elements on a page, tools typically record some metadata that depends on the internal representation of the page in a browser. Our technique eliminates metadata almost entirely. Instead, it identifies UI elements relative to other prominent elements on the page. The core of our technique automatically identifies a series of contextual clues that unambiguously identify a UI element, without recording anything about the internal representation.

Empirical evidence shows that our technique is highly accurate in computing contextual clues, and outperforms existing techniques in its resilience to UI changes as well as browser changes.},
 duplicado = {false},
 inserir = {true},
 title = {Robust test automation using contextual clues},
 year = {2014}
}

@article{381,
 abstract = {Abstract:
Behavior models of applications are widely used for diagnosing security incidents in complex web-based systems. However, Ajax techniques that enable better web experiences also make it fairly challenging to model Ajax application behaviors in the complex browser environment. In Ajax applications, server-side states are no longer synchronous with the views to end users at the client side. Therefore, to model the behaviors of Ajax applications, it is indispensable to incorporate client-side application states into the behavior models, as being explored by prior work. Unfortunately, how to leverage behavior models to perform security diagnosis in Ajax applications has yet been thoroughly examined. Existing models extracted from Ajax application behaviors are insufficient in a security context. In this paper, we propose a new behavior model for diagnosing attacks in Ajax applications, which abstracts both client-side state transitions as well as their communications to external servers. Our model articulates different states with the browser events or user actions that trigger state transitions. With a prototype implementation, we demonstrate that the proposed model is effective in attack diagnosis for real-world Ajax applications.},
 duplicado = {false},
 inserir = {false},
 title = {A Comprehensive Client-Side Behavior Model for Diagnosing Attacks in Ajax Applications},
 year = {2013}
}

@article{382,
 abstract = {Abstract
To decouple test code from web page details, web testers adopt the Page Object design pattern. Page objects are facade classes abstracting the internals of web pages (e.g., form fields) into high-level business functions that can be invoked by test cases (e.g., user authentication). However, writing such page objects requires substantial effort, which is paid off only later, during software evolution. In this paper we propose a clustering-based approach for the identification of meaningful abstractions that are automatically turned into Java page objects. Our clustering approach to page object identification has been integrated into our tool for automated page object generation, Apogen. Experimental results indicate that the clustering approach provides clusters of web pages close to those manually produced by a human (with, on average, only three differences per web application). 75 % of the code generated by Apogen can be used as-is by web testers, breaking down the manual effort for page object creation. Moreover, a large portion (84 %) of the page object methods created automatically to support assertion definition corresponds to useful behavioural abstractions.},
 duplicado = {false},
 inserir = {false},
 title = {Clustering-Aided Page Object Generation for Web Testing},
 year = {2016}
}

@article{383,
 abstract = {ABSTRACT
AdMotional is a research project aiming at achieving a win-win
situation for online advertisers and web users alike by optimizing
the campaign selection process and creating personalized ads.
This results in increased campaign performance for advertisers,
and in more relevant and thus less annoying ads for consumers.
We give a general overview and present the system architecture,
before describing the main components in greater detail. We also
introduce the learning and optimization component and strategies,
before concluding with a summary and brief outlook into future
developments. },
 duplicado = {false},
 inserir = {false},
 title = {AdMotional: Towards Personalized Online Ads},
 year = {2011}
}

@article{385,
 abstract = {Writing oracles is challenging. As a result, developers often create oracles that check too little, resulting in tests that are unable to detect failures, or check too much, resulting in tests that are brittle and difficult to maintain. In this paper we present a new technique for automatically analyzing test oracles. The technique is based on dynamic tainting and detects both brittle assertions that depend on values that are derived from uncontrolled inputs and unused inputs provided by the test that are not checked by an assertion. We also presented OraclePolish, an implementation of the technique that can analyze tests that are written in Java and use the JUnit testing framework. Using OraclePolish, we conducted an empirical evaluation of more than 4000 real test cases. The results of the evaluation show that OraclePolish is effective; it detected 164 tests that contain brittle assertions and 1618 tests that have unused inputs. In addition, the results also demonstrate that the costs associated with using the technique are reasonable.},
 duplicado = {false},
 inserir = {false},
 title = {Improving oracle quality by detecting brittle assertions and unused inputs in tests},
 year = {2014}
}

@article{386,
 abstract = {Client-side JavaScript is increasingly used for enhancing web application functionality, interactivity, and responsiveness. Through the execution of JavaScript code in browsers, the DOM tree representing a webpage at runtime, can be incrementally updated without requiring a URL change. This dynamically updated content is hidden from general search engines. In this paper, we present the first empirical study on measuring and characterizing the hidden-web induced as a result of clientside JavaScript execution. Our study reveals that this type of hidden-web content is prevalent in online web applications today: from the 500 websites we analyzed, 95% contain client-side hidden-web content; On those websites that contain client-side hidden-web content, (1) on average, 62% of the web states are hidden, (2) per hidden state, there is an average of 19 kilobytes of data that is hidden from which 0.6 kilobytes contain textual content, (3) the DIV element is the most common clickable element used (61%) to initiate this type of hidden-web state transition, and (4) on average 25 minutes is required to dynamically crawl 50 DOM states. Further, our study indicates that there is a correlation between DOM tree size and hidden-web content, but no correlation exists between the amount of JavaScript code and client-side hidden-web.},
 duplicado = {false},
 inserir = {false},
 title = {Hidden-Web Induced by Client-Side Scripting: An Empirical Study},
 year = {2013}
}

@article{387,
 abstract = {Abstract:
An attractive and visually appealing appearance is important for the success of a website. Presentation failures in a site's web pages can negatively impact end users' perception of the quality of the site and the services it delivers. Debugging such failures is challenging because testers must visually inspect large web pages and analyze complex interactions among the HTML elements of a page. In this paper we propose a novel automated approach for debugging web page user interfaces. Our approach uses computer vision techniques to detect failures and can then identify HTML elements that are likely to be responsible for the failure. We evaluated our approach on a set of real-world web applications and found that the approach was able to accurately and quickly identify faulty HTML elements.},
 duplicado = {false},
 inserir = {false},
 title = {Detection and Localization of HTML Presentation Failures Using Computer Vision-Based Techniques},
 year = {2015}
}

@article{388,
 abstract = {Web applications are popular among developers because of the ease of development and deployment through the ubiquitous web browsing platform. However, differences in a web application's execution across different web browsers manifest as Cross-browser Inconsistencies (XBIs), which are a serious concern for web developers. Testing for XBIs manually is a laborious and error-prone process. In this demo we present X-PERT, which is a tool to identify XBIs in web applications automatically, without requiring any effort from the developer. X-PERT implements a comprehensive technique to identify XBIs and has been found to be effective in detecting real-world XBIs in our empirical evaluation. The source code of X-PERT and XBI reports from our evaluation are available at http://gatech.github.io/xpert.},
 duplicado = {false},
 inserir = {true},
 title = {X-PERT: a web application testing tool for cross-browser inconsistency detection},
 year = {2014}
}

@article{389,
 abstract = {Abstract:
Cross-browser compatibility testing is a time consuming and monotonous task. In its most manual form, Web testers open Web pages one-by-one on multiple browser-platform combinations and visually compare the resulting page renderings. Automated cross-browser testing tools speed up this process by extracting screenshots and applying image processing techniques so as to highlight potential incompatibilities. However, these systems suffer from insufficient accuracy, primarily due to a large percentage of false positives. Improving accuracy in this context is challenging as the criteria for classifying a difference as an incompatibility are to some extent subjective. We present our experience building a cross-browser testing tool (Browser bite) based on image segmentation and differencing in conjunction with machine learning. An experimental evaluation involving a dataset of 140 pages, each rendered in 14 browser-system combinations, shows that the use of machine learning in this context leads to significant accuracy improvement, allowing us to attain an F-score of over 90%.},
 duplicado = {false},
 inserir = {true},
 title = {Browserbite: Accurate Cross-Browser Testing via Machine Learning over Image Features},
 year = {2013}
}

@article{390,
 abstract = {Abstract:
Web applications have become the most popular type of software in the past decade, attracting the attention of both the academia and the industry. In parallel with their popularity, the complexity of aesthetics and functionality of web applications have also increased significantly, creating a big challenge for maintenance and cross-browser compliance testing. Since such testing and verification activities require visual analysis, web application testing has not been sufficiently automated. In this paper, we propose a novel pairwise image comparison approach suitable for web application testing where the location of layout faults needs to be detected efficiently while insignificant variations being neglected. This technique is developed based on the characteristics of fault patterns of browser layouts. An empirical study conducted with the industry partner shows our approach is more effective and efficient than existing methods in this area.},
 duplicado = {false},
 inserir = {true},
 title = {Adaptive Random Testing for Image Comparison in Regression Web Testing},
 year = {2014}
}

@article{391,
 abstract = {ABSTRACT
A good Web site is more than just something to look at, it is functional interactive and flawless. As
technologies are becoming smart so we need to be smarter enough to utilize them. With the rapid evolution
of web technologies, the complexity of web applications has also grown up. Specially making a web
application that works well with cross browser is a great challenge. Clearly, cross-browser means
something works with all versions of all browsers to have existed since the web began. By this paper we
have pointed out some reasons why applications behave or appear differently in different browsers because
if you know the cause, you can get a solution. },
 duplicado = {false},
 inserir = {true},
 title = {Cross Browser Incompatibility: Reasons and Solutions},
 year = {2011}
}

@article{392,
 abstract = {RESUMO
A method, media, and systems for performing incremental visual comparison of web browser screens are provided. Structured representations of a current state of an event-driven application are accessed, one with respect to a plurality of client-tier environments. Where the current state is not an initial state of the event-driven application, structured representations of a prior state of the event-driven application are accessed, with respect to each client-tier environment. A set of one or more pairwise equivalences between the current state and the prior state of the event-driven application are determined with respect to each client-tier environment. A set of one or more matched portions between the first and second sets of pairwise equivalences is determined. Finally, one or more differences in a first end-user experience in the first client-tier environment are determined with respect to a second end-user experience in the second client-tier environment.},
 duplicado = {false},
 inserir = {true},
 title = {Incremental visual comparison of web browser screens},
 year = {2013}
}

@article{393,
 abstract = {The World Wide Web has led to a new kind of software, web systems, which are based on web technologies. Just like software in other domains, web systems have evolution challenges. This chapter discusses evolution of web systems on three dimensions: architecture, (conceptual) design, and technology. For each of these dimensions we introduce the state-of-the-art in the techniques and tools that are currently available. In order to place current evolution techniques into context, we also provide a survey of the different kinds of web systems as they have emerged, tracing the most important achievements of web systems evolution research from static web sites over dynamic web applications and web services to Ajax-based Rich Internet Applications.},
 duplicado = {false},
 inserir = {false},
 title = {Evolution of Web Systems},
 year = {2014}
}

@article{394,
 abstract = {Cascading Style Sheets (CSS) is the standard language for styling web documents and is extensively used in the industry. However, CSS lacks constructs that would allow code reuse (e.g., functions). Consequently, maintaining CSS code is often a cumbersome and error-prone task. Preprocessors (e.g., Less and Sass) have been introduced to fill this gap, by extending CSS with the missing constructs. Despite the clear maintainability benefits coming from the use of preprocessors, there is currently no support for migrating legacy CSS code to preprocessors. In this paper, we propose a technique for automatically detecting duplicated style declarations in CSS code that can be migrated to preprocessor functions (i.e., mixins). Our technique can parameterize differences in the style values of duplicated declarations, and ensure that the migration will not change the presentation semantics of the web documents. The evaluation has shown that our technique is able to detect 98% of the mixins that professional developers introduced in websites and Style Sheet libraries, and can safely migrate real CSS code.},
 duplicado = {false},
 inserir = {false},
 title = {Migrating cascading style sheets to preprocessors by introducing mixins},
 year = {2016}
}

@article{397,
 abstract = {Abstract:
The web has had a significant impact on our lives. A technology that was initially created for sharing documents across the network has evolved into a strong medium for developing and distributing software applications. In this paper, we first provide a concise overview of the evolution of the web itself. We then focus on some of the main industrial and research achievements in software analysis and testing techniques geared toward web apps, in the past two decades. We discuss static, dynamic, and hybrid analyses approaches, software testing and test adequacy techniques, as well as techniques that help developers write, analyze and maintain their code. Finally, we present some of the current and future challenges and research opportunities ahead in this field.},
 duplicado = {false},
 inserir = {false},
 title = {Software Analysis for the Web: Achievements and Prospects},
 year = {2016}
}

@article{398,
 abstract = {Software services companies offer software development, testing and maintenance as a service to other organizations. As a thriv- ing industry in its own right, software services offers certain unique research problems as well as different takes on research problems typically considered in software engineering research. In this paper, we highlight some of these research problems, drawing heavily upon our involvement with IBM Global Business Services organization over the past several years. We focus on four selected topics: how to organize people and the flow of work through people, how to manage knowledge at an organizational level, how to estimate and manage risk in a services engagement, and finally, testing services. These topics by no means cover all areas pertinent to soft- ware services; rather, they reflect ones in which we have personal perspectives to offer. We also share our experience in deployment of research innovations in a large service delivery organization.},
 duplicado = {false},
 inserir = {false},
 title = {Software services: a research roadmap},
 year = {2014}
}

@article{399,
 abstract = {Abstract
JavaScript is a flexible and expressive prototype-based scripting language that is used by developers to create interactive web applications. The language is interpreted, dynamic, weakly typed, and has first-class functions. It also interacts extensively with other web languages such as CSS and HTML at runtime. All these characteristics make JavaScript code particularly error-prone and challenging to analyze and test. In this chapter, we explore recent advances made in analysis and testing techniques geared toward JavaScript-based web applications. In particular, we look at recent empirical studies, testing techniques, test oracle automation approaches, test adequacy assessment methods, fault localization and repair, and Integrated Development Environment support to help programmers write better JavaScript code.},
 duplicado = {false},
 inserir = {false},
 title = {Chapter Five - Advances in Testing JavaScript-Based Web Applications},
 year = {2015}
}

@article{401,
 abstract = {Open data is an emerging paradigm to share large and diverse datasets -- primarily from governmental agencies, but also from other organizations -- with the goal to enable the exploitation of the data for societal, academic, and commercial gains. There are now already many datasets available with diverse characteristics in terms of size, encoding and structure. These datasets are often created and maintained in an ad-hoc manner. Thus, open data poses many challenges and there is a need for effective tools and techniques to manage and maintain it. In this paper we argue that software maintenance and reverse engineering have an opportunity to contribute to open data and to shape its future development. From the perspective of reverse engineering research, open data is a new artifact that serves as input for reverse engineering techniques and processes. Specific challenges of open data are document scraping, image processing, and structure/schema recognition. From the perspective of maintenance research, maintenance has to accommodate changes of open data sources by third-party providers, traceability of data transformation pipelines, and quality assurance of data and transformations. We believe that the increasing importance of open data and the research challenges that it brings with it may possibly lead to the emergence of new research streams for reverse engineering as well as for maintenance.},
 duplicado = {false},
 inserir = {false},
 title = {Open Data: Reverse Engineering and Maintenance Perspective},
 year = {1202}
}

@article{404,
 abstract = {Abstract
Modern web applications are characterized by ultra-rapid development cycles, and web testers tend to pay scant attention to the quality of their automated end-to-end test suites. Indeed, these quickly become hard to maintain, as the application under test evolves. As a result, end-to-end automated test suites are abandoned, despite their great potential for catching regressions. The use of the Page Object pattern has proven to be very effective in end-to-end web testing. Page objects are facade classes abstracting the internals of web pages into high-level business functions that can be invoked by the test cases. By decoupling test code from web page details, web test cases are more readable and maintainable. However, the manual development of such page objects requires substantial coding effort, which is paid off only later, during software evolution. In this paper, we describe a novel approach for the automatic generation of page objects for web applications. Our approach is implemented in the tool Apogen, which automatically derives a testing model by reverse engineering the target web application. It combines clustering and static analysis to identify meaningful page abstractions that are automatically turned into Java page objects for Selenium WebDriver. Our evaluation on an open-source web application shows that our approach is highly promising: Automatically generated page object methods cover most of the application functionalities and result in readable and meaningful code, which can be very useful to support the creation of more maintainable web test suites.},
 duplicado = {false},
 inserir = {false},
 title = {APOGEN: automatic page object generator for web testing},
 year = {2017}
}

@article{406,
 abstract = {RESUMO
One embodiment presents a user interface to a first user, the user interface being configured to enable the first user to provide user input that: specifies a first web application, and a behavior exploration specification, a plurality of web browsers, and a set of comparison rules for the first web application; requests a plurality of models to be constructed with respect to the web browsers for the first web application based on the behavior exploration specification; requests the models of the first web application to be compared with each other based on the set of comparison rules; and requests one or more results of the comparison performed on the models of the first web application to be presented; and in response to each user input received from the first user, performs one or more operations for the first user.},
 duplicado = {false},
 inserir = {true},
 title = {Web service for automated cross-browser compatibility checking of web applications},
 year = {2015}
}

@article{407,
 abstract = {Abstract Web technologies of inconsistency and Web
standards it to be consistent with different web technology
evolution, Web application developers has been broadly rang
of face on the different certain problem . Web applications
become difficult part for them to keep track problem of their
web correctly rendered across broad range of browsers and
platforms. It kind of cross-browser inconsistency (XBI),
developers are keep checking that document and platform
produced by the web application is appropriate pass the across
every useful browser-platform so many combinations. It take the
requires more than execution time and error results in web
application. This web cross-browser inconsistency existing
testing tools for speed up the process of automating and the
rendering of a document in cross browsers and different
platforms, and using either image analysis, and Document
Object Model (DOM) analysis to the feature of cross-browser
inconsistency. This dissertation are comparisons of the problem
of cross-browser inconsistency testing with the modern web
applications and their functionality check the accuracy of web
applications behavior with different different web browsers
and their present a solution. In the reasons of cross-browser
inconsistency issues and their so many solutions to them are been
presented feature different cross-browser inconsistency.
Proposed solution has been used the concept of Automation
Testing using Behavioral methodology of the web application on
different browsers such as Firefox, Internet Explorer and Google
Chrome. For the test bed selenium and TestNG variation has
been integrated onto the eclipse java environment. Obtained
results has proven that the proposed XBI validation method
outperform and provides the cost effective solution instead of
paying high cost to other testing service provider lie Browserstack.},
 duplicado = {false},
 inserir = {true},
 title = {Analysis and Identification of Cross Browser Inconsistency Issues in Web Application using Automation Testing},
 year = {2016}
}

@article{408,
 abstract = {RESUMO
In one embodiment, a method includes dynamically crawling an event-driven application in a plurality of client-tier environments; for each of the client-tier environments, generating a behavioral model of the event-driven application based on the dynamic crawling; for each of the behavioral models, determining one or more pairwise equivalences with respect to one or more other ones of the behavioral models; and, for each of the client-tier environments, determining one or more differences in an end-user experience of the event-driven application in the client-tier environment with respect to one or more other end-user experiences of the event-driven application in one or more other ones of the client-tier environments.},
 duplicado = {false},
 inserir = {true},
 title = {Determining differences in an event-driven application accessed in different client-tier environments},
 year = {2015}
}

@article{410,
 abstract = {Abstract:
With the advent of Web 2.0 application, and the increasing number of browsers and platforms on which the applications can be executed, cross-browser incompatibilities (XBIs) are becoming a serious problem for organizations to develop web-based software. Although some techniques and tools have been proposed to identify XBIs, they cannot assure the same execution when the application runs across different browsers as only explicit user activity is considered, and thus prone to generating both false positives and false negatives. To address this limitation, this paper describes X-Check, a platform that enables cross-browser testing as a service by leveraging record/replay technique. Comparing to existing techniques and tools, X-Check supports to detect cross-browser issues with high accuracy. It also provides useful support to developers for diagnosis and (eventually) elimination of XBIs. Our empirical evaluation shows that X-Check is effective, improves the state of the art.},
 duplicado = {false},
 inserir = {true},
 title = {X-Check: A Novel Cross-Browser Testing Service Based on Record/Replay},
 year = {2016}
}

@article{411,
 abstract = {RESUMO
An electronic device includes a memory and a processor coupled to the memory. The memory contains a master state graph. The master state graph includes information regarding the operation of interactive client-server application. The processor is configured to send a first job to a first worker node, send a second job to a second worker node, receive results of crawling the interactive client-server application, and integrate results of crawling the interactive client-server application into the master state graph. The first job includes crawling instructions for crawling a first portion of an interactive client-server application. The second job includes crawling instructions for crawling a second portion of the interactive client-server application. The first worker node and second worker node crawl the interactive client-server application in parallel.},
 duplicado = {false},
 inserir = {false},
 title = {Technique for coordinating the distributed, parallel crawling of interactive client-server applications},
 year = {2014}
}

@article{414,
 abstract = {Abstract
Thanks to the expensive research the number of nanoparticles and nanomaterials
and their variations have signifantly increased during the last years.
Even though great expectations are placed on nanoparticles there are still a
lot of unanswered questions regarding the bene
ts and risks towards human
and environment.
In September 2014, EMPA developed in cooperation with the companies
TNO and NCB the LICARA guidelines to be used to conduct a Life Cycle,
Risk and Bene
t Assessment of nanoparticles. The LICARA nanoSCAN Excel
version is connected to these guidelines and is acting as a semi-quantitative
decision support tool while answering questions.
Within the scope of this bachelor thesis a web based version of the Excel
tool has been developed to increase the usability of the software. The existing
Excel tool has been analysed using the Design principles of Donald Norman.
Based on these results the requirements were made. The developed tool has
been tested by users and developers of the Excel tool and their feedback has
been considered during the re-design.},
 duplicado = {false},
 inserir = {false},
 title = {Webbasierte Re-Implementierung von LICARA nanoSCAN},
 year = {2015}
}

@article{416,
 abstract = {Cascading Style Sheets is the standard styling language, and is extensively used for defining the presentation of web, mobile and desktop applications. Despite its popularity, the language's design shortcomings have made CSS development and maintenance challenging. This thesis aims at developing techniques for safely transforming CSS code (through refactoring, or migration to a preprocessor language), with the goal of optimization and improved maintainability.},
 duplicado = {false},
 inserir = {false},
 title = {Refactoring and migration of cascading style sheets: towards optimization and improved maintainability},
 year = {2016}
}

@article{417,
 abstract = {RESUMO
A new method and computer program for evaluating page resizing of a web browser. A web page with a test area having test elements is created. Then, the web page is displayed by the web browser and locations of at least two original edge points for each of the test elements are determined. After this, the test area within the web page is resized and displayed with the resized test area. Then, a further determination step is performed for each resized test element in the displayed web page, whereby locations of at least two respective resized edge points of the test element are determined. After obtaining these locations, comparisons between the locations of the at least two original edge points and the locations of the at least two respective resized edge points are made.},
 duplicado = {false},
 inserir = {false},
 title = {Evaluation of resizing capability of web browser},
 year = {2016}
}

@article{418,
 abstract = {Abstract
Modern software systems are becoming increasingly integrated and are required
to operate over organizational boundaries through networks. The development
of such distributed software systems has been shaped by the orthogonal trends
of service-orientation and process-awareness. These trends put an emphasis
on technological neutrality, loose coupling, independence from the execution
platform, and location transparency. Execution platforms supporting these
trends provide context and cross-cutting functionality to applications and are
referred to as engines.
Applications and engines interface via language standards. The engine implements
a standard. If an application is implemented in conformance to this
standard, it can be executed on the engine. A primary motivation for the usage
of standards is the portability of applications. Portability, the ability to move
software among different execution platforms without the necessity for full or partial
reengineering, protects from vendor lock-in and enables application migration
to newer engines.
The arrival of cloud computing has made it easy to provision new and scalable
execution platforms. To enable easy platform changes, existing international
standards for implementing service-oriented and process-aware software name
the portability of standardized artifacts as an important goal. Moreover, they
provide platform-independent serialization formats that enable the portable
implementation of applications. Nevertheless, practice shows that service-oriented
and process-aware applications today are limited with respect to their portability.
The reason for this is that engines rarely implement a complete standard, but
leave out parts or differ in the interpretation of the standard. As a consequence,
even applications that claim to be portable by conforming to a standard might
not be so.
This thesis contributes to the development of portable service-oriented and
process-aware software in two ways: Firstly, it provides evidence for the existence
of portability issues and the insufficiency of standards for guaranteeing software
portability. Secondly, it derives and validates a novel measurement framework
for quantifying portability. We present a methodology for benchmarking the
conformance of engines to a language standard and implement it in a fully
automated benchmarking tool. Several test suites of conformance tests for two
different languages, the Web Services Business Process Execution Language 2.0
and the Business Process Model and Notation 2.0, allow to uncover a variety of
standard conformance issues in existing engines. This provides evidence that the
standard-based portability of applications is a real issue. Based on these results,
this thesis derives a measurement framework for portability. The framework
v
is aligned to the ISO/IEC Systems and software Quality Requirements and
Evaluation method, the recent revision of the renowned ISO/IEC software quality
model and measurement methodology. This quality model separates the software
quality characteristic of portability into the subcharacteristics of installability,
adaptability, and replaceability. Each of these characteristics forms one part of the
measurement framework. This thesis targets each characteristic with a separate
analysis, metrics derivation, evaluation, and validation. We discuss existing
metrics from the body of literature and derive new extensions specifically tailored
to the evaluation of service-oriented and process-aware software. Proposed metrics
are defined formally and validated theoretically using an informal and a formal
validation framework. Furthermore, the computation of the metrics has been
prototypically implemented. This implementation is used to evaluate metrics
performance in experiments based on large scale software libraries obtained from
public open source software repositories.
In summary, this thesis provides evidence that contemporary standards and
their implementations are not sufficient for enabling the portability of processaware
and service-oriented applications. Furthermore, it proposes, validates, and
practically evaluates a framework for measuring portability.},
 duplicado = {false},
 inserir = {false},
 title = {Portability of Process-Aware and Service-Oriented Software},
 year = {2016}
}

@article{421,
 abstract = {RESUMO
In one embodiment, a distributed computing system includes a first worker node configured to execute a first job, a second worker node configured to execute a second job, and a master node including a processor coupled to a memory. The first job indicates a first portion of an interactive client-server application to be crawled. The second job indicates a second portion of an interactive client-server application to be crawled. The second worker node and the first worker node are configured to execute their respective jobs in parallel. The second job indicates a second portion of an interactive client-server application to be crawled. The master node is configured to assign the first job to the first worker node, assign the second job to the second worker node, and integrate the results from the first worker node and the second worker node into a record of operation of the application.},
 duplicado = {false},
 inserir = {false},
 title = {Architecture for distributed, parallel crawling of interactive client-server applications},
 year = {2016}
}

@article{423,
 abstract = {A browser and operating system (OS) compatibility system may include a user interface to select a first combination of a browser and OS for comparison to a second combination of a browser and OS. A download engine may capture source files for browsers for the first and second combinations. A parse engine may create Document Object Model (DOM) trees of objects of first and second website pages respectively for the browsers for the first and second combinations. A metric engine may read the DOM trees for attributes of the objects to capture metrics of the objects. An analysis metric engine may compare a metric of an object of the first website page to a metric of the same object of the second website page to determine a compatibility of the browser and the OS for the first combination to the browser and the OS for the second combination.},
 duplicado = {false},
 inserir = {true},
 title = {Browser and operating system compatibility},
 year = {2015}
}

@article{424,
 abstract = {Machine Learning (ML) software, used to implement an ML algorithm, is widely used in many application domains such as financial, business, and engineering domains. Faults in ML software can cause substantial losses in these application domains. Thus, it is very critical to conduct effective testing of ML software to detect and eliminate its faults. However, testing ML software is difficult, especially on producing test oracles used for checking behavior correctness (such as using expected properties or expected test outputs). To tackle the test-oracle issue, this thesis presents a novel black-box approach of multiple-implementation testing for supervised learning software. The insight underlying the approach is that there can be multiple implementations (independently written) for a supervised learning algorithm, and majority of them may produce the expected output for a test input (even if none of these implementations are fault-free). In particular, the proposed approach derives a pseudo oracle for a test input by running the test input on n implementations of the supervised learning algorithm, and then using the common test output produced by a majority (determined by a percentage threshold) of these n implementations. The proposed approach includes techniques to address challenges in multiple-implementation testing (or generally testing) of supervised learning software: the definition of test cases in testing supervised learning software, along with resolution of inconsistent algorithm configurations across implementations. In addition, to improve dependability of supervised learning software during in-field usage while incurring low runtime overhead, The approach includes a multiple-implementation monitoring technique. The evaluations on the proposed approach show that multiple-implementation testing is effective in detecting real faults in real-world ML software (even popularly used ones), including 5 faults from 10 NaiveBayes implementations and 4 faults from 20 k-nearest neighbor implementations, and the proposed technique of multiple-implementation monitoring substantially reduces the need of running multiple implementations with high prediction accuracy.},
 duplicado = {false},
 inserir = {false},
 title = {Multiple-implementation testing of supervised learning software},
 year = {2018}
}

@article{427,
 abstract = {As the number and variety of devices being used to access the World Wide Web grows exponentially, ensuring the correct presentation of a web page, regardless of the device used to browse it, is an important and challenging task. When developers adopt responsive web design (RWD) techniques, web pages modify their appearance to accommodate a devices display constraints. However, a current lack of automated support means that presentation failures may go undetected in a pages layout when rendered for different viewport sizes. A central problem is the difficulty in providing an automated oracle to validate RWD layouts against, meaning that checking for failures is largely a manual process in practice, which results in layout failures in many live responsive web sites. This paper presents an automated failure detection technique that checks the consistency of a responsive pages layout across a range of viewport widths, obviating the need for an explicit oracle. In an empirical study, this method found failures in 16 of 26 real-world production pages studied, detecting 33 distinct failures in total.},
 duplicado = {false},
 inserir = {false},
 title = {Automated layout failure detection for responsive web pages without an explicit oracle},
 year = {2017}
}

@article{428,
 abstract = {This thesis is about online shopping cart systems. After researching the various shopping cart systems, a user friendly shopping cart system has been developed and a mail delivery system has been incorporated into this shopping cart system. A mail delivery system is a system where the user can enter the email ID of a friend and suggest a product. Moreover, users can have a record of the items bought.},
 duplicado = {false},
 inserir = {false},
 title = {Online shopping cart},
 year = {2010}
}

@article{429,
 abstract = {Abstract:
With the advent of Web 2.0 application, and the increasing number of browsers and platforms on which the applications can be executed, cross-browser incompatibilities (XBIs) are becoming a serious problem for organizations to develop web-based software. Although some techniques and tools have been proposed to identify XBIs, a number of false positives and false negatives still exist as they cannot assure the same execution when the application runs across different browsers. To address this limitation, leveraging existing record/replay technique, we developed X-Check, a novel cross-browser testing technique and tool, which supports automated XBIs detection with high accuracy. Our empirical evaluation shows that X-Check is effective and improves the state of the art.},
 duplicado = {false},
 inserir = {true},
 title = {Detect Cross-Browser Issues for JavaScript-Based Web Applications Based on Record/Replay},
 year = {2016}
}

@article{430,
 abstract = {Abstract:
Web application internationalization frameworks allow businesses to more easily market and sell their products and services around the world. However, internationalization can lead to problems. Text expansion and contraction after translation may result in a distortion of the layout of the translated versions of a webpage, which can reduce their usability and aesthetics. In this paper, we investigate and report on the frequency and severity of different types of failures in webpages' user interfaces that are due to internationalization. In our study, we analyzed 449 real world internationalized webpages. Our results showed that internationalization failures occur frequently and they range significantly in terms of severity and impact on the web applications. These findings motivate and guide future work in this area.},
 duplicado = {false},
 inserir = {false},
 title = {An Empirical Study of Internationalization Failures in the Web},
 year = {2016}
}

@article{431,
 abstract = {With the advent of Web 2.0 application, and the increasing number of browsers and platforms on which the applications can be executed, cross-browser incompatibilities (XBIs) are becoming a serious problem for organizations to develop web-based software with good user experience. Although some techniques and tools have been proposed to identify XBIs, some XBIs are still missed as only partial state space is explored (by the crawler) in the testing environment. To address this limitation, based on record/replay technique, this paper proposed a crowdsourcing framework to detect cross-browser issues for Web application deployed in the field. Our empirical evaluation shows that the proposed technique is effective and efficient, improves on the state of the art.},
 duplicado = {false},
 inserir = {},
 title = {A Crowdsourcing framework for Detecting Cross-Browser Issues in Web Application},
 year = {2015}
}

@article{433,
 abstract = {Graphical user interfaces (GUIs) are populated with recurring behaviors that vary only slightly. For example, authentication (login / password) is a behavior common to many software applications. However, there are different behaviors between different implementations of this behavior. Sometimes a message appears when the user does not enter the correct data, sometimes, the application software only erases entered data and shows no indication to the user. These recurring behaviors (UI patterns) are well identified in the literature.The goal of this dissertation is to continue the work already done on an existing tool called PARADIGM-RE, a dynamic reverse engineering approach to extract User Interface (UI) Patterns from existent Web applications. As such, we will develop a data analysis module with the goal of improving and substantiate the existing identifying heuristics set, and we will extend the current set of identifiable patterns.},
 duplicado = {false},
 inserir = {false},
 title = {Engenharia reversa de padroes de interacao},
 year = {2014}
}

@article{434,
 abstract = {Abstract:
Failures in the presentation layer of a web application can negatively impact its usability and end users' perception of the application's quality. The problem of verifying the consistency of a web application's user interface across its different pages is one of the many challenges that software development teams face in testing the presentation layer. In this paper we propose a novel automated approach to detect and localize visual inconsistencies in web applications. To detect visual inconsistencies, our approach uses computer vision techniques to compare a test web page with its reference. Then to localize, our approach analyzes the structure and style of the underlying HTML elements to find the faulty elements responsible for the observed inconsistencies.},
 duplicado = {false},
 inserir = {false},
 title = {Detecting and Localizing Visual Inconsistencies in Web Applications},
 year = {2016}
}

@article{438,
 abstract = {In one embodiment, a user interface includes at least one instance of each of at least one widget. Recording a plurality of widget interaction instances (WIIs) for the user interface, each WII resulting from a user interaction applied to a particular instance of a particular widget. Clustering the plurality of WIIs based on a text value and a path value of each WII, such that each cluster of WIIs is associated with a particular widget. Determining, for each of at least one cluster of WIIs, whether the particular widget associated with the cluster of WIIs is erroneous based on whether user interactions corresponding to the WIIs in the cluster have produced responses from a software application that includes the user interface.},
 duplicado = {false},
 inserir = {false},
 title = {Detection of dead widgets in software applications},
 year = {2014}
}

@article{439,
 abstract = {Abstract
Developers have been trying to create uniform and consistent webpages in the different browsers available in the market. Known as Crossbrowser issue, it affects pages in different ways, on its functionalities and visually aspects and sometimes not related to the source code. Using screenshot and image comparison algorithms, this paper presents a technique for automated detection of visual deformations in web pages using a tool developed during the research called Automatic Deformations Detection in Internet Interfaces (ADDII).},
 duplicado = {false},
 inserir = {true},
 title = {Automatic Deformations Detection in Internet Interfaces: ADDII},
 year = {2015}
}

@article{440,
 abstract = {A distributed computing system includes worker nodes and a master node including a processor coupled to a memory. Each worker node crawls a portion of an interactive client-server application. The memory includes a master state graph, including the results of crawling. The master node is configured to examine the master state graph to determine a number of reconverging traces, receive a result from a job from a worker node if the number of reconverging traces is below a threshold, and add the result to the master state graph without attempting to remove duplicate states or transitions. A trace includes states and transitions representing valid. A reconvergent trace includes a trace including a reconvergent state, which is a state that can be reached through two or more distinct traces. The result containing states and transitions is associated with crawling a first portion of the interactive client-server application.},
 duplicado = {false},
 inserir = {false},
 title = {Technique for stateless distributed parallel crawling of interactive client-server applications},
 year = {2014}
}

@article{441,
 abstract = {Cascading Style Sheets (CSS) is a widely-used language for defining the presentation of structured documents and user interfaces. Despite its popularity, CSS still lacks adequate tool support for everyday maintenance tasks, such as debugging and refactoring. In this paper, we present CSSDev, a tool suite for analyzing CSS code to detect refactoring opportunities. (https://youtu.be/lu3oITi1XrQ)},
 duplicado = {false},
 inserir = {false},
 title = {CSSDev: refactoring duplication in cascading style sheets},
 year = {2017}
}

@article{442,
 abstract = {Web browsing has evolved from the viewing of static, content-oriented web pages to the execution of dynamic applications over the web. This has initiated the problem of cross browser incompatibility from simple look and feel issues to critical functional failures. Internet Explorer (IE) is the most unpopular browser among the developers due to its lack of web standards compliance and peculiar features. However, it is the most leading choice in the market according to recent browser usage statistics. As a result, developers discover that the support of IE in the web applications is needed for satisfying the customers. However, providing such support is difficult as IE has certain restrictions in its features. This paper introduces cross browser compatibility, uncovers few of the idiosyncrasies of IE and provides solutions which can be useful in developing web applications with IE browser support.},
 duplicado = {false},
 inserir = {true},
 title = {IE browser compatibility for web applications},
 year = {2012}
}

@article{443,
 abstract = {Abstract:
Incompatibility of webpages under different browsers and platforms is a typical technical obstruction for webpage design. To address this issue, a key challenge is to automatically detect the incompatible components and quantitatively assess the distortion extent in cross-browser tests. This paper presents a new algorithm for image pair comparison from webpages, called iterative perceptual hash (IPH), as well as a new distortion evaluation index called structure-color-saliency (SCS). The IPH that operates in an iterative manner is proposed to detect content changes considering both global structure and local content difference. The SCS assesses the distortion extent in both dimensions of image structure and color and is capable of imitating the nonlinear human perception. Experiment results demonstrate the effectiveness of IPH (e.g., F1-score 96%) and the high consistency of SCS with subjective results.},
 duplicado = {false},
 inserir = {true},
 title = {Webpage cross-browser test from image level},
 year = {2017}
}

@article{444,
 abstract = {Abstract:
Due to the rapid evolution of Web technologies and the failure of Web standards to uniformize every single technology evolution, Web developers are faced with the challenge of ensuring that their applications are correctly rendered across a broad range of browsers and platforms. While abidance to Web standards may reduce the chances of Web documents being inconsistently rendered across multiple browsers, in practice cross-browser compatibility issues are recurrent and range from minor layout bugs to critical functional failures such as a button being invisible in a given browser-platform combination. To detect cross-browser incompatibilities, developers often resort to visually checking that each document produced by their application is consistently rendered across all relevant browser-platform combinations. This manual testing approach is time consuming and error-prone. Existing cross-browser compatibility testing tools speed up this process by automating the rendering of a Web document in multiple browsers and platforms, and applying either image analysis or Document Object Model (DOM) analysis to highlight potential cross-browser incompatibilities. However, existing tools in this space suffer from over-sensitivity, meaning that they produce a large number of false positives as they tend to classify even insignificant differences as potential incompatibilities. Reducing the number of false positives produced by cross-browser compatibility testing tools is challenging, since defining criteria for classifying a difference as an incompatibility is to some extent subjective. This Master's thesis presents a machine learning approach to improve the accuracy of two techniques for cross-browser compatibility testing one based on image analysis (Browserbite) and one based on DOM analysis (Mogotest). To this end, we selected over 140 Web pages, each rendered in 10 to 14 browser-system combinations and built statistical classifiers to differentiate between true incompatibilities and false alarms. Two classification algorithms were used, namely classification trees and neural networks. An extensive experimental evaluation shows that neural networks produce highly accurate classifiers, both when post-processing the outputs of the image-based and the DOM-based technique. An attempt to combine image and DOM-based analysis is also reported.},
 duplicado = {false},
 inserir = {true},
 title = {Masinoppel pohinev veebilehtede uhilduvusdiagnostika},
 year = {2013}
}

@article{446,
 abstract = {Recently, technologies of web applications and web browser are becoming more complex, which causes different issues on different web browsers. Although there are many cross-browser detection tools on the market, all of them mainly focus on the front-end, and they are lack of detection on web page exploring and cross-browser issues detecting. The purpose of this research is to track the web applications structure and find their cross-browser issues.  This research first defines and classifies cross-browser issues, purposes the browser debugger XB-tester, collects data through cross-browser crawling technologies, and finally analyzes cross-browser issues and displays error reports.  Web developers can use this tool to effectively reduce the human cost to achieve cross-browser automation testing purposes.},
 duplicado = {false},
 inserir = {true},
 title = {Cross-Browser Compatibility Testing of Web Applications},
 year = {2015}
}

@article{447,
 abstract = {Using JavaScript and dynamic DOM manipulation on the client side of Web applications is becoming a widespread approach for achieving rich interactivity and responsiveness in modern Web applications. At the same time, such techniques---collectively known as Ajax---shatter the concept of webpages with unique URLs, on which traditional Web crawlers are based. This article describes a novel technique for crawling Ajax-based applications through automatic dynamic analysis of user-interface-state changes in Web browsers. Our algorithm scans the DOM tree, spots candidate elements that are capable of changing the state, fires events on those candidate elements, and incrementally infers a state machine that models the various navigational paths and states within an Ajax application. This inferred model can be used in program comprehension and in analysis and testing of dynamic Web states, for instance, or for generating a static version of the application. In this article, we discuss our sequential and concurrent Ajax crawling algorithms. We present our open source tool called Crawljax, which implements the concepts and algorithms discussed in this article. Additionally, we report a number of empirical studies in which we apply our approach to a number of open-source and industrial Web applications and elaborate on the obtained results.},
 duplicado = {false},
 inserir = {false},
 title = {Crawling Ajax-Based Web Applications through Dynamic Analysis of User Interface State Changes},
 year = {2012}
}

@article{448,
 abstract = {Abstract:
Context: Mobile app development is a relatively new phenomenon that is increasing rapidly due to the ubiquity and popularity of smartphones among end-users. Objective: The goal of our study is to gain an understanding of the main challenges developers face in practice when they build apps for different mobile devices. Method: We conducted a qualitative study, following a Grounded Theory approach, in which we interviewed 12 senior mobile developers from 9 different companies, followed by a semi-structured survey, with 188 respondents from the mobile development community. Results: The outcome is an overview of the current challenges faced by mobile developers in practice, such as developing apps across multiple platforms, lack of robust monitoring, analysis, and testing tools, and emulators that are slow or miss many features of mobile devices. Conclusion: Based on our findings of the current practices and challenges, we highlight areas that require more attention from the research and development community.},
 duplicado = {false},
 inserir = {false},
 title = {Real Challenges in Mobile App Development},
 year = {2013}
}

@article{449,
 abstract = {Abstract:
Ajax-based Web 2.0 applications rely on stateful asynchronous client/server communication, and client-side runtime manipulation of the DOM tree. This not only makes them fundamentally different from traditional web applications, but also more error-prone and harder to test. We propose a method for testing Ajax applications automatically, based on a crawler to infer a state-flow graph for all (client-side) user interface states. We identify Ajax-specific faults that can occur in such states (related to, e.g., DOM validity, error messages, discoverability, back-button compatibility) as well as DOM-tree invariants that can serve as oracles to detect such faults. Our approach, called Atusa, is implemented in a tool offering generic invariant checking components, a plugin-mechanism to add application-specific state validators, and generation of a test suite covering the paths obtained during crawling. We describe three case studies, consisting of six subjects, evaluating the type of invariants that can be obtained for Ajax applications as well as the fault revealing capabilities, scalability, required manual effort, and level of automation of our testing approach.},
 duplicado = {false},
 inserir = {false},
 title = {Invariant-Based Automatic Testing of Modern Web Applications},
 year = {2011}
}

@article{451,
 abstract = {Context

The Web has had a significant impact on all aspects of our society. As our society relies more and more on the Web, the dependability of web applications has become increasingly important. To make these applications more dependable, for the past decade researchers have proposed various techniques for testing web-based software applications. Our literature search for related studies retrieved 147 papers in the area of web application testing, which have appeared between 2000 and 2011.

Objective

As this research area matures and the number of related papers increases, it is important to systematically identify, analyze, and classify the publications and provide an overview of the trends in this specialized field.

Method

We review and structure the body of knowledge related to web application testing through a systematic mapping (SM) study. As part of this study, we pose two sets of research questions, define selection and exclusion criteria, and systematically develop and refine a classification schema. In addition, we conduct a bibliometrics analysis of the papers included in our study.

Results

Our study includes a set of 79 papers (from the 147 retrieved papers) published in the area of web application testing between 2000 and 2011. We present the results of our systematic mapping study. Our mapping data is available through a publicly-accessible repository. We derive the observed trends, for instance, in terms of types of papers, sources of information to derive test cases, and types of evaluations used in papers. We also report the demographics and bibliometrics trends in this domain, including top-cited papers, active countries and researchers, and top venues in this research area.

Conclusion

We discuss the emerging trends in web application testing, and discuss the implications for researchers and practitioners in this area. The results of our systematic mapping can help researchers to obtain an overview of existing web application testing approaches and indentify areas in the field that require more attention from the research community.},
 duplicado = {false},
 inserir = {false},
 title = {A systematic mapping study of web application testing},
 year = {2013}
}

@article{454,
 abstract = {Abstract:
As a result of the ubiquity and popularity of smart phones, the number of third party mobile applications is explosively growing. With the increasing demands of users for new dependable applications, novel software engineering techniques and tools geared towards the mobile platform are required to support developers in their program comprehension and analysis tasks. In this paper, we propose a reverse engineering technique that automatically (1) hooks into, dynamically runs, and analyzes a given iOS mobile application, (2) exercises its user interface to cover the interaction state space and extracts information about the runtime behaviour, and (3) generates a state model of the given application, capturing the user interface states and transitions between them. Our technique is implemented in a tool called iCrawler. To evaluate our technique, we have conducted a case study using six open-source iPhone applications. The results indicate that iCrawler is capable of automatically detecting the unique states and generating a correct model of a given mobile application.},
 duplicado = {false},
 inserir = {false},
 title = {Reverse Engineering iOS Mobile Applications},
 year = {2012}
}

@article{457,
 abstract = {Modern web applications consist of a significant amount of client- side code, written in JavaScript, HTML, and CSS. In this paper, we present a study of common challenges and misconceptions among web developers, by mining related questions asked on Stack Over- flow. We use unsupervised learning to categorize the mined questions and define a ranking algorithm to rank all the Stack Overflow questions based on their importance. We analyze the top 50 questions qualitatively. The results indicate that (1) the overall share of web development related discussions is increasing among developers, (2) browser related discussions are prevalent; however, this share is decreasing with time, (3) form validation and other DOM related discussions have been discussed consistently over time, (4) web related discussions are becoming more prevalent in mobile development, and (5) developers face implementation issues with new HTML5 features such as Canvas. We examine the implications of the results on the development, research, and standardization communities.},
 duplicado = {false},
 inserir = {false},
 title = {Mining questions asked by web developers},
 year = {2014}
}

@article{458,
 abstract = {Abstract:
This paper provides a retrospective examination of GUI Ripping - reverse engineering a workflow model of the graphical user interface of a software application - born a decade ago out of recognition of the severe need for improving the then largely manual state-of-the-practice of functional GUI testing. In these last 10 years, GUI ripping has turned out to be an enabler for much research, both within our group at Maryland and other groups. Researchers have found new and unique applications of GUI ripping, ranging from measuring human performance to re-engineering legacy user interfaces. GUI ripping has also enabled large-scale experimentation involving millions of test cases, thereby helping to understand the nature of GUI faults and characteristics of test cases to detect them. It has resulted in large multi-institutional Government-sponsored research projects on test automation and benchmarking. GUI ripping tools have been ported to many platforms, including Java AWT and Swing, iOS, Android, UNO, Microsoft Windows, and web. In essence, the technology has transformed the way researchers and practitioners think about the nature of GUI testing, no longer considered a manual activity; rather, thanks largely to GUI Ripping, automation has become the primary focus of current GUI testing techniques.},
 duplicado = {false},
 inserir = {false},
 title = {The first decade of GUI ripping: Extensions, applications, and broader impacts},
 year = {2013}
}

@article{459,
 abstract = {Abstract Dynamic exploration techniques play a significant
role in automated web application testing and analysis. However,
a general web application crawler that exhaustively explores
the states can become mired in limited specific regions of
the web application, yielding poor functionality coverage. In
this paper, we propose a feedback-directed web application
exploration technique to derive test models. While exploring,
our approach dynamically measures and applies a combination
of code coverage impact, navigational diversity, and structural
diversity, to decide a-priori (1) which state should be expanded,
and (2) which event should be exercised next to maximize the
overall coverage, while minimizing the size of the test model.
Our approach is implemented in a tool called FEEDEX. We
have empirically evaluated the efficacy of FEEDEX using six web
applications. The results show that our technique is successful in
yielding higher coverage while reducing the size of the test model,
compared to classical exhaustive techniques such as depth-first,
breadth-first, and random exploration.},
 duplicado = {false},
 inserir = {false},
 title = {Feedback-Directed Exploration of Web Applications to Derive Test Models},
 year = {2013}
}

@article{461,
 abstract = {JavaScript has become one of the most popular programming languages, yet it is known for its suboptimal design. To effectively use JavaScript despite its design flaws, developers try to follow informal code quality rules that help avoid correctness, maintainability, performance, and security problems. Lightweight static analyses, implemented in "lint-like" tools, are widely used to find violations of these rules, but are of limited use because of the language's dynamic nature. This paper presents DLint, a dynamic analysis approach to check code quality rules in JavaScript. DLint consists of a generic framework and an extensible set of checkers that each addresses a particular rule. We formally describe and implement 28 checkers that address problems missed by state-of-the-art static approaches. Applying the approach in a comprehensive empirical study on over 200 popular web sites shows that static and dynamic checking complement each other. On average per web site, DLint detects 49 problems that are missed statically, including visible bugs on the web sites of IKEA, Hilton, eBay, and CNBC.},
 duplicado = {false},
 inserir = {false},
 title = {DLint: dynamically checking bad coding practices in JavaScript},
 year = {2015}
}

@article{462,
 abstract = {Todays web applications increasingly rely on client-side code execution. HTML is not just created on the server, but manipulated extensively within the browser through JavaScript code. In this paper, we seek to understand the software engineering implications of this. We look at deviations from many known best practices in such areas of performance, accessibility, and correct structuring of HTML documents. Furthermore, we assess to what extent such deviations manifest themselves through client-side code manipulation only. To answer these questions, we conducted a large scale experiment, involving automated client-enabled crawling of over 4000 web applications, resulting in over 100,000,000 pages analyzed, and close to 1,000,000 unique client-side user interface states. Our findings show that the majority of sites contain a substantial number of problems, making sites unnecessarily slow, inaccessible for the visually impaired, and with layout that is unpredictable due to errors in the dynamically modified DOM trees.},
 duplicado = {false},
 inserir = {false},
 title = {Software engineering for the web: the state of the practice},
 year = {2014}
}

@article{464,
 abstract = {Abstract:
Although several distance or similarity functions for trees have been introduced, their performance is not always satisfactory in many applications, ranging from document clustering to natural language processing. This research proposes a new similarity function for trees, namely Extended Subtree (EST), where a new subtree mapping is proposed. EST generalizes the edit base distances by providing new rules for subtree mapping. Further, the new approach seeks to resolve the problems and limitations of previous approaches. Extensive evaluation frameworks are developed to evaluate the performance of the new approach against previous proposals. Clustering and classification case studies utilizing three real-world and one synthetic labeled data sets are performed to provide an unbiased evaluation where different distance functions are investigated. The experimental results demonstrate the superior performance of the proposed distance function. In addition, an empirical runtime analysis demonstrates that the new approach is one of the best tree distance functions in terms of runtime efficiency.},
 duplicado = {false},
 inserir = {false},
 title = {Extended Subtree: A New Similarity Function for Tree Structured Data},
 year = {2013}
}

@article{465,
 abstract = {Abstract:
Although asynchronous technologies such as Ajax make Rich Internet Applications (RIAs) responsive, they can result in unexpected behavior due to nondeterministic client-side processing and asynchronous communication. One difficulty in understanding such erroneous behavior lies in the unpredictable contexts of the running system. Dynamic behavior analysis techniques do not help to verify the correctness of certain "blind spots" in the execution path. In this work, we present a static approach for extracting all possible state transitions described in source code from the RIAs. Our approach is based on the assumption that user, server and self interactions with the RIAs can change the states of the application. Our method consists of three steps: (i) annotating interactions and extracting their controls in source code (ii) abstracting a call graph to extract relationships among the interactions and (iii) refining the relationships with interaction controls By extracting the state machines of test scenarios of the correct and wrong behavior, it can help developers to pinpoint the statements in the source code that lead to the erroneous behavior. Our approach has been evaluated against a few experimental cases and we conclude that it can extract comprehensible state machines in a reasonable time.},
 duplicado = {false},
 inserir = {false},
 title = {Extracting Interaction-Based Stateful Behavior in Rich Internet Applications},
 year = {2012}
}

@article{468,
 abstract = {Abstract:
Due to the increasing popularity and diversity of mobile devices, developers write the same mobile app for different platforms. Since each platform requires its own unique environment in terms of programming languages and tools, the teams building these multi-platform mobile apps are usually separate. This in turn can result in inconsistencies in the apps developed. In this paper, we propose an automated technique for detecting inconsistencies in the same native app implemented for iOS and Android platforms. Our technique (1) automatically instruments and traces the app on each platform for given execution scenarios, (2) infers abstract models from each platform execution trace, (3) compares the models using a set of code-based and GUI-based criteria to expose any discrepancies, and finally (4) generates a visualization of the models, highlighting any detected inconsistencies. We have implemented our approach in a tool called CheckCAMP. CheckCAMP can help mobile developers in testing their apps across multiple platforms. An evaluation of our approach with a set of 14 industrial and open-source multi-platform native mobile app-pairs indicates that CheckCAMP can correctly extract and abstract the models of mobile apps from multiple platforms, infer likely mappings between the generated models based on different comparison criteria, and detect inconsistencies at multiple levels of granularity.},
 duplicado = {false},
 inserir = {false},
 title = {Detecting inconsistencies in multi-platform mobile apps},
 year = {2015}
}

@article{469,
 abstract = {Abstract
In this paper we review five years of research in the field of automated crawling and testing of web applications. We describe the open source Crawljax tool, and the various extensions that have been proposed in order to address such issues as cross-browser compatibility testing, web application regression testing, and style sheet usage analysis.
Based on that we identify the main challenges and future directions of crawl-based testing of web applications. In particular, we explore ways to reduce the exponential growth of the state space, as well as ways to involve the human tester in the loop, thus reconciling manual exploratory testing and automated test input generation. Finally, we sketch the future of crawl-based testing in the light of upcoming developments, such as the pervasive use of touch devices and mobile computing, and the increasing importance of cyber-security.},
 duplicado = {false},
 inserir = {false},
 title = {Crawl-based analysis of web applications: Prospects and challenges},
 year = {2015}
}

@article{470,
 abstract = {Abstract:
Acceptance testing is an important part of software development and it is performed to ensure that a system delivers its required functionalities. Today, most modern interactive web applications are designed using Web 2.0 technologies, many among them relying on JavaScript. JavaScript enables the development of client-side functionality through the dynamic modification of the web-page's content and structure without calls to the server. This implies that server-side testing frameworks will necessarily fail to test the complete application behaviors. In this paper we present a method for automated acceptance testing of JavaScript web applications to ensure that required functionalities have been implemented. Using an intuitive, human-readable scripting language our method allows users to describe user stories in high level declarative test scripts and to then execute these test scripts on a web application using an automated website crawler. We also describe a case study that evaluates our approach in terms of capabilities to translate user stories in automated acceptance test scripts.},
 duplicado = {false},
 inserir = {false},
 title = {Automated Acceptance Testing of JavaScript Web Applications},
 year = {2012}
}

@article{472,
 abstract = {Abstract:
Graphical User Interface (GUI) testing literature emphasizes testing a system's functionality through its GUI, rather than testing visual aspects of the GUI itself. In this paper we introduce the notion of visual testing as a subset of GUI testing. To explore visual testing, we have conducted a study of defects in four open source systems. We found that visual defects represent between 16% and 33% of reported defects in those systems. Two categories of visual defects are identified with six subcategories within each of them. Other findings are also reported that are aimed at motivating the importance and the need for systematically conducting visual testing among researchers and practitioners.},
 duplicado = {false},
 inserir = {false},
 title = {Visual testing of Graphical User Interfaces: An exploratory study towards systematic definitions and approaches},
 year = {2012}
}

@article{478,
 abstract = {Abstract
Together with the expansion of the WWW we are seeing the expansion of mobile devices that are becoming more and more pervasive. Mobile application development is becoming more and more complex as users of mobile applications are demanding more high quality software. Our contribution is to frame the positive and negative aspects of native and multiple targeted mobile applications that should be considered by the involved stakeholders more particularly the software organization decision-makers.},
 duplicado = {false},
 inserir = {false},
 title = {Native and Multiple Targeted Mobile Applications},
 year = {2015}
}

@article{480,
 abstract = {When developing asynchronous JavaScript and XML (Ajax) applications, developers implement Ajax design patterns for increasing the usability of the applications. However, unpredictable contexts of running applications might conceal faults that will break the design patterns, which decreases usability. We propose a support tool called JSVerifier that automatically verifies interaction invariants; the applications handle their interactions in invariant occurrence and order. We also present a selective set of interaction invariants derived from Ajax design patterns, as input. If the application behavior breaks the design patterns, JSVerifier automatically outputs faulty execution paths for debugging. The results of our case studies show that JSVerifier can verify the interaction invariants in a feasible amount of time, and we conclude that it can help developers increase the usability of Ajax applications.},
 duplicado = {false},
 inserir = {false},
 title = {Automated verification of pattern-based interaction invariants in Ajax applications},
 year = {2013}
}

@article{483,
 abstract = {The challenge of validating Asynchronous JavaScript and XML (Ajax) applications lies in actual errors exposed in a user environment. Several studies have proposed effective and efficient testing techniques to identify executable faults. However, the applications might have faults that are not executed during testing, but might cause actual errors in a user environment. Although we have investigated static methods for finding ``potential faults'' that seem to cause actual errors if executed, developers need to confirm whether or not the potential faults are actually executable. Herein, we propose a mutation-based testing method implemented in a tool called JSPreventer. Even if the potential faults are not easily executable in a given environment, our method mutates the applications until they are executable using two delay-based mutation operators to manipulate the timing of the applications handling interactions. Thus, JSPreventer provides executable evidences of the not-easily-executable faults for developers, if it reveals actual errors by testing the mutated applications. We applied our method to real-world applications and found actual errors that developers could debug to improve their reliability. Therefore, JSPreventer can help developers validate reliable real-world Ajax applications.},
 duplicado = {false},
 inserir = {false},
 title = {Validating ajax applications using a delay-based mutation technique},
 year = {2014}
}

@article{487,
 abstract = {Architecture that provides a convenient and effective test tool for testing and ensuring that webpages using micro-browsers are sufficiently designed and operational. A task library is developed for manipulating browsers on a handheld device, and includes one or more seamless methods that operate the different browsers in the same way. Seamless virtual functions that manipulate different micro-browsers include, but are not limited to browser launch, exiting a browser, navigating to a home page, clearing a cache, navigating to a webpage, reloading a webpage, getting the current info for a webpage, navigating back to a webpage, checking a page title, and capturing a screen, for example. Device features can also be manipulated to ensure consistency across the testing process.},
 duplicado = {false},
 inserir = {true},
 title = {Automatic test tool for webpage design with micro-browsers on mobile platforms},
 year = {2014}
}

@article{490,
 abstract = {Cross-Site Scripting (XSS) is one of the most common web application vulnerabilities. It is therefore sometimes referred to as the buffer overflow of the web. Drawing a parallel from the current state of practice in preventing unauthorized native code execution (the typical goal in a code injection), we propose a script whitelisting approach to tame JavaScript-driven XSS attacks. Our scheme involves a transparent script interception layer placed in the browsers JavaScript engine. This layer is designed to detect every script that reaches the browser, from every possible route, and compare it to a list of valid scripts for the site or page being accessed; scripts not on the list are prevented from executing. To avoid the false positives caused by minor syntactic changes (e.g., due to dynamic code generation), our layer uses the concept of contextual fingerprints when comparing scripts.

Contextual fingerprints are identifiers that represent specific elements of a script and its execution context. Fingerprints can be easily enriched with new elements, if needed, to enhance the proposed methods robustness. The list can be populated by the websites administrators or a trusted third party. To verify our approach, we have developed a prototype and tested it successfully against an extensive array of attacks that were performed on more than 50 real-world vulnerable web applications. We measured the browsing performance overhead of the proposed solution on eight websites that make heavy use of JavaScript. Our mechanism imposed an average overhead of 11.1% on the execution time of the JavaScript engine. When measured as part of a full browsing session, and for all tested websites, the overhead introduced by our layer was less than 0.05%. When script elements are altered or new scripts are added on the server side, a new fingerprint generation phase is required. To examine the temporal aspect of contextual fingerprints, we performed a short-term and a long-term experiment based on the same websites. The former, showed that in a short period of time (10 days), for seven of eight websites, the majority of valid fingerprints stay the same (more than 92% on average). The latter, though, indicated that, in the long run, the number of fingerprints that do not change is reduced. Both experiments can be seen as one of the first attempts to study the feasibility of a whitelisting approach for the web.},
 duplicado = {false},
 inserir = {false},
 title = {How to Train Your Browser: Preventing XSS Attacks Using Contextual Script Fingerprints},
 year = {2016}
}

@article{492,
 abstract = {Abstract
Due to the increased usage of JavaScript in web applications and the speed at which web technologies and browsers are evolving, web applications are be- coming ever more complex. Our hypothesis is that these applications contain severe errors, take unnecessary performance penalties, and violate accessibility standards. This study analyzes such errors and tries to quantify the need for a tool that can help developers make web applications with less errors. The research is conducted by first showing how much of the DOM is modified after the initial page load. This could indicate that static analysis does not suffice anymore. After that we quantify the amount of faults in the web application. The research is done on 3,422 sites randomly selected from the internet. They were automatically analyzed using a crawler. We conclude that the use of static analysis tools to prevent these faults does not suffice anymore. The errors and accessibility standard violations happen in dynamically generated DOM, which are not detectable by static analysis. The performance penalties are only visible through dynamic analysis. We propose to develop a random testing tool based on a crawler that checks for these errors. Our main contributions are the design of such a tool, the large dataset that we have gathered during this research and the quantification of both the level of dynamism of modern web applications and the fault-proneness of these applications due to this dynamism.},
 duplicado = {false},
 inserir = {false},
 title = {Analyzing web applications: An empirical study},
 year = {2013}
}

@article{493,
 abstract = {Abstract  -  In this current era of information technology websites are very important means of communication. Lot of efforts is required by different institutions / organizations to portray complete information on beautifully designed websites.  Websites act as an online agent through which a user can get his work done without physically visiting the organizations.  Website design is given with a very critical look by the designer so that it can provide users with all the facilities of the concerned institutions / organizations online. To make websites behavior similar in all the different browsers employed by the different categories of the users, the responsibility of the designer and the concerned institutions / organizations increases manifold. In this research paper author developed an online tool using .NET Framework using C# to study cross browser compatibility as Design issue in various categories of the websites like Job portals, Government, educational,  Commercial and Social networking. The automated tool developed by author function on the basis of the different standards prescribed in W3C guidelines document UAAG 2.0 [7] and act like a parser and renders the complete code of the website and produces result on basis of the behavior of the websites in five most popular and widely used Browsers like parameters like Internet Explorer[7,8,9], Chrome, Safari, Fire fox. Each Browser is tested on the basis of the five parameters which are included in the parser are Blinking, Active X control, Website Resolution; image Formats, HTML Tag errors. The results  obtained after testing five different categories of websites shows that educational and social networking sites shows least compatibility in multiple browsers where as job portals, commercial and government websites shows 100% compliance to the website design standards recommended by W3C w.r.t browser compatibility of different websites on different browsing platform.    },
 duplicado = {false},
 inserir = {false},
 title = {cross browser compatibility as design issue in various websites.},
 year = {2015}
}

@article{495,
 abstract = {???????Xaa S????????????????????,????????????????????????????,?????????????????????????????????????Xaa S????????????????,????????????????},
 duplicado = {false},
 inserir = {false},
 title = {???XaaS????????????????},
 year = {2000}
}

@article{504,
 abstract = {Software testing is an expensive task that consumes around half of a projects effort. To reduce the cost of testing and improve the software quality, test cases can be produced automatically. Random Testing (RT) is a low cost and straightforward automated test generation approach. However, its effectiveness is not satisfactory. To increase the effectiveness of RT, researchers have developed more effective test generation approaches such as Adaptive Random Testing (ART) which improves the testing by increasing the test case coverage of the input domain. This research proposes new test case generation methods that improve the effectiveness of the test cases by increasing the diversity of the test cases. Numerical, string, and tree test case structures are investigated. For numerical test generation, the use of Centroidal Voronoi Tessellations (CVT) is proposed. Accordingly, a test case generation method, namely Random Border CVT (RBCVT), is introduced which can enhance the previous RT methods to improve their coverage of the input space. The generated numerical test cases by the other methods act as the input to the RBCVT algorithm and the output is an improved set of test cases. An extensive simulation study and a mutant based software testing investigation have been performed demonstrating that RBCVT outperforms previous methods. For string test cases, two objective functions are introduced to produce effective test cases. The diversity of the test cases is the first objective, where it can be measured through string distance functions. The second objective is guiding the string length distribution into a Benford distribution which implies shorter strings have, in general, a higher chance of failure detection. When both objectives are enforced via a multi-objective optimization algorithm, superior string test sets are produced. An empirical study is performed with several real-world programs indicating that the generated string test cases outperform test cases generated by other methods. Prior to tree test generation study, a new tree distance function is proposed. Although several distance or similarity functions for trees have been introduced, their failure detection performance is not always satisfactory. This research proposes a new similarity function for trees, namely Extended Subtree (EST), where a new subtree mapping is proposed. EST generalizes the edit base distances by providing new rules for subtree mapping. Further, the new approach seeks to resolve the problems and limitations of previous approaches. Extensive evaluation frameworks are developed to evaluate the performance of the new approach against previous methods. Clustering and classification case studies are performed to provide an evaluation against different tree distance functions. The experimental results demonstrate the superior performance of the proposed distance function. In addition, an empirical runtime analysis demonstrates that the new approach is one of the best tree distance functions in terms of runtime efficiency. Finally, the study on the string test case generation is extended to tree test case generation. An abstract tree model is defined by a user based on a program under the test. Then, tree test cases are produced according to the model where diversity is maximized through an evolutionary optimization technique. Real world programs are used to investigate the performance of generated test cases where superior performance of the introduced method is demonstrated compared to the previous methods. Further, the proposed tree distance function is compared against the previous functions in the tree test case generation context. The proposed tree distance function outperforms other functions in tree test generation. },
 duplicado = {false},
 inserir = {false},
 title = {Diversity-Based Automated Test Case Generation},
 year = {2015}
}

@article{507,
 abstract = {Abstract
JavaScript has become one of the widely-used languages. However, as the size of JavaScript-based applications grows, the number of defects grows as well. Recent studies have produced a set of manually defined rules to identify these defects. We propose, in this work, the automation of deriving these rules to ensure scalability and potentially the detection of a wider set of defects without requiring any extensive knowledge on rules tuning. To this end, we rely on a base of existing code smells that is used to train the detection rules using Genetic Programming and find the best threshold of metrics composing the rules. The evaluation of our work on 9 JavaScript web projects has shown promising results in terms of detection precision of 92% and recall of 85%, with no threshold tuning required.},
 duplicado = {false},
 inserir = {false},
 title = {On the Use of Smelly Examples to Detect Code Smells in JavaScript},
 year = {2017}
}

@article{512,
 abstract = {Search-based testing seeks to solve many novel problems including testing Ajax applications, and there have been a number of tools created to accomplish this purpose. Objective: This thesis aims to identify search-based software testing tools for Ajax web applications and how they have been evaluated. Method: Systematic literature review is used as the research methodology. Result: There are six different tools identified in scientific literature of which three are variants of Crawljax. Also, searchbased testing tools for Ajax are primarily evaluated through the use of case studies. Conclusion: The evaluation of the identified tools should be conducted using an experimental design in order to make them comparable and repeatable as well as follow benchmarking frameworks proposed in scientific literature.},
 duplicado = {false},
 inserir = {false},
 title = {Search-based testing tools for Ajax- A systematic literature review},
 year = {2015}
}

@article{513,
 abstract = {Modern web applications consist of a significant amount of client-side code,
written in JavaScript, HTML, and CSS. In this thesis, we present a study
of common challenges and misconceptions among web developers, by mining
related questions asked on Stack Overflow. We use unsupervised learning to
categorize the mined questions and define a ranking algorithm to rank all the
Stack Overflow questions based on their importance. We analyze the top 50
questions qualitatively. The results indicate that (1) the overall share of web
development related discussions is increasing among developers, (2) browser
related discussions are prevalent; however, this share is decreasing with time,
(3) form validation and other DOM related discussions have been discussed
consistently over time, (4) web related discussions are becoming more prevalent
in mobile development, and (5) developers face implementation issues
with new HTML5 features such as Canvas. We examine the implications
of the results on the development, research, and standardization communities.
Our results show that there is a consistent knowledge gap between the
options available and options known to developers. Given the presence of
knowledge gap among developers, we need better tools customized to assist
developers in building web applications.},
 duplicado = {false},
 inserir = {false},
 title = {Mining Stack Overflow for questions asked by web developers : an empirical study},
 year = {2014}
}

@article{514,
 abstract = {Web browsers are built by different organizations and writing
software that runs smoothly on all existing browsers is
a challenging task. Due to the pace that browsers implement
or adopt certain web features, users experience may
be hindered by visual and functional incompatibilities due
to unsupported or non-standard features in a given browser.
In order to address the detection of cross-browser incompatibilities
in early stages of web development, we propose
XCompatibility-Checker a lightweight tool that automates
the identification of features that are not supported by different
browsers. The tools evaluation was twofold (i) it
was able to detect cross-browser incompatibilities in 38 open
source web applications; as well as (ii) a user study and qualitative
survey indicates that the tool improves developers
awareness and ability to detect cross-browser incompatibilities.
Therefore, our proposed tool helps web developers
improve the quality of their web applications.},
 duplicado = {false},
 inserir = {true},
 title = {XCompatibility Checker: a tool for detecting cross-browser incompatibilities},
 year = {2018}
}

@article{521,
 abstract = {ABSTRACT
Browser compatibility is an aspect of web development
that reaches back to the beginning of the internet. Because
browsers have been marketed by various vendors
in a competitive landscape, fragmentation was inevitable.
This creates technical challenges for web developers whom
have incentive to create websites that perform predictably
on different browsers, browser versions and rendering engines.
Over the years the browser landscape has changed
and so has mutual compatibility. Until now, no research
has been conducted that provides insight in the developments
of this area. Little is therefore known about the
state of the art relative to the historical context. This paper
provides this missing information by using available
data sources. The outcomes show that developers should
be less concerned with supporting legacy browsers and versions
and can be more confident about feature support because
of an increase in use of more compatible browsers.
However, it was also found that feature support among
browsers is decreasing.},
 duplicado = {false},
 inserir = {false},
 title = {Browser Compatibility: the State of the Art from a Historical Perspective},
 year = {2016}
}

@article{524,
 abstract = {Test cases that drive an application under test via its graphical user interface (GUI) consist of sequences of steps that perform actions on, or verify the state of, the application user interface. Such tests can be hard to maintain, especially if they are not properly modularized---that is, common steps occur in many test cases, which can make test maintenance cumbersome and expensive. Performing modularization manually can take up considerable human effort. To address this, we present an automated approach for modularizing GUI test cases. Our approach consists of multiple phases. In the first phase, it analyzes individual test cases to partition test steps into candidate subroutines, based on how user-interface elements are accessed in the steps. This phase can analyze the test cases only or also leverage execution traces of the tests, which involves a cost-accuracy tradeoff. In the second phase, the technique compares candidate subroutines across test cases, and refines them to compute the final set of subroutines. In the last phase, it creates callable subroutines, with parameterized data and control flow, and refactors the original tests to call the subroutines with context-specific data and control parameters. Our empirical results, collected using open-source applications, illustrate the effectiveness of the approach.},
 duplicado = {false},
 inserir = {false},
 title = {Automated modularization of GUI test cases},
 year = {2015}
}

@article{527,
 abstract = {Abstract

We have several distance or similarity functions for trees, but their performance is not always adequate in different applications. In the base paper the Extended Sub tree (EST) function, where a new sub tree mapping is proposed. This similarity function is to compare tree structured data by defining a new set of mapping rules where sub trees are mapped rather than nodes. To reduce the time complexity as well as computational complexity of the system, efficient pruning algorithm is proposed. In the proposed system the unnecessary computation is reduced in the tree structured data by using the lossless pruning strategy. This paper provides major advancement in efficiency. This pruning strategy is ignoring the node or sub tree which has greater value than the ignoring probability. By using this technique, we can reduce the extra computation complexity.},
 duplicado = {false},
 inserir = {false},
 title = {A Study on Similarity Function for Tree Structured Data},
 year = {2014}
}

@article{535,
 abstract = {Asynchronous technologies such as Ajax make Rich Internet Applications (RIAs) responsive. When implementing and maintaining RIAs, developers have difficulties in figuring out complex behavior of the applications due to nondeterministic elements such as user events. Several researches have conducted to extract state machines based on execution results of Ajax applications for understanding support and testing. However, these execution results are within a limit of execution scenarios and environments prepared by developers. In this paper, we propose a tool that statically extracts state machines from Ajax-based RIAs by focusing on interactions with RIAs. We argue that the interactions can change the states of the application. Looking at both the extracted state machines and the source code, developers can verify the correctness of certain blind spots in the execution paths. From experimental results, we concluded that our tool could help participants understand the behavior and find faults. },
 duplicado = {false},
 inserir = {false},
 title = {Supporting to Find Faults in Rich Internet Applications by Extracting Interaction-based State Machines},
 year = {2013}
}

@article{541,
 abstract = {To test web applications, developers currently write test cases in frameworks such as Selenium. On the other hand, most web test generation techniques rely on a crawler to explore the dynamic states of the application. The first approach requires much manual effort, but benefits from the domain knowledge of the developer writing the test cases. The second one is automated and systematic, but lacks the domain knowledge required to be as effective. We believe combining the two can be advantageous. In this paper, we propose to (1) mine the human knowledge present in the form of input values, event sequences, and assertions, in the human-written test suites, (2) combine that inferred knowledge with the power of automated crawling, and (3) extend the test suite for uncovered/unchecked portions of the web application under test. Our approach is implemented in a tool called Testilizer. An evaluation of our approach indicates that Testilizer (1) outperforms a random test generator, and (2) on average, can generate test suites with improvements of up to 150% in fault detection rate and up to 30% in code coverage, compared to the original test suite.},
 duplicado = {false},
 inserir = {false},
 title = {Leveraging existing tests in automated test generation for web applications},
 year = {2014}
}

@article{551,
 abstract = {Abstract. Web applications are complex; they consist of many subsystems
and run on various browsers and platforms. This makes it difficult to
conduct adequate integration testing to detect faults in the connections
between subsystems or in the specific environments. Therefore, establishing
an efficient integration testing method with the proper test adequacy
criteria and tools is an important issue.
In this paper, we propose a new test coverage called template variable
coverage. We also propose a novel technique for generating skeleton test
code that includes accessor methods and improves the template variable
coverage criterion, using a tool that we developed called POGen. Our
experiments show that template variable coverage correlates highly with
the capability to detect faults, and that POGen can reduce testing costs},
 duplicado = {false},
 inserir = {false},
 title = {POGen: A Test Code Generator Based on Template Variable Coverage in Gray-Box Integration Testing for Web Applications},
 year = {2013}
}

@article{582,
 abstract = {Abstract. As the mobile platform continues to pervade all aspects of
human activity, and mobile applications, or mobile apps for short, on
this platform tend to be faulty just like other types of software, there is
a growing need for automated testing techniques for mobile apps. Modelbased
testing is a popular and important testing approach that operates
on a model of an apps behavior. However, such a model is often not available
or of insufficient quality. To address this issue, we present a novel
grey-box approach for automatically extracting a model of a given mobile
app. In our approach, static analysis extracts the set of events supported
by the Graphical User Interface (GUI) of the app. Then dynamic crawling
reverse-engineers a model of the app, by systematically exercising
these events on the running app. We also present a tool implementing
this approach for the Android platform. Our empirical evaluation of this
tool on several Android apps demonstrates that it can efficiently extract
compact yet reasonably comprehensive models of high quality for such
apps.},
 duplicado = {false},
 inserir = {false},
 title = {A Grey-box Approach for Automated GUI-Model Generation of Mobile Applications},
 year = {2013}
}

@article{584,
 abstract = {Today's enterprise web applications demand very high release cycles---and consequently, frequent tests. Automating these tests typically requires a behavior model: A description of the states the application can be in, the transitions between these states, and the expected results. Furthermore one needs scripts to make the abstract actions (transitions) in the model executable. As specifying such behavior models and writing the necessary scripts manually is a hard task, a possible alternative could be to extract them from existing applications. However, mining such models can be a challenge, in particular because one needs to know when two states are equivalent, as well as how to reach that state. We present ProCrawl (PROcess CRAWLer), a generic approach to mine behavior models from (multi-user) enterprise web applications. ProCrawl observes the behavior of the application through its user interface, generates and executes tests to explore unobserved behavior. In our evaluation of three non-trivial web applications (an open-source shop system, an SAP product compliance application, and an open-source conference manager), ProCrawl produces models that precisely abstract application behavior and which can be directly used for effective model-based regression testing.},
 duplicado = {false},
 inserir = {false},
 title = {Mining behavior models from enterprise web applications},
 year = {2013}
}

@article{586,
 abstract = {Abstract:
A great deal of effort in model-based testing is related to the creation of the model. In addition, the model itself, while a powerful tool of abstraction, can have conceptual errors, introduced by the tester. These problems can be reduced by generating those models automatically. This paper presents a dynamic reverse engineering approach that aims to extract part of the model of an existing web application through the identification of User Interface (UI) patterns. This reverse engineering approach explores automatically any web application, records information related to the interaction, analyses the gathered information, tokenizes it, and infers the existing UI patterns via syntactical analysing. After being complemented with additional information and validated, the model extracted is the input for the Pattern-Based Graphical User Interface Testing (PBGT) approach for testing existing web application under analysis.},
 duplicado = {false},
 inserir = {false},
 title = {Web Application Model Generation through Reverse Engineering and UI Pattern Inferring},
 year = {2014}
}

@article{587,
 abstract = {Abstract:
Test case generation from formal models using model checking software is an established method. This paper presents a model-based testing approach for web applications based on a domain-specific language model. It is shown how the domain-specific language is transformed into the input language of the NuSMV model checker and how the resulting traces are converted into executable test scripts for various test automation tools. The presented approach has been implemented with comprehensive automation in a research tool which architecture is outlined.},
 duplicado = {false},
 inserir = {false},
 title = {A Testing Tool for Web Applications Using a Domain-Specific Modelling Language and the NuSMV Model Checker},
 year = {2013}
}

@article{590,
 abstract = {Dynamic languages, such as JavaScript, give programmers the freedom to ignore types, and enable them to write concise code in short time. Despite this freedom, many programs follow implicit type rules, for example, that a function has a particular signature or that a property has a particular type. Violations of such implicit type rules often correlate with problems in the program. This paper presents TypeDevil, a mostly dynamic analysis that warns developers about inconsistent types. The key idea is to assign a set of observed types to each variable, property, and function, to merge types based in their structure, and to warn developers about variables, properties, and functions that have inconsistent types. To deal with the pervasiveness of polymorphic behavior in real-world JavaScript programs, we present a set of techniques to remove spurious warnings and to merge related warnings. Applying TypeDevil to widely used benchmark suites and real-world web applications reveals 15 problematic type inconsistencies, including correctness problems, performance problems, and dangerous coding practices.},
 duplicado = {false},
 inserir = {false},
 title = {TypeDevil: dynamic type inconsistency analysis for JavaScript},
 year = {2015}
}

@article{592,
 abstract = {Abstract This paper presents a model reverse-engineering approach
for mobile applications that belong to the Graphical User
Interface (GUI) application category. This approach covers the
interfaces of an application with automatic testing to incrementally
infer a formal model expressing the navigational paths and
states of the application. We propose the definition of a specialised
GUI application model which stores the discovered interfaces
and helps limit the application exploration. Then, we present an
algorithm based upon the Ant Colony Optimisation technique
which offers the possibility to parallelise the exploration and
to conceive any application exploration strategy. Finally, our
approach is experimented on Android applications and compared
to other tools available in the literature.},
 duplicado = {false},
 inserir = {false},
 title = {Model Reverse-engineering of Mobile Applications with Exploration Strategies},
 year = {2014}
}

@article{593,
 abstract = {Components of numerous software systems are developed and maintained by multiple stakeholders, and there is significant overlap and synergy in the process of testing systems with shared components. We have designed and implemented infrastructure that enables testers of different components to share their test results and artifacts so that they can collaborate in testing shared components. We also develop an example collaborative testing process that leverages our infrastructure to save effort for regression testing of systems with shared components. Our empirical study of this process shows that collaborative testing of component-based software systems can not only save significant effort by sharing test results and artifacts, but also improve test quality of individual components by utilizing synergistic data shared among component testers.},
 duplicado = {false},
 inserir = {false},
 title = {Enabling collaborative testing across shared software components},
 year = {2014}
}

@article{594,
 abstract = {Abstract:
Modern business applications predominantly rely on web technology, enabling software vendors to efficiently provide them as a service, removing some of the complexity of the traditional release and update process. While this facilitates shorter, more efficient and frequent release cycles, it requires continuous testing. Having insight into application behavior through explicit models can largely support development, testing and maintenance. Model-based testing allows efficient test creation based on a description of the states the application can be in and the transitions between these states. As specifying behavior models that are precise enough to be executable by a test automation tool is a hard task, an alternative is to extract them from running applications. However, mining such models is a challenge, in particular because one needs to know when two states are equivalent, as well as how to reach that state. We present Process Crawler (ProCrawl), a tool to mine behavior models from web applications that support multi-user workflows. ProCrawl incrementally learns a model by generating program runs and observing the application behavior through the user interface. In our evaluation on several real-world web applications, ProCrawl extracted models that concisely describe the implemented workflows and can be directly used for model-based testing.},
 duplicado = {false},
 inserir = {false},
 title = {Mining Workflow Models from Web Applications},
 year = {2015}
}

@article{595,
 abstract = {Many works relating to software engineering rely upon formal models to perform
model-checking or automatic test case generation. Nonetheless, producing
these models is tedious and error-prone. Model inference is a recent
research field helping in the production of models. This approach aims at
generating models from documentations or from execution traces (observed
action sequences). This paper presents a new model generation method combining
model inference with expert systems. Intuitively, an engineer is able
to recognise the functional behaviours of an application from its traces by
applying deduction rules. We propose a framework, simulating this way of
deducting, with inference rules organised into layers. Each yields partial
IOSTSs (Input Output Symbolic Transition System), which becomes more
and more abstract and understandable. For event-driven applications, our
proposal is also composed of a crawler, which aims at exploring the application
by means of automatic testing. This crawler is guided in the traversal
of the application with strategies that are implemented with inference rules
as well.},
 duplicado = {false},
 inserir = {false},
 title = {Model inference combining expert systems and formal models},
 year = {2014}
}

@article{596,
 abstract = {Many works related to software engineering rely upon formal models, e.g., to perform model-checking or automatic test case generation. Nonetheless, producing such models is usually tedious and error-prone. Model inference is a research field helping in producing models by generating partial models from documentation or execution traces (observed action sequences). This paper presents a new model generation method combining model inference and expert systems. It appears that an engineer is able to recognise the functional behaviours of an application from its traces by applying deduction rules. We propose a framework, applied to Web applications, simulating this reasoning mechanism, with inference rules organised into layers. Each yields partial IOSTSs (Input Output Symbolic Transition Systems), which become more and more abstract and understandable.},
 duplicado = {false},
 inserir = {false},
 title = {Inferring models with rule-based expert systems},
 year = {2014}
}

@article{597,
 abstract = {Abstract We consider, in this paper, the problem of automatically
testing Mobile applications while inferring formal
models expressing their functional behaviours. We propose a
framework called MCrawlT, which performs automatic testing
through application interfaces and collects interface changes to
incrementally infer models expressing the navigational paths
and states of the applications under test. These models could
be later used for comprehension aid or to carry out some
tasks automatically, e.g., the test case generation. The main
contributions of this paper can be summarised as follows: we
introduce a flexible Mobile application model that allows the
definition of state abstraction with regard to the application
content. This definition also helps define state equivalence
classes that segment the state space domain. Our approach
supports different exploration strategies by applying the Ant
Colony Optimisation technique. This feature offers the advantage
to change the exploration strategy by another one
as desired. The performances of MCrawlT in terms of code
coverage, execution time, and bug detection are evaluated on 30
Android applications and compared to other tools found in the
literature. The results show that MCrawlT achieves significantly
better code coverage in a given time budget.},
 duplicado = {false},
 inserir = {false},
 title = {Model Inference and Automatic Testing of Mobile Applications},
 year = {2015}
}

@article{601,
 abstract = {Abstract Model inference methods are attracting increased
attention from industrials and researchers since they can be
used to generate models for software comprehension, for test
case generation, or for helping devise a complete model
(or documentation). In this context, this paper presents an
original inference model approach which recovers models
from Web application HTTP traces. This approach combines
formal model inference with domain-driven expert systems. Our
framework, whose purpose is to simulate this human behaviour,
is composed of inference rules, translating the domain expert
knowledge, organised into layers. Each yields partial IOSTSs
(Input Output Symbolic Transition System), which become more
and more abstract and intelligible.},
 duplicado = {false},
 inserir = {false},
 title = {Domain-Driven Model Inference Applied To Web Applications},
 year = {2014}
}

@article{603,
 abstract = {A new generation of complex interactive dynamic web applications has emerged with the introduction
of Web 2.0 technologies and development frameworks. The characteristics of dynamic web
applications such as runtime DOM structure and content updates introduced new challenges in the
understanding, maintenance and testing of this type of web applications. In this work we address
two important challenges in the field of web application maintenance. The first challenge is that of
modelling web application behaviour. To solve this task we develop an automatic method for reverse
engineering the features of dynamic web applications by applying a hierarchical clustering algorithm
based on a novel composite-tree-edits-aware distance metric between DOM tree instances of a web
application. The proposed distance metric recognizes simple and composite structural changes in
a DOM tree. We have evaluated our method on three real-world web applications. The evaluation
results demonstrated that the proposed distance metric produces a number of clusters that is close to
the actual number of features and, also, classifies DOM trees into feature clusters more accurately
than other traditional distance metrics. The second challenge is that of systematic acceptance (and
regression) testing at the user-interface level, which we address by developing a tool, CrawlScripter,
for performing automated acceptance testing of JavaScript web applications. CrawlScripter allows
to create easy-to-understand acceptance tests using the provided library of high-level instructions.
The ability of CrawlScripter to create automated acceptance tests for different test scenarios was
evaluated on both pedagogical and real-world dynamic web applications.},
 duplicado = {false},
 inserir = {false},
 title = {Reverse Engineering and Testing Dynamic Web Applications},
 year = {2013}
}

@article{609,
 abstract = {Testing is paramount in order to assure the quality of a software product. Over the last years, several techniques have been proposed to leverage the testing phase as a simple and efficient step during software development. However, the features of the web environment make application testing fairly complex. The existing approaches for web application testing are usually driven to specific scenarios or application types, and few solutions are targeted for testing the functional requirements of applications. In order to tackle this problem, we propose a task-based testing approach that provides high coverage of functional requirements. Our technique consists of reassembling classical graph algorithms in order to generate all the possible paths for the execution of a task. Performed experiments indicate that our approach is effective for supporting the functional testing of web applications.},
 duplicado = {false},
 inserir = {false},
 title = {Leveraging task-based data to support functional testing of web applications},
 year = {2015}
}

@article{610,
 abstract = {Web applications are quickly replacing standalone applications for everyday tasks.
These web applications need to be tested to ensure proper functionality and reliability.
There have been substantial efforts to create tools that assist with the testing of web
applications, but there is no standard set of tools or a recommended workflow to
ensure speed of development and strength of application.
We have used and outlined the merits of a number of existing testing tools and
brought together the best among them to create what we believe is a fully-featured,
easy to use, testing framework and workflow for web application development.
We then took an existing web application, PolyXpress, and augmented its development
process to include our workflow suggestions in order to incorporate testing at
all levels. PolyXpress is a web application that allows you to create location-based
stories, build eTours, or create restaurant guides. It is the tool that will bring people
to locations in order to entertain, educate, or provide amazing deals.[10] After incorporating
our testing procedures, we immediately detected previously unknown bugs
in the software. In addition, there is now a workflow in place for future developers to
use which will expedite their testing and development.},
 duplicado = {false},
 inserir = {false},
 title = {CREATING A TESTING FRAMEWORK AND WORKFLOW FOR DEVELOPERS NEW TO WEB APPLICATION ENGINEERING},
 year = {2014}
}

@article{612,
 abstract = {Even so many years after its genesis, the Internet is still growing. Not only are
the users increasing, so are the number of different programming languages or
frameworks for building Web applications. However, this plethora of technologies
makes Web applications source code hard to comprehend and understand,
thus deteriorating both their debugging and their maintenance costs.
In this context, a number of proposals have been put forward to solve
this problem. While, on one hand, there are techniques that analyze the entire
source code of Web applications, the diversity of available implementation
technology makes these techniques return unsatisfactory results. On the other
hand, there are also techniques that dynamically (but blindly) explore the applications
by running them and analyzing the results of randomly exploring
them. In this case the results are better, but there is always the chance that
some part of the application might be left unexplored.
This thesis investigates if an hybrid approach combining static analysis and
dynamic exploration of the user interface can provide better results. FREIA, a
framework developed in the context of this thesis, is capable of analyzing Web
applications automatically, deriving structural and behavioral interface models
from them. },
 duplicado = {false},
 inserir = {false},
 title = {Reverse engineering of web applications},
 year = {2002}
}

@article{615,
 abstract = {Abstract:
Like all software, mobile applications ("apps") must be adequately tested to gain confidence that they behave correctly. Therefore, in recent years, researchers and practitioners alike have begun to investigate ways to automate apps testing. In particular, because of Android's open source nature and its large share of the market, a great deal of research has been performed on input generation techniques for apps that run on the Android operating systems. At this point in time, there are in fact a number of such techniques in the literature, which differ in the way they generate inputs, the strategy they use to explore the behavior of the app under test, and the specific heuristics they use. To better understand the strengths and weaknesses of these existing approaches, and get general insight on ways they could be made more effective, in this paper we perform a thorough comparison of the main existing test input generation tools for Android. In our comparison, we evaluate the effectiveness of these tools, and their corresponding techniques, according to four metrics: ease of use, ability to work on multiple platforms, code coverage, and ability to detect faults. Our results provide a clear picture of the state of the art in input generation for Android apps and identify future research directions that, if suitably investigated, could lead to more effective and efficient testing tools for Android.},
 duplicado = {false},
 inserir = {false},
 title = {Automated Test Input Generation for Android: Are We There Yet? (E)},
 year = {2015}
}

@article{617,
 abstract = {Abstract:
As the use of mobile devices becomes increasingly ubiquitous, the need for systematically testing applications (apps) that run on these devices grows more and more. However, testing mobile apps is particularly expensive and tedious, often requiring substantial manual effort. While researchers have made much progress in automated testing of mobile apps during recent years, a key problem that remains largely untracked is the classic oracle problem, i.e., to determine the correctness of test executions. This paper presents a novel approach to automatically generate test cases, that include test oracles, for mobile apps. The foundation for our approach is a comprehensive study that we conducted of real defects in mobile apps. Our key insight, from this study, is that there is a class of features that we term user-interaction features, which is implicated in a significant fraction of bugs and for which oracles can be constructed - in an application agnostic manner -- based on our common understanding of how apps behave. We present an extensible framework that supports such domain specific, yet application agnostic, test oracles, and allows generation of test sequences that leverage these oracles. Our tool embodies our approach for generating test cases that include oracles. Experimental results using 6 Android apps show the effectiveness of our tool in finding potentially serious bugs, while generating compact test suites for user-interaction features.},
 duplicado = {false},
 inserir = {false},
 title = {Automated Generation of Oracles for Testing User-Interaction Features of Mobile Apps},
 year = {2014}
}

@article{619,
 abstract = {Presentation failures in web applications can negatively affect an application's usability and user experience. To find such failures, testers must visually inspect the output of a web application or exhaustively specify invariants to automatically check a page's correctness. This makes finding presentation failures labor intensive and error prone. In this paper, we present a new automated approach for detecting and localizing presentation failures in web pages. To detect presentation failures, our approach uses image processing techniques to compare a web page and its oracle. Then, to localize the failures, our approach analyzes the page with respect to its visual layout and identifies the HTML elements likely to be responsible for the failure. We evaluated our approach on a set of real-world web applications and found that the approach was able to accurately detect failures and identify the faulty HTML elements.},
 duplicado = {false},
 inserir = {false},
 title = {Finding HTML presentation failures using image comparison techniques},
 year = {2014}
}

@article{622,
 abstract = {Abstract:
An attractive and visually appealing appearance is important for the success of a website. Presentation failures in a site's web pages can negatively impact end users' perception of the quality of the site and the services it delivers. Debugging such failures is challenging because testers must visually inspect large web pages and analyze complex interactions among the HTML elements of a page. In this paper we propose a novel automated approach for debugging web page user interfaces. Our approach uses computer vision techniques to detect failures and can then identify HTML elements that are likely to be responsible for the failure. We evaluated our approach on a set of real-world web applications and found that the approach was able to accurately and quickly identify faulty HTML elements.},
 duplicado = {false},
 inserir = {false},
 title = {Detection and Localization of HTML Presentation Failures Using Computer Vision-Based Techniques  Sign In or Purchase},
 year = {2015}
}

@article{624,
 abstract = {Abstract:
We present Web Mate, a tool for automatically generating test cases for Web applications. Given only the URL of the starting page, Web Mate automatically explores the functionality of a Web application, detecting differences across multiple browsers or operating systems, as well as across different revisions of the same Web application. Web Mate can handle full Web 2.0 functionality and explore sites as complex as Facebook. In addition to autonomously exploring the application, Web Mate can also leverage existing written or recorded test cases, and use these as an exploration base, this combination allows for quick expansion of the existing test base. Originating from research in generating test cases for specification mining, Web Mate is now the core product of a startup specializing in automated Web testing - a transfer that took us two years to complete. We report central lessons learned from this transfer, reflecting how robust, versatile, and pragmatic an innovative tool must be to be a success in the marketplace.},
 duplicado = {false},
 inserir = {true},
 title = {WebMate: Web Application Test Generation in the Real World},
 year = {2014}
}

@article{625,
 abstract = {Abstract:
Mobile app developers often wish to make their apps available on a wide variety of platforms, e.g., Android, iOS, and Windows devices. Each of these platforms uses a different programming environment, each with its own language and APIs for app development. Small app development teams lack the resources and the expertise to build and maintain separate code bases of the app customized for each platform. As a result, we are beginning to see a number of cross-platform mobile app development frameworks. These frameworks allow the app developers to specify the business logic of the app once, using the language and APIs of a home platform (e.g., Windows Phone), and automatically produce versions of the app for multiple target platforms (e.g., iOS and Android). In this paper, we focus on the problem of testing cross-platform app development frameworks. Such frameworks are challenging to develop because they must correctly translate the home platform API to the (possibly disparate) target platform API while providing the same behavior. We develop a differential testing methodology to identify inconsistencies in the way that these frameworks handle the APIs of the home and target platforms. We have built a prototype testing tool, called X-Checker, and have applied it to test Xamarin, a popular framework that allows Windows Phone apps to be cross-compiled into native Android (and iOS) apps. To date, X-Checker has found 47 bugs in Xamarin, corresponding to inconsistencies in the way that Xamarin translates between the semantics of the Windows Phone and the Android APIs. We have reported these bugs to the Xamarin developers, who have already committed patches for twelve of them.},
 duplicado = {false},
 inserir = {false},
 title = {Testing Cross-Platform Mobile App Development Frameworks (T)},
 year = {2015}
}

@article{626,
 abstract = {With the emergence of new computing platforms, software written for traditional platforms is being re-targeted to reach the users on these new platforms. In particular, due to the proliferation of mobile computing devices, it is common practice for companies to build mobile-specific versions of their existing web applications to provide mobile users with a better experience. Because the differences between desktop and mobile versions of a web application are not only cosmetic, but can also include substantial rewrites of key components, it is not uncommon for these different versions to provide different sets of features. Whereas some of these differences are intentional, such as the addition of location-based features on mobile devices, others are not and can negatively affect the user experience, as confirmed by numerous user reports and complaints. Unfortunately, checking and maintaining the consistency of different versions of an application by hand is not only time consuming, but also error prone. To address this problem, and help developers in this difficult task, we propose an automated technique for matching features across different versions of a multi-platform web application. We implemented our technique in a tool, called FMAP, and used it to perform a preliminary empirical evaluation on nine real-world multi-platform web applications. The results of our evaluation are promising, as FMAP was able to correctly identify missing features between desktop and mobile versions of the web applications considered, as confirmed by our analysis of user reports and software fixes for these applications.},
 duplicado = {false},
 inserir = {false},
 title = {Cross-platform feature matching for web applications},
 year = {2014}
}

@article{630,
 abstract = {Presentation failures in web applications can negatively impact users' perception of the application's quality and its usability. Such failures are challenging to diagnose and correct since the user interfaces of modern web applications are defined by a complex interaction between HTML tags and their visual properties defined by CSS and HTML attributes. In this paper, we introduce a novel approach for automatically identifying the root cause of presentation failures in web applications that uses image processing and search based techniques. In an experiment conducted for assessing the accuracy of our approach, we found that it was able to identify the correct root cause with 100% accuracy.},
 duplicado = {false},
 inserir = {false},
 title = {Root cause analysis for HTML presentation failures using search-based techniques},
 year = {2014}
}

@article{636,
 abstract = {Statistics comes in two main flavors: frequentist and Bayesian. For historical and technical reasons, frequentist statistics has dominated data analysis in the past; but Bayesian statistics is making a comeback at the forefront of science. In this paper, we give a practical overview of Bayesian statistics and illustrate its main advantages over frequentist statistics for the kinds of analyses that are common in empirical software engineering, where frequentist statistics still is standard. We also apply Bayesian statistics to empirical data from previous research investigating agile vs. structured development processes, the performance of programming languages, and random testing of object-oriented programs. In addition to being case studies demonstrating how Bayesian analysis can be applied in practice, they provide insights beyond the results in the original publications (which used frequentist statistics), thus showing the practical value brought by Bayesian statistics.},
 duplicado = {false},
 inserir = {false},
 title = {Bayesian Statistics in Software Engineering: Practical Guide and Case Studies},
 year = {1608}
}

@article{642,
 abstract = {Since people frequently access websites with a wide variety of devices (e.g., mobile phones, laptops, and desktops), developers need frameworks and tools for creating layouts that are useful at many viewport widths. While responsive web design (RWD) principles and frameworks facilitate the development of such sites, there is a lack of tools supporting the detection of failures in their layout. Since the quality assurance process for responsively designed websites is often manual, time-consuming, and error-prone, this paper presents ReDeCheck, an automated layout checking tool that alerts developers to both potential unintended regressions in responsive layout and common types of layout failure. In addition to summarizing ReDeChecks benefits, this paper explores two different usage scenarios for this tool that is publicly available on GitHub.},
 duplicado = {false},
 inserir = {false},
 title = {ReDeCheck: an automatic layout failure checking tool for responsively designed web pages},
 year = {2017}
}

@article{645,
 abstract = {Since Chromes initial release in 2008 it has grown in market share, and now controls roughly half of the desktop browsers market. In contrast with Internet Explorer, the previous dominant browser, this was not achieved by marketing practices such as bundling the browser with a pre-loaded operating system. This raises the question of how Chrome achieved this remarkable feat, while other browsers such as Firefox and Opera were left behind. We show that both the performance of Chrome and its conformance with relevant standards are typically better than those of the two main contending browsers, Internet Explorer and Firefox. In addition, based on a survey of the importance of 25 major features, Chrome product managers seem to have made somewhat better decisions in selecting where to put effort. Thus the rise of Chrome is consistent with technical superiority over the competition.},
 duplicado = {false},
 inserir = {false},
 title = {The rise of Chrome},
 year = {2015}
}

@article{646,
 abstract = {Abstract
The paper focuses on bugs in web applications that can be detected by analyzing the contents and layout of page elements inside a browser's window. Based on an empirical analysis of 35 real-world web sites and applications (such as Facebook, Dropbox, and Moodle), it provides a survey and classification of more than 90 instances of layout-based bugs. It then introduces Cornipickle, an automated testing tool that provides a declarative language to express desirable properties of a web application as a set of human-readable assertions on the page's HTML and CSS data. Such properties can be verified on-the-fly as a user interacts with an application.},
 duplicado = {false},
 inserir = {false},
 title = {Declarative layout constraints for testing web applications},
 year = {2016}
}

@article{648,
 abstract = {Cascading Style Sheets is the standard styling language, and is extensively used for defining the presentation of web, mobile and desktop applications. Despite its popularity, the language's design shortcomings have made CSS development and maintenance challenging. This thesis aims at developing techniques for safely transforming CSS code (through refactoring, or migration to a preprocessor language), with the goal of optimization and improved maintainability.},
 duplicado = {false},
 inserir = {false},
 title = { Refactoring and migration of cascading style sheets: towards optimization and improved maintainability},
 year = {2016}
}

@article{649,
 abstract = {Abstract:
Revolutionary changes in technology has brought a lots of different types of devices towards the Internet enabled services. It has become challenging to keep web sites functional and user friendly in a wide range of devices. Responsive Web Design (RWD) steps up to facilitate the necessary support to overcome those challenges with complex cascading style sheet (CSS) and JavaScript defined layouts. Fluidly modified layout that controls appearance of Responsive Web Pages (RWP) is subjected to erroneous when faulty CSS and JavaScript modifies the actual look of the web page. Though several methods have already been proposed to detect layout faults in web pages, they are error-prone and time consuming in nature. Thus an automatic approach considering both CSS and JavaScript defined dynamic layout fault detection technique have been proposed in this paper. Simulation result demonstrates that proposed technique outperforms the existing methods in case of time requirement, memory requirement and fault detection rate.},
 duplicado = {false},
 inserir = {false},
 title = {An automatic layout faults detection technique in responsive web pages considering JavaScript defined dynamic layouts},
 year = {2016}
}

@article{651,
 abstract = {Web applications pervade all aspects of human activity today. Therefore the content of the web has become extremely important. According to the great number of applications present nowadays, as a consequence, the manifestation of a bug has become very common. Testing modern web applications, so called "Web 2.0" applications has become more difficult due to their "stateful" nature, so the development of an approach capable of testing these applications in order to detect these bugs has become a necessity. The paper presents an automated approach for testing these dynamic web applications, where a combination of dynamic crawling and back-end testing is used to automatically detect behavioural bugs.},
 duplicado = {false},
 inserir = {false},
 title = {Searching for behavioural bugs with stateful test oracles in web crawlers},
 year = {2017}
}

@article{665,
 abstract = {The increasing complexity of software systems has lead to increased demands
on the tools and methods used when developing software systems.
To determine if a tool or method is more efficient or accurate than others
empirical studies are used. The data used in empirical studies might be
affected by outliers i.e. data points that deviates significantly from the
rest of the data set. Hence, the statistical analysis might be distorted by
these outliers as well.
This study investigates if outliers are present within Empirical Software
Engineering (ESE) studies using unsupervised methods for detection.
It also tries to assess if the statistical analyses performed in ESE studies
are affected by outliers by removing them and performing a re-analysis.
The subjects used in this study comes from a narrow literature review of
recently published papers within Software Engineering (SE). While collecting
the samples needed for this study the current state of practise
regarding data availability and analysis reproducibility is investigated.
This studys results shows that outliers can be found in ESE studies
and it also identifies issues regarding data availability within the same
field. Finally, this study presents guidelines for how to improve the way
outlier detection is presented within ESE studies as well as guidelines for
publishing data.},
 duplicado = {false},
 inserir = {false},
 title = {Unsupervised Outlier Detection in Software Engineering},
 year = {2014}
}

@article{668,
 abstract = {Abstract:
Online shopping new way of business in present days based on the previous surfing and purchasing products are recommended
to the users. The existing method of recommending the product has to undergo several processes or functionalities and these
processes or functionalities are manually tested for the accuracy. The manual testing method requires lot of time and money and
other resources. To overcome the problem this paper proposes a Automation Testing for the recommender system, with Feature
Vector Algorithm and perform a automation on each modules of the Feature Vector algorithm and also checks the CrossBrowser
compatibility across the browser and also collecting the online reviews from by using Web Crawling Technique.},
 duplicado = {false},
 inserir = {false},
 title = {Automated Recommender System for Web Based Applications},
 year = {2016}
}

@article{669,
 abstract = {Abstract:
Mobile devices such as smartphones and tablets have become an integral part of a person's life. These portable devices opened up a new software market for mobile application development resulting in various applications from healthcare, banking till entertainment. Therefore, there is a need for mobile applications to be reliable and maintainable. In this paper we introduce an equivalent class based technique for testing the graphical user interface of Android applications. This technique is a specification based approach, in which test cases are generated based on the functionalities and the graphical user interface specification. For each possible user interface event a set of test cases are generated using equivalence class partitioning approach. Once the test cases are generated for the given application, the app is executed based on the generated test cases and results are compared with the other testing techniques. From the obtained results we can infer that our approach detects more bugs than other previous work. In addition, this approach helps in the generation of test cases at an early in the app development life cycle.},
 duplicado = {false},
 inserir = {false},
 title = {Class coverage GUI testing for Android applications},
 year = {2016}
}

@article{675,
 abstract = {Abstract:
A great deal of effort in model-based testing is related to the creation of the model. In addition, the model itself, while a powerful tool of abstraction, can have conceptual errors, introduced by the tester. These problems can be reduced by generating those models automatically. This paper presents a dynamic reverse engineering approach that aims to extract part of the model of an existing web application through the identification of User Interface (UI) patterns. This reverse engineering approach explores automatically any web application, records information related to the interaction, analyses the gathered information, tokenizes it, and infers the existing UI patterns via syntactical analysing. After being complemented with additional information and validated, the model extracted is the input for the Pattern-Based Graphical User Interface Testing (PBGT) approach for testing existing web application under analysis.},
 duplicado = {false},
 inserir = {false},
 title = {Web Application Model Generation through Reverse Engineering and UI Pattern Inferring  Sign In or Purchase},
 year = {2014}
}

@article{677,
 abstract = {SQL injections are still the most exploited web application vulnerabilities. We present a technique to automatically detect such vulnerabilities through targeted test generation. Our approach uses search-based testing to systematically evolve inputs to maximize their potential to expose vulnerabilities. Starting from an entry URL, our BIOFUZZ prototype systematically crawls a web application and generates inputs whose effects on the SQL interaction are assessed at the interface between Web server and database. By evolving those inputs whose resulting SQL interactions show best potential, BIOFUZZ exposes vulnerabilities on real-world Web applications within minutes. As a black-box approach, BIOFUZZ requires neither analysis nor instrumentation of server code; however, it even outperforms state-of-the-art white-box vulnerability scanners.},
 duplicado = {false},
 inserir = {false},
 title = {Search-based security testing of web applications},
 year = {2014}
}

@article{679,
 abstract = {Abstract:
The paper focuses on bugs in web applications that can be detected by analyzing the contents and layout of page elements inside a browser's window. Based on an empirical analysis of 35 real-world web sites and applications (such as Facebook, Dropbox, and Moodle), it provides a survey and classification of more than 90 instances of layout-based bugs. It then introduces Cornipickle, an automated testing tool that provides a declarative language to express desirable properties of a web application as a set of human-readable assertions on the page's HTML and CSS data. Such properties can be verified on-the-fly as a user interacts with an application.},
 duplicado = {false},
 inserir = {false},
 title = {Testing Web Applications Through Layout Constraints},
 year = {2015}
}

@article{680,
 abstract = {WebMole is a browser-based tool that automatically and exhaustively explores all pages inside a web application. Contrarily to classical web crawlers, which only explore pages accessible through regular anchors, WebMole can find its way through Ajax applications that use JavaScript-triggered links, and handles state changes that do not involve a page reload. User-defined functions called oracles can be used to bound the range of pages explored by WebMole to specific parts of an application, as well as to evaluate Boolean test conditions on all visited pages. Overall, WebMole can prove a more flexible alternative to automated testing suites such as Selenium WebDriver.},
 duplicado = {false},
 inserir = {false},
 title = { Automated exploration and analysis of ajax web applications with WebMole},
 year = {2013}
}

@article{681,
 abstract = {Abstract:
Exploring modern web applications is a difficult task with the presence of client-side JavaScript code, as a crawler cannot jump or backtrack arbitrarily inside applications that maintain a state. In this paper, we present Web Mole, an automated crawler that implements a formal framework for web exploration that generalizes existing approaches. Web Mole uses an algorithm that explores an application without the need for arbitrary backtracking, it intercepts HTTP requests called from client-side code, and uses that information to perform selectively jump to pages while preserving the client-server state relationship. Comparisons with existing crawlers on various classes of graphs show that this strategy incurs a lower exploration cost.},
 duplicado = {false},
 inserir = {false},
 title = {Exhaustive Exploration of Ajax Web Applications with Selective Jumping},
 year = {2014}
}

@article{682,
 abstract = {Abstract:
Web crawling is the process of exhaustively exploring the contents of a web site or application through automated means. While the results of such a crawling can be put through numerous uses ranging from a simple backup to comprehensive testing and analysis, features of modern-day applications prevent crawlers from properly exploring applications. We provide an in-depth analysis of 15 such features, and report on their presence in a study of 16 real-world web sites. Based on that study, we develop a configurable web application where the presence of each such feature can be turned on or off, aimed as a test bench where existing crawlers can be compared in a uniform way. Our results, which are the first exhaustive comparison of available crawlers, indicates areas where future work should be aimed.},
 duplicado = {false},
 inserir = {false},
 title = {A Reference Framework for the Automated Exploration of Web Applications},
 year = {2014}
}

@article{683,
 abstract = {In the last 30 years there has been growing interest in additive manufacturing technology. Coauthorship
of published papers plays such an important role in scientific development. The
collaborative research social network on the field of additive manufacturing is paid particular
attention in this paper. A framework based on open source software is proposed to automate the
data collection and social network analysis. The characteristics of the overall network structure is
analyzed and visualized. The development process of AM collaboration community mode is
discussed. So as to provide inspiration for collaborative research and further development of
additive manufacturing technology.},
 duplicado = {false},
 inserir = {false},
 title = {Co-authorship Networks in Additive Manufacturing Studies Based on Social Network Analysis},
 year = {2016}
}

@article{684,
 abstract = {In this paper, we present ALEX, a web application that enables non-programmers to fully automatically infer models of web applications via active automata learning. It guides the user in setting up dedicated learning scenarios, and invites her to experiment with the available options in order to infer models at adequate levels of abstraction. In the course of this process, characteristics that go beyond a mere site map can be revealed, such as hidden states that are often either specifically designed or indicate errors in the application logic. Characteristic for ALEX is its support for mixed-mode learning: REST and web services can be executed simultaneously in one learning experiment, which is ideal when trying to compare back-end and front-end functionality of a web application. ALEX has been evaluated in a comparative study with 140 undergraduate students, which impressively highlighted its potential to make formal methods like active automata learning more accessible to a non-expert crowd.},
 duplicado = {false},
 inserir = {false},
 title = {ALEX: Mixed-Mode Learning of Web Applications at Ease},
 year = {2016}
}

@article{686,
 abstract = {In the field of software modernization static and dynamic analysis of the system is a core
part of the modernization process. The static analysis focuses on the extraction of structural
and architectural information by searching the source code for dependencies. The dynamic
analysis tracks method calls at runtime and enables statements about the real usage of
the software in a productive environment. Often the monitoring is executed from the
systems point of view and not from the user behavior perspective focusing on architectural
information. The idea of user behavior profile extraction was picked up by research groups
dealing with automatic generation of test cases. Based on user sessions, a user behavior
model is created. The derived test cases are augmented with workload characteristics for
simulating realistic load during the test execution.
This thesis we combine the concept of user behavior profiles and software modernization.
We introduce a session extraction tool and a behavior model extraction tool based on the
TeeTime framework. The Pipe-and-Filter architecture provides a good performance and
reusability. The session extraction tool processes custom records created with the Kieker
framework in order to create log files of user sessions. The session log files are analyzed
with the behavior model extraction tool. It supports several visualizations, calculates think
time statistics and is highly customizable for analyzing single processes.
We instrument the b+m bAV-Manager, an session-based, workflow-oriented administration
software for customer and calculation data of insurers, developed by the b+m
Informatik AG. The implementation of instrumentation components is realized with interceptors
of the Spring framework. In an experiment we monitor the web application and
record the user behavior with the session extraction tool. Based on the session logs we
analyze the screenflow and the workflow with the behavior extraction tool. The results
allow us, to make quantitative and qualitative statements about the user behavior in comparison
with the application model. We derive several suggestions regarding the imminent
modernization of the b+m bAV-Manager, that improve the matching of defined screen- and
workflows and the extracted user behavior profiles.},
 duplicado = {false},
 inserir = {false},
 title = {Extraction of User Behavior Profiles for Software Modernization},
 year = {2016}
}

@article{689,
 abstract = {As web applications increase in popularity, complexity, and size, approaches and tools to automate testing the correctness of web applications must continually evolve. In this chapter, we provide a broad background on web applications and the challenges in testing these distributed, dynamic applications made up of heterogeneous components. We then focus on the recent advances in web application testing that were published between 2010 and 2014, including work on test-case generation, oracles, testing evaluation, and regression testing. Through this targeted survey, we identify trends in web application testing and open problems that still need to be addressed.},
 duplicado = {false},
 inserir = {false},
 title = {Chapter Four - Advances in Web Application Testing, 2010 2014},
 year = {2016}
}

@article{690,
 abstract = {Abstract Automatic test case generation is an approach to decrease cost and time in software testing. Although there
have been lots of proposed methods for automatic test case generation of web applications, there still exists some
challenges which needs more researches. The most important problem in this area is the lack of a complete descriptive
model which indicates the whole behaviors of web application as guidance for the generation of test cases with high
software coverage. In this paper, test cases are generated automatically to test web applications using a machine
learning method. The proposed method called RTCGW (Rule-based Test Case Generator for Web Applications)
generates test cases based on a set of fuzzy rules that try to indicate the whole software behaviors to reach to a high
level of software coverage. For this purpose a novel machine learning approach based on fuzzy neural networks is
proposed to extract fuzzy rules from a set of data and then used to generate a set of fuzzy rules representing software
behaviors. The fuzzy rule set is then used to generate software test cases and the generated test cases are optimized
using an optimization algorithm based on combination of genetic and simulated annealing algorithms. Two
benchmark problems are tested using the optimized test cases. The results show a high level of coverage and
performance for the proposed method in comparison with other methods. },
 duplicado = {false},
 inserir = {false},
 title = {Automatic Test Case Generation for Modern Web Applications Using Population-Based Automatic Fuzzy Neural Network},
 year = {2014}
}

@article{692,
 abstract = {Abstract:
Load testing and cross-browser testing are ones of the web testing types particularly reliant on the support of cloud computing platforms for the implementation of TaaS. The main challenge involved in the composition of heterogeneous web application testing tools is the incompatibility of their inputs and outputs. However, the need to manually configure the tools greatly undermines the convenience and applicability of their applications. This paper proposes a system for the composition and delivery of heterogeneous web testing tools with the following contributions: (1) four adapters to automatically bridge the gap between the inputs and outputs of six state-of-the-art testing tools, (2) a composite web testing service with the adapters, and (3) two adapters to enable delivering the composite service via emails. Experiment results demonstrate the effectiveness of the proposed system in reducing the effort required for load testing and cross-browser testing by comparison with a conventional method.},
 duplicado = {false},
 inserir = {false},
 title = {Composing and Delivering Heterogeneous Web Testing Software as a Composite Web Testing Service},
 year = {2016}
}

