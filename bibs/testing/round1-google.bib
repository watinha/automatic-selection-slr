@article{100,
 abstract = {Combinatorial Testing (CT) can detect hard-to-find software faults triggered by interactions of parameters more efficiently than manual test case selection methods. It has been the active field of research today. Combinatorial strategy is a class of test case selection methods where test cases are identified by choosing values that satisfy the input criteria. Combinatorial Strategies are the test-case selection methods where the test cases are identified by combining those values to test object input parameters. This paper targets to review the previous work on CT, present the evolution of CT, presents a clear picture of combinatorial testing strategies with basic algorithms and identifies the important issues, methods and applications of CT that directs a pathway to future research in CT. This survey also includes a hierarchy that attempts to relate various criteria associated with combination strategies. Keywords: Combinatorial Testing, Test Case Selection, Partition Testing, Random Testing},
 duplicado = {false},
 inserir = {false},
 title = {A chronological survey on combinatorial testing strategies},
 year = {2014}
}

@article{101,
 abstract = {Both researchers and practitioners have emphasized the importance of learning from past experiences and its consequential impact on project time, cost, and quality. However, from the survey we conducted of requirements engineering (RE) practitioners, over 70\% of the respondents stated that they seldom use RE lessons in the RE process, though 85\% of these would use such lessons if readily available. Our observation, however, is that RE lessons are scattered, mainly implicitly, in the literature and practice, which obviously, does not help the situation. We, therefore, present ``maps of RE lessons which would highlight weak (dark) and strong (bright) areas of RE (and hence RE theories). Such maps would thus be: (a) a driver for research to ``light up the darker areas of RE and (b) a guide for practice to benefit from the brighter areas. To achieve this goal, we populated the maps with over 200 RE lessons elicited from literature and practice using a systematic literature review and survey. The results show that approximately 80\% of the elicited lessons are implicit and that approximately 70\% of the lessons deal with the elicitation, analysis, and specification RE phases only. The RE Lesson Maps, elicited lessons, and the results from populating the maps provide novel scientific groundings for lessons learnt in RE as this topic has not yet been systematically studied in the field.},
 duplicado = {false},
 inserir = {false},
 title = {Maps of Lessons Learnt in Requirements Engineering},
 year = {2013}
}

@article{103,
 abstract = {This study examined an EFL on-line writing programme with its automated feedback in terms of the students writing progress, the discrepancy between the on-line and teachers scores, and the teachers and students perceptions. Fifty seven EFL students aged 18-19 years old in an English-teacher education programme in Indonesia participated in the study. They completed eight practice writings and two writing tests using MY Access, an online writing programme. The analytical examination of their writing found that the students made progress most evidently in organization, as well as in content and development. A paired-sample t-test reported that the on-line programme and teachers scoring based on the same rubrics were significantly different, with the on-line programme giving higher grades than the teachers. The questionnaire and interview revealed that the teachers and students generally had a positive attitude towards the on-line programme, especially for its immediate feedback. Yet, three issues were repeatedly raised: 1) non-specific feedback, 2) overrated evaluations, and 3) the need for teacher feedback. The findings heighten the desirability for blended learning, and writing and feedback paradigm shifts. Writing should not only be placed on the cognitive plane but also be embedded in socio-cultural contexts. Feedback should helps the students develop their agency in writing and take ownership of it.},
 duplicado = {false},
 inserir = {false},
 title = {An EFL On-line Writing: Exploring Its Gains},
 year = {2015}
}

@article{106,
 abstract = {Purpose: The purpose of this paper is to describe a doctoral research study, including a summary of key literature review topics, the application of an action research methodology, key research findings and potential areas of future research. Design/methodology/approach: Research reported in this paper was based upon a series of five cycles of action research. Findings: An Outcome Profile template can be used to guide project stakeholders through the process of identifying, defining and aligning intangible project outcomes with tangible project outputs. Practical implications: The research study will advance general management and project management research and practice by providing organisations with an improved template and method for linking project stakeholders' outcomes and benefits?based perspective of project success with the more traditional project management outputs?based perspective of project success. The research study, in itself provides a how?to guide. Originality/value: This paper alerts practitioners and academics to this recent research work to improve the definition of intangible project outcomes; to better determine what value a given project is expected to deliver.},
 duplicado = {false},
 inserir = {false},
 title = {Development of a method to improve the definition and alignment of intangible project outcomes and tangible project outputs},
 year = {2008}
}

@article{107,
 abstract = {This paper seeks to contribute to the on-going research in knowledge management (KM) by presenting a study conducted in six public service agencies in Singapore. The study was guided by three research foci, namely, (1) to elucidate the nebulous nature of KM initiatives, (2) to uncover the motivation behind KM measurement and (3) to identify the various elements of a KM initiative that can be measured. Data collected from the public service agencies revealed that KM initiatives were generally top-down and technology-focused. Project management and the need to quantify the value of KM initiatives drove KM measurement. The measurement indicators adopted by the agencies encompassed four elements of measurement: activities, knowledge assets, organizational processes and business outcomes. In conclusion, this paper highlights two practical implications for the design of a KM measurement regime and suggests a number of possible directions for further research.},
 duplicado = {false},
 inserir = {false},
 title = {Untying the knot of knowledge management measurement: a study of six public service agencies in Singapore},
 year = {2008}
}

@article{108,
 abstract = {Purpose: This paper summarises a doctoral research study. The purpose is to provide a summary of the scope, literature review, main issues raised in the thesis, the application of case study research methodology, key research findings and potential areas for future research. Design/methodology/approach: Research reported here is based on a case study methodology for a better understanding of new product development (NPD) projects in the Australian Telecommunications service provider (Telco) industry, and what makes their project managers successful. Theoretical conclusions from the literature review were tested using empirical data from one?on?one interview with experienced project management professionals and major stakeholders (i.e. unit of data collection). Sample projects (i.e. units of analysis) were analysed to gain a better understanding of Telco NPD projects and the skill sets required for their success. Findings: A theoretical skill set framework of technical, leadership, managerial and administrative skills, has been proposed and summarised with clear definitions grounded in the recent management and leadership literature and with a rationale of how they contribute to project management success that this thesis fleshes out. This framework was empirically proven to define successful Telco NPD project managers. Practical implications: The research study advances knowledge of projects and their practices in a specific Telco industry NPD context, and proposes a better description of successful project managers. Its results have important implications for hiring, developing and assigning project managers to projects in different phases. Generic project manager competency standards need to be complemented and adjusted in accord with the findings from this research. Originality/value: This paper alerts academicians and practitioners to this research work on Telco industry NPD projects, and their project managers. It presents an up?to?date status of this industry, and requirements for project managers.},
 duplicado = {false},
 inserir = {false},
 title = {New product development projects and project manager skill sets in the telecommunications industry},
 year = {2009}
}

@article{11,
 abstract = {Aims and objectives: The aim was to explore how a participatory design (PD) approach involving adult patients and parents of children with disabilities could contribute to the development of an electronic Individual Care Plan (e-ICP) in Norway. The system was intended to simplify multi-disciplinary cross-sector documentation and collaboration between care professionals and patients in care planning. Methods: The data in the study comprised semi-structured interviews with patients and parents, as well as field notes. Systematic text condensation (STC) in a stepwise analysis model was performed on the data. Results: Testing through three phases resulted in system improvements and additional functionality according to the participating patients needs and requests. PD was initially applied, enabling a constructive dialogue between developers and patients. System training and collecting patient expectations was a preliminary task. Patients then brought testing experiences to the system developers, focusing first on access to information and document filing. Later, finalizing testing towards a tool for interaction with care professionals was a main concern. Conclusion: Adult patients and parents participating in the study provided various insights and expectations that informed system improvements and resulted in new functionality. System development and testing in healthcare can successfully incorporate patient involvement.},
 duplicado = {false},
 inserir = {false},
 title = {Patients' contribution to the development of a web-based plan for integrated care a participatory design study},
 year = {2015}
}

@article{110,
 abstract = {This study presents a decision making process in three steps of knowledge management for test organization using process simulation and financial analysis. First, project effort cost assessment of test knowledge management process subjects to different project duration and number of staffs is established. Two knowledge management simulation models representing experienced personnel with knowledge sharing and inexperienced personnel with internal training respectively are employed to contrast test personnel capability. Second, performance evaluation of software testing process by different personnel capability is conducted to simulate system test using three project metrics, namely, duration, effort cost, and quality. Third, a comparative financial analysis is prepared to determine the best solution by return on investment, payback period, and benefit cost ratio. The results from three stages of finding are discussed to arrive at the final scenario. We provide a case study evaluating how software testing industry needs to build effective test organization with high quality personnel for sustainable development and improvement.},
 duplicado = {false},
 inserir = {true},
 title = {Simulation-Based Evaluation for the Impact of Personnel Capability on Software Testing Performance},
 year = {2012}
}

@article{111,
 abstract = {Knowledge Management Systems (KMS) seek to offer a framework to stimulate the sharing of the intellectual capital of an organization so that the resources invested in time and technology can be effectively utilized. Recent research has shown that some businesses invest thousands of dollars to establish knowledge management (KM) processes in their organizations. Others are still in the initial phase of introduction, and many of them would like to embark on such projects. It can be observed, however, that the great majority of such initiatives have not delivered the returns hoped for, since the greatest emphasis is given to questions of technology and to the methodologies of KM projects. In this study, we call attention to an emerging problem which recent studies of the phenomenon of knowledge sharing have not sufficiently addressed: the difficulties and efforts of organizations in identifying their centers of knowledge, in developing and implementing KM projects, and in utilizing them effectively. Thus, the objective of this chapter is to propose a framework to evaluate the present state of an organization's processes and activities and identify which information and communication technologies (ICT) are supporting these initiatives, with the intention of diagnosing its real need for KM. Another objective of this instrument is to create a base of knowledge, with all the evaluations undertaken in organizations in different sectors and areas of specialization available to all participants in the process, as a way of sharing knowledge for continual improvement and dissemination of the best practices. About 30 companies took part in the first phase of investigation in 2008, and the knowledge base is under construction.},
 duplicado = {false},
 inserir = {false},
 title = {A Structure for Knowledge Management Systems Assessment and Audit},
 year = {2010}
}

@article{112,
 abstract = {Software testing involves the process of detecting software discrepancies so that they can be corrected before they are installed into a live environment supporting operational business units. To better support this complex task of software-testing, this study proposes identifying and applying a knowledge management (KM) approach to software testing. Based on literature review, three frameworks are identified, each of which was used in a project based environment. An integrated hybrid KM framework for software testing is developed by incorporating desirable aspects of the first two frameworks into the third one. To assess the effectiveness of the hybrid framework, an empirical study needs to be conducted.},
 duplicado = {false},
 inserir = {true},
 title = {Applying Knowledge Management Approach for Software Testing},
 year = {2007}
}

@article{113,
 abstract = {Effective software reuse has long been regarded as an important foundation for a more engineering-like approach to software development. Proactive recommendation systems that have the ability to unobtrusively suggest immediately applicable reuse opportunities can become a crucial step toward realizing this goal and making reuse more practical. This chapter focuses on tools that support reuse through the recommendation of source code-reuse-oriented code recommendation systems (ROCR). These support a large variety of common code reuse approaches from the copy-and-paste metaphor to other techniques such as automatically generating code using the knowledge gained by mining source code repositories. In this chapter, we discuss the foundations of software search and reuse, provide an overview of the main characteristics of ROCR systems, and describe how they can be built.},
 duplicado = {false},
 inserir = {false},
 title = {Reuse-Oriented Code Recommendation Systems},
 year = {2014}
}

@article{115,
 abstract = {Software testing is a mainly manually performed and thus very labour intensive process. Beside time, it demands a high amount of domain knowledge, concentration and problem awareness from the developer.
Although software reuse is a well examined area in both academia and industry it is mainly focussed on the reuse of different kinds of documentation and program code. In this thesis we create a client-side
recommendation system for the novel idea for an automated test recommendation approach that is based on lessons learned from traditional software reuse and recommendation. While most existing testing assistance systems help a developer by providing information about various coverage criteria only ex post, we want to support the developer pro-actively while writing the test and create as little overhead as possible during his work. Thereby we benefit from the lessons learned in the area of traditional software reuse and apply them in a kind of test reuse for test recommendation approach. To validate our theoretical considerations, we present a tool that will help writing tests with less effort.},
 duplicado = {false},
 inserir = {true},
 title = {Realizing Automated Test Recommendations in Software Development Environments},
 year = {2013}
}

@article{116,
 abstract = {The motivation for this research is the idea that training for each software developer should be in individually identified and focused areas of software security rather than a generalized security training for all developers. The main objective of our proposed system is to recommend precise and focused training articles to software developers. Our approach intends to help developers avoid unsafe coding practices in an efficient and effective way.},
 duplicado = {false},
 inserir = {false},
 title = {A method for recommending computer-security training for software developers},
 year = {2016}
}

@article{117,
 abstract = {For the past couple of decades, the usage of the Web as a platform for deploying software products has become incredibly popular. Web applications became more prevalent, as well as more complex. Countless Web applications have already been designed, developed, tested, and deployed on the Internet. However, it is noticeable that many common functionalities are present among these vast number of applications. This paper proposes an approach based on a database containing information from previous test artifacts. The information is used to generate test scenarios for Web applications under test. We have developed a tool based on our proposed approach, with the aim of reducing the effort required from software test engineers and professionals during the test planning and creation stage of software engineering. We evaluated our approach from three viewpoints: comparison between our approach and manual generation, qualitative evaluation by professional software engineers, and comparison between our approach and two open-source tools.},
 duplicado = {false},
 inserir = {true},
 title = {Test Scenario Generation for Web Application Based on Past Test Artifacts},
 year = {2014}
}

@article{118,
 abstract = {Model-driven software engineering uses models and meta models as key artefacts in the software development process. Typically, changes in the models (or meta models) do not come in isolation but are part of more complex change sets where a single change depends on other changes, e.g., a component is added to an architectural model and thereafter ports and connectors connect this component to other components. Furthermore, these sets of related and depending changes are often recurring, e.g., always when a component is added to an architecture, it is highly likely that ports are added to that component, too. This is similar for changes in meta models. Our goal is to help engineers by (1) automatically identifying clusters of related changes on model histories and (2) recommending corresponding changes after the engineer performs a single change. In this position paper, we present an initial technique to achieve our goal. We evaluate our technique with models from the Eclipse GMF project and present our recommendations as well as the recommendation quality. Our evaluation found an average precision between 0.43 and 0.82 for our recommendations.},
 duplicado = {false},
 inserir = {false},
 title = {Automatic Change Recommendation of Models and Meta Models Based on Change Histories},
 year = {2016}
}

@article{119,
 abstract = {Abstract: Security breaches in software systems cause massive financial and reputation losses to organizations and put their customers at risk by having their confidential data stolen. Delivering proper software security training to software developers is key to prevent such breaches. Conventional training methods do not take into account the code written by developers. The recommender system described in this paper analyzes developer code for security vulnerabilities and recommends mitigation strategies specific to each developer based on the detected vulnerabilities. The system utilizes a public vulnerability repository as its knowledge base. Such mitigation strategies are platform independent, giving further strength to the utility of the system. This paper extends our previous work and describes a human subject evaluation conducted to determine the usefulness of the system. Our evaluation suggests that this system successfully retrieves relevant training articles from the public vulnerability repository and human subjects found these articles to be suitable for training. The recommender system was found to be as effective as a commercial tool.},
 duplicado = {false},
 inserir = {false},
 title = {Human Subject Evaluation of Computer-Security Training Recommender},
 year = {2016}
}

@article{12,
 abstract = {Managing software development projects requires the coordination of different processes that may be performed by different teams, e.g., a development team and a separate testing team. This coordination aims at optimizing the tradeoff between cost, schedule and delivered quality. Simulation models are a powerful tool to explore what-if scenarios that help managers to achieve this trade-off and to fine-tune different project parameters. This paper presents a simulation model based on a multi-paradigm approach that connects development and testing processes. The testing process model is based on the process model described in the ISO/IEC/IEEE 29119-2:2013 standard. The simulation model is built using two different methods: the discrete-event approach, to simulate the execution of the dynamic testing processes, and the agentbased approach, to in-depth simulate defects life cycle. Results show how the simulation model is used to explore the evolution of a number of process metrics. Then, the simulation model is used to determine the resource distributions in order to optimize two relevant process metrics: the efficiency of the testing process and the average defect life},
 duplicado = {false},
 inserir = {false},
 title = {Simulation-based optimization for software dynamic testing processes},
 year = {2014}
}

@article{120,
 abstract = {Abstract: Customer knowledge has been increasingly recognized as a key strategic resource in any company's success. Recent studies conducted in the fields of Knowledge Management and Customer Relationship Management have proposed that the two approaches can have great synergies. In this paper, our purpose is to provide an understanding of Customer Knowledge Management (CKM) as an integrated management approach and competence it requires. We describe CKM as an ongoing process of generating, disseminating and using customer knowledge within an organization and between an organization and its customers. In addition, we propose a tentative theoretical framework of CKM competence, i.e., the ability to integrate customer knowledge into customer relationship management processes.},
 duplicado = {false},
 inserir = {false},
 title = {Customer Knowledge Management Competence: Towards a Theoretical Framework},
 year = {2005}
}

@article{121,
 abstract = {Purpose: Effective customer?specific knowledge transfer is the cornerstone of customer value creation in professional service organizations. In order to formulate a coherent service offering across different expertise areas, it is crucial to share customer?specific knowledge between professionals, business functions and units. The purpose of this study is to offer insights into the role of key account management (KAM) systems in facilitating this process. Design/methodology/approach: The work is based on an explorative case study in which the implementation of the KAM system in two consulting and training companies was investigated. Comparison of the two cases in terms of KAM design and success in knowledge transfer enabled conclusions to be drawn about the role of KAM as a knowledge carrier and a linking pin in a loosely coupled organization. Findings: Organizational fragmentation and insufficient communication channels among experts and subgroups of professional organizations cause problems in relation to knowledge transfer. This also makes it more difficult to combine expertise and to create innovative service concepts for customers. A KAM system, if managed effectively, provides a powerful tool for counteracting these problems. It functions as a linking pin in a loosely coupled organization, helping to maintain customer?specific knowledge transfer and continuity in customer relationships. Originality/value: Very little research has been conducted on customer?specific knowledge transfer in professional service organizations in spite of its central role in the creation of customer value. This study is unique in offering empirical evidence of the role of KAM systems in facilitating knowledge transfer. In the future, it would be interesting to study the role of different organizational conditions and practices, including organizational structures, the use of technological knowledge tools and cooperative working methods. The effectiveness of KAM systems in terms of financial performance and the creation of value for clients also deserve more research attention.},
 duplicado = {false},
 inserir = {false},
 title = {Customer knowledge transfer and key account management in professional service organizations},
 year = {2006}
}

@article{122,
 abstract = {Purpose: The knowledge of inhibitors of internal customer knowledge transfer in b?to?b professional service organizations is still in its infancy. Previous literature on professional service organizations has focused on knowledge processes on a general level without paying closer attention to inhibitors of internal knowledge transfer. This study aims to contribute by increasing the knowledge of various inhibitors of customer?related knowledge transfer and their influence on customer?related knowledge utilization in collaborative customer relationships. Design/methodology/approach: The present empirical article is based on a case study of two professional service organizations in the field of business?to?business education and consultancy services. An in?depth analysis of organizations developing collaborative relationships was conducted. Findings: This paper shows that internal fragmentation seems to be inherent in this type of organization, and may cause many problems in customer?related knowledge transfer among individuals, collegial groups and hierarchical levels in a professional service organization. All these problems in collective knowledge utilization influence both the service offering creation and general relationship coordination in the collaborative relationship. Orinality/value: This paper provides managerial suggestions for how to deal with the inhibitors of customer knowledge transfer. This includes developing unified goals, strengthening cultural cohesion and cooperation in the organization, building forums of dialogue between individuals and subgroups, and structuring relationship coordination systems (i.e. key account management systems), keeping customer?related knowledge transfer in mind.},
 duplicado = {false},
 inserir = {false},
 title = {Loose coupling as an inhibitor of internal customer knowledge transfer: findings from an empirical study in B to B professional services},
 year = {2008}
}

@article{123,
 abstract = {Knowledge management has an increasing importance in organizations. Not only as a way to capture knowledge, but also to allow the incorporation of knowledge in products and services provided. Wikis are finding their way into organizational departments serving as collaborative tools for knowledge creation. In this paper, we study how an organizational wiki can be used as a knowledge management (KM) tool from the point of view of two KM models. A survey was conducted in a corporate IT department. It was used to identify the processes of the SECI model and the contexts of the four competencies of a learning organization in which wikis can be used.},
 duplicado = {false},
 inserir = {false},
 title = {Organizational wiki as a knowledge management tool},
 year = {2010}
}

@article{124,
 abstract = {This article is based on a case study of a professional service organisation in the field of business-to-business education and consultancy services. This study contributes by increasing the knowledge of organisational inhibitors of customer knowledge utilisation in collaborative customer relationships by describing four organisational aspects inhibiting internal customer knowledge utilisation. The first is a professional service organisation's dominant logic, which refers to a barrier between organisation and customer. The second relates to cultural characteristics, referring to barriers between individuals and groups. The third barrier is the organisational structure of the professional service firm and the fourth barrier relates to systems and administrative routines.},
 duplicado = {false},
 inserir = {false},
 title = {What prevents effective utilisation of customer knowledge in professional B-to-B services? An empirical study},
 year = {2008}
}

@article{125,
 abstract = {Purpose: Knowledge is a main resource of any organization. Knowledge management (KM) is identified by four processes: creating, capturing, distributing and sharing of knowledge. Technology can enable successful KM. The purpose of this paper is to propose a technology knowledge management (TKM) taxonomy, which lists popular electronic tools that can enhance KM processes and shows which tool can contribute to which processes. Design/methodology/approach: The taxonomy was developed by an extensive literature review of electronic KM tools and a three-year extensive analysis of different knowledge sources at the Jeddah Municipality (JM) in Saudi Arabia. Findings: The taxonomy can be used by practitioners developing an organizational KM system to guide them to choose a sufficient subset of tools that covers all four processes in order to ensure that no process is overlooked. Research limitations/implications: The result of using the TKM taxonomy and its effect on KM success is an interesting area for further research. However, the current value underlies in it offering practitioners a rough roadmap to an electronic KM system and aids in giving at least a starting point. Practical implications: The TKM taxonomy can be used by large scale organizations to guide in developing a KM system effectively and more efficiently. Furthermore, the JM KC is a good model for similar organizations to use, with all the tools explained in the paper. Social implications: The paper addresses some of the social elements related to successful KM in organizations. However, it is more technically targeted. Originality/value: Researchers have investigated either the holistic effect of IT on KM or described certain tools. The types of IT tools and their effect on KM have not been investigated. Furthermore, limited research addresses the design of effective KM systems and no tools exist to guide designers. The TKM taxonomy is a tool that can help KM practitioners and strategists to design effective KM systems efficiently, by guiding them in choosing tools that are suitable for certain KM processes. The paper also describes the JM Knowledge Center as a KMS model for organizations which addresses all four KM processes.},
 duplicado = {false},
 inserir = {false},
 title = {Technology knowledge management (TKM) taxonomy: Using technology to manage knowledge in a Saudi municipality},
 year = {2014}
}

@article{127,
 abstract = {Abstract: The aim of this paper is to identify the benefits of an improved Knowledge Management (KM) strategy for a small/medium-sized professional services firm. The research examined how the three epistemological views (cognitivistic, connectionistic and autopoietic) influenced the management and use of organizational knowledge via the company intranet. The Intranet Evaluation Model (Skok and Kalmanovitch, 2005) was selected to evaluate the company's KM strategy and was extended to emphasize group, cultural and external aspects. Knowledge Evaluation Maps were used to present findings in a revealing graphical display. The results indicated that the company's KM strategy should focus on the high-value tacit knowledge of experts, develop the intranet in a connectionistic form, build upon existing knowledge activities and initiate more group activities. A knowledge steward should be appointed to motivate management and staff, and to overcome possible barriers to management buy-in and a knowledge silo culture. Furthermore, it was found that for small/medium-sized professional service firms, knowledge can be used as the enabler of competitive advantage but that the epistemologies within the firm should be considered when evaluating a KM strategy.},
 duplicado = {false},
 inserir = {false},
 title = {Managing organizational knowledge: developing a strategy for a professional services company},
 year = {2007}
}

@article{128,
 abstract = {One of the methods to achieve organization change and improvement has been organizational learning, and there has been much written about this subject. The benefits of organizational learning theory have not been sufficiently quantified in the extant literature. Therefore, the topic to which this study was directed is the relationship between organizational learning and organizational change. There is much literature in support of the benefits of organizational learning. Senge (1990a), Argyris and Schon (1974), Marsick and Watkins (1999), Knowles et al. (1998) and many others have laid out theoretical frameworks in the literature. Authors such a Jashapara (2003), Cameron & Whetten (1983), B.T. Phillips (2003) and others have noted the lack of quantifiable data in the literature in support of this theory. This study evaluated; (a) The relationship between organizational learning and organizational change success, (b) the relationship between organizational learning and adult learning, (c) the relationship between adult learning and organizational change success, and (d) the relationship between the combination of adult learning and organizational learning and organizational change success. Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.
A quantitative study was carried out using a 66 question survey instrument. A nonrandom convenience sample of professionals from organizations measured respondents'
perceptions of their organizations on organizational learning, adult learning and organizational change effectiveness. The results of this dissertation were that all hypotheses were supported with significant positive correlations between all related constructs, which were adult learning, organizational learning, and organizational change success. All anticipated hypotheses were supported.
},
 duplicado = {false},
 inserir = {false},
 title = {The relationship between organizational learning and organizational *change: Analyzing adult learning and organizational learning factors},
 year = {2007}
}

@article{13,
 abstract = {Abstract This study aims to design a framework for a knowledge management system in a cloud computing environment using a knowledge engineering approach. The research comprised 2 steps: 1) analyzing and synthesizing the relevant literature with regard to knowledge management systems in a cloud computing environment using a knowledge engineering approach and 2) designing a framework for a knowledge management system in a cloud computing environment using a knowledge engineering approach. Data were analyzed using content analysis. A knowledge management system consists of four components as follows: 1) knowledge retrieval 2) knowledge storage 3) knowledge sharing and 4) knowledge publishing. The knowledge engineering approach consists of three components as follows: 1) knowledge acquisition 2) knowledge storage and 3) knowledge tilization. },
 duplicado = {false},
 inserir = {false},
 title = {A Framework for a Knowledge Management System in a Cloud Computing Environment Using a Knowledge Engineering Approach },
 year = {2015}
}

@article{130,
 abstract = {Industrial research tends to be knowledge-intensive. Reports prepared by well-known industrial research concerns are often invaluable as they provide important directions for the industry. In Taiwan, the Industrial Technology Intelligence Service (ITIS) Program, supported by the Ministry of Economic Affairs, has served as a cradle for many industrial research professionals, and its success has been pivotal to the island nation's industrial might. This paper studies how ITIS could promote and influence the industrial research capabilities utilising the Knowledge Management (KM). Industrial Technology Research Institute (ITRI), the biggest state-fostered research institute in Taiwan, has successfully achieved its expectations by exercising KM. Each of the two has its own unique character that could generate different methodologies and processes. This research combines the two cases, analysing archived log files of the past 15 years, to provide the key success factors of their use of KM systems.},
 duplicado = {false},
 inserir = {false},
 title = {Application of the knowledge management in the knowledge-intensive service business: the case studies at ITIS and ITRI in Taiwan},
 year = {2007}
}

@article{131,
 abstract = {Knowledge management (KM) has an increasing importance in organizations. Not only as a way to capture knowledge, but also to allow the incorporation of knowledge in products and services provided. Wikis are finding their way into organizational departments serving as collaborative tools for knowledge creation. In this paper, as a future work of previous research, we study how an organizational wiki can be used as a knowledge management tool and how can it serve KM codification strategy purposes.},
 duplicado = {false},
 inserir = {false},
 title = {Different contributor profiles in an organizational wiki},
 year = {2010}
}

@article{133,
 abstract = {Knowledge Management (KM) has assumed a strategic role in organizations, being extremely
important that knowledge persists within organizations despite high employee turnover rates. To
achieve this goal, many companies have adopted wikis as collaborative knowledge management
tools, facilitating knowledge creation, capture and sharing.
This research studies how an organizational wiki may be used as a KM tool. Based on the literature
review, two models were selected, the organizational knowledge creation (SECI) and the four
competencies of a learning organization model. From this review emerged a set of research
hypotheses tested through an empirical study. Statistic analyses were performed with data collected
through a survey.
Finding the most common types of employees contributions allowed the identification of their profiles
as wiki contributors. Results obtained in this research were compared with those found by previous
studies in different contexts.
Results also show that, in the studied organization, the wiki is supporting knowledge transfer activities
from senior to junior employees, in the conversion from tacit to explicit knowledge.
Employees report that using the wiki has a positive impact on their performance, and the statistical
analysis of the collected data allowed the identification of the processes of the SECI model that have
higher influence in the performance indicators.},
 duplicado = {false},
 inserir = {false},
 title = {Os Wikis como sistemas colaborativos na gestao do conhecimento},
 year = {2010}
}

@article{134,
 abstract = {The government of Ethiopia gives great attention to agriculture and rural development for the country's economy development. Dairy development is one of the components of agricultural development. To improve dairy production in certain locality, dairy producers should able to access and use appropriate knowledge for the particular problem at the right time. This research was conducted to assess agricultural knowledge management system and its challenges and opportunities of knowledge management processes in Bure district. To address these objectives, both primary and secondary data were used. These were collected from primary (i.e. dairy producers and experts of different GOs and NGOs using semi-structured questionnaire and checklist) and secondary sources (i.e. literature reviews). To select representative respondents, multi stage sampling techniques were used. SPSS software (version 15) was used to analysis the data which is collected by questionnaire. As survey result, the major objective of the majority respondents engaged in dairy farming is for milk consumption and obtaining ox for draught power. Keeping the health condition of animals, feed green pasture to their milking cows, animal selection and using crossbreed cow are the major mechanisms, which are used by dairy producers, to improve the milk production in the district. They obtain these knowledge/mechanisms from WARDO, their own experience neighbors and family through different means. These are observing the farmer's farm, listening to radio, experience sharing sessions and on-farm demonstrations.. Majority of the dairy producers use the new knowledge by doing partial modification. They also transferred their knowledge to their neighbors, friends, relative and children. Dairy production in the district is not market oriented, rather it use for household consumption and obtaining ox for draught power. Therefore, to change this production system into market oriented, concerned bodies should provide adequate dairy technologies and trainings to dairy producers. Besides, promoting and strengthening the existing good practices in knowledge managements processes.},
 duplicado = {false},
 inserir = {false},
 title = {Agricultural knowledge management: Case of dairy production improvement},
 year = {2010}
}

@article{135,
 abstract = {The purpose of this Master's thesis was to study customer knowledge transfer processes in multinational corporations (MNCs). The main objective was to examine how customer knowledge is transferred in MNCs and what kind of factors enhance or inhibit the knowledge transfer process, and to create a framework on the basis of the existing literature and the empirical findings. In this thesis the factors were organized according to whether they are properties of the unit involved in knowledge management, properties of relationships between the units or properties of the knowledge itself. There are various properties that influence knowledge transfer but in this thesis the focus was on examining the relevant findings from the customer knowledge viewpoint. Empirical results show that internal fragmentation in the MNC seems to be inherent in this type of organization, and may cause many problems in customer knowledge transfer and utilization. These knowledge transfer inhibitors rise from the organization's properties: it's absorptive capacity, motivation, organizational culture, and the two dimensions of knowledge. However, in spite of the inherent forces causing internal fragmentation and inhibiting knowledge transfer, moderate customer knowledge and expertise codification, cooperative working practices among the experts, and socialization mechanisms posed by the headquarters seem to help maintain customer knowledge transfer, and value creation in the long-term relationship. This value creation can be seen to be based on accessing and integrating a wide variety of knowledge resources in order to create a coherent product and service offering.},
 duplicado = {false},
 inserir = {false},
 title = {Customer Knowledge Transfer in MNCs},
 year = {2015}
}

@article{136,
 abstract = {Information technology (IT) project failure (close to 71%) has become the norm despite adopting various IT methodologies; this failure rate is concerning to IT program and project managers. The purpose of this study was to identify knowledge management (KM) activities that might address, through an integrated KM framework, the causes for such failures. KM theories by Nonaka and Boisot form the basis of this work. The research questions in this qualitative case study with five project cases, implemented during 2007-2010, were designed to explore KM activities via five major variables affecting IT projects: scope, time, quality, money, and staff. Data were coded and themes generated from the data. The research participants included project managers and project leads from financial services industry in the USA. Results suggest knowledge flow was the major KM component lacking in the project teams and was included in the construction of the KM framework along with the rest of the building blocks. The research prescribes a system of metrics that allows project teams adopting the KM framework to assess its value. Results suggest that formal adoption of the KM framework would inculcate a knowledge culture in the project teams and in the organizations. The intent of the study was to affect positive social change through improving diffusion, dissemination and facilitation of knowledge. The implications for social change are improved and positive interactions among people leading to shift in mental models, which mend human behavior towards collaborative thinking and working style.},
 duplicado = {false},
 inserir = {fase},
 title = {Integrated Knowledge Management Framework for Addressing IT Project Failures},
 year = {2011}
}

@article{137,
 abstract = {Professional services represent a specific type of business service, one that is strongly based on the knowledge of individual professionals and organizational processes to harness that knowledge. Modularization can help in coordinating creation of the offering and enhancing the cost-effectiveness of processes. However, utilizing modularity in services, and particularly in professional service firms, is still a field that deserves specific studies. While the product modularity literature has developed definitions, metrics, and a number of hypotheses about modularity, in the field of professional services, we still have to clarify how firms can use modularity.While professional services are often generated from expert-embedded and even tacit knowledge, which is hard to transfer, implementing modularity may offer one way to facilitate knowledge sharing related to service offerings, organizational processes, and practices. However, when considering the strategic importance of modularity for a professional service firm, it is also necessary to define what the characteristics are in the knowledge base of the organization. Not all professional service firms are alike. The purpose of the present study is to examine how service modularity can be employed within the context of professional services. More specifically, (1) what knowledge-related characteristics vary between different types of professional service firms? (2) How do these characteristics influence modularization implementation? Two company cases are analyzed to reveal how modularity can be implemented in varying professional service firm environments.},
 duplicado = {false},
 inserir = {false},
 title = {Implementing Modularization in Professional Services The Influence of Varied Knowledge Environments},
 year = {2017}
}

@article{138,
 abstract = {Abstract. As marketing professionals Knowledge management is not the first subject that pops to our mind ,for most of the people from our profession what sparks to our mind with KM is the huge amount of data and documents but the fact is that managing these data effectively can dramatically enhance marketing activities of the company and can fetch a better result to the company in fact it can improve service quality, marketing effectiveness, the success of new products and services, customer advocacy, profitability, and much more. But reaching to this goal need a profound change in our perspective of Marketing Knowledge Management. This paper is trying to provide an introduction to the field of Marketing knowledge management and puts its focus on market knowledge which is always overlooked and examines role it can play in the organizational success and provides an overview of core concepts and compares different perspectives of Marketing knowledge management discussed in the literature it will be concluded with the key implications of KM for marketing managers and provides some suggestion for future research.},
 duplicado = {false},
 inserir = {false},
 title = {The Opportunity and Need for Marketing Knowledge Management},
 year = {2012}
}

@article{139,
 abstract = {This chapter discusses the basic properties of corporate Wikis that make them an effective learning and knowledge management tool. Wikis offer a user-friendly environment that enhances informal knowledge sharing and the collaborative creation of new knowledge. Enterprise-wide adoption of Wikis promotes the reuse of existing know-hows and prevents employee re-invention of the wheel. Four cases of successful implementations of Wikis in large, hi-tech global organizations are described in detail including their goals, design considerations, implementation and actual use for formal and informal knowledge creation and sharing. The adoption and long-term sustainability of Wikis is attributed to perceived business outcomes by managers and to perceived usefulness and ease of use by individual contributors and users. Good practices based on one or more of these use-cases can provide practical guidance to organizations that wish to use a Wiki for KM purposes.},
 duplicado = {false},
 inserir = {false},
 title = {Principles and Good Practices for Using Wikis within Organizations},
 year = {2017}
}

@article{14,
 abstract = {The increasing adoption and use of Open Source Software (OSS) motivates study of its development. This chapter explores the state-of-the art in OSS development processes, in general, and OSS testing processes, in particular. A conceptual model for software Testing Knowledge Management (TKM) that aims to provide an understanding of the testing domain is introduced. The TKM model is informed by earlier studies and guided by international testing standards. Moreover, the TKM model is equipped with different forms of knowledge, reusable across software projects. Using the TKM model as an integrative conceptual model enables understanding of how knowledge life cycle stages are mapped onto the test process of OSS, what type of knowledge is created at each stage, and how knowledge is converted from one stage to another. The chapter is supported by representative examples of OSS that are mature and currently in widespread use.},
 duplicado = {false},
 inserir = {true},
 title = {Managing Knowledge in Open Source Software Test Process},
 year = {2015}
}

@article{142,
 abstract = {Abstract: In order to maintain their survival and prosperity, organizations must understand the importance of knowledge. Knowledge management is the study of how organizations create and transform knowledge in order to achieve a competitive advantage. The development of communication and information technology has allowed an increasing cooperation and interaction of the knowledge flux inside the organizational environment. Wiki is one of the technological tools which have been highlighted because it allows companies to boost their knowledge related processes. This investigation studied how Wikis are used as a knowledge management tool, in the organizational environment of a public non profit company (thus departing from studies that are centered on private companies). Our theoretical development highlights the relevance of KM models, namely the SECI model. Using the SECI model we were able to identify the importance of this tool for the creation of knowledge and the overall performance of the users, showing that it increased their autonomy and efficiency.},
 duplicado = {false},
 inserir = {false},
 title = {Wiki as a Knowledge Management Tool: The Case of a Non?Profit Administrative Entity},
 year = {2014}
}

@article{143,
 abstract = {Software engineering comprehends several disciplines devoted to prevent and remedy malfunctions and to warrant adequate behaviour. Testing, the subject of this paper, is a widespread validation approach in industry, but it is still largely ad hoc, expensive, and unpredictably effective. Indeed, software testing is a broad term encompassing a variety of activities along the development cycle and beyond, aimed at different goals. Hence, software testing research faces a collection of challenges. A consistent roadmap of the most relevant challenges to be addressed is here proposed. In it, the starting point is constituted by some important past achievements, while the destination consists of four identified goals to which research ultimately tends, but which remain as unreachable as dreams. The routes from the achievements to the dreams are paved by the outstanding research challenges, which are discussed in the paper along with interesting ongoing work.},
 duplicado = {false},
 inserir = {false},
 title = {Software Testing Research: Achievements, Challenges, Dreams},
 year = {2007}
}

@article{144,
 abstract = {Software development is a data rich activity with many sophisticated metrics. Yet engineers often lack the tools and techniques necessary to leverage these potentially powerful information resources toward decision making. In this paper, we present the data and analysis needs of professional software engineers, which we identified among 110 developers and managers in a survey. We asked about their decision making process, their needs for artifacts and indicators, and scenarios in which they would use analytics. The survey responses lead us to propose several guidelines for analytics tools in software development including: Engineers do not necessarily have much expertise in data analysis; thus tools should be easy to use, fast, and produce concise output. Engineers have diverse analysis needs and consider most indicators to be important; thus tools should at the same time support many different types of artifacts and many indicators. In addition, engineers want to drill down into data based on time, organizational structure, and system architecture.},
 duplicado = {false},
 inserir = {false},
 title = {Information needs for software development analytics},
 year = {2012}
}

@article{145,
 abstract = {Abstract: Practitioners report that experience plays an important role in effective software testing. We investigate the role of experience in a multiple case study about three successful projects conducted at Siemens Austria and document the state of practice in testing software systems. The studied projects were employed from the domains telecommunications, insurance and banking, as well as safety-critical railway systems. The study shows that test design is to a considerable extent based on experience in all three projects and that experience-based testing is an important supplementary approach to requirements-based testing. The study further analyzes the different sources of experience, the perceived value of experience for testing, and the measures taken to manage and evolve this experience.},
 duplicado = {false},
 inserir = {false},
 title = {The Role of Experience in Software Testing Practice},
 year = {2008}
}

@article{147,
 abstract = {Abstract: Although the aim of empirical software engineering is to provide evidence for selecting the appropriate technology, it appears that there is a lack of recognition of this work in industry. Results from empirical research only rarely seem to find their way to company decision makers. If information relevant for software managers is provided in reports on experiments, such reports can be considered as a source of information for them when they are faced with making decisions about the selection of software engineering technologies. To bridge this communication gap between researchers and professionals, we propose characterizing the information needs of software managers in order to show empirical software engineering researchers which information is relevant for decision-making and thus enable them to make this information available. We empirically investigated decision makers information needs to identify which information they need to judge the appropriateness and impact of a software technology. We empirically developed a model that characterizes these needs. To ensure that researchers provide relevant information when reporting results from experiments, we extended existing reporting guidelines accordingly. We performed an experiment to evaluate our model with regard to its effectiveness. Software managers who read an experiment report according to the proposed model judged the technology's appropriateness significantly better than those reading a report about the same experiment that did not explicitly address their information needs. Our research shows that information regarding a technology, the context in which it is supposed to work, and most importantly, the impact of this technology on development costs and schedule as well as on product quality is crucial for decision makers.},
 duplicado = {false},
 inserir = {false},
 title = {Reporting experiments to satisfy professionals information needs},
 year = {2014}
}

@article{148,
 abstract = {More and more software development organisations are paying attention to the improvement of the software testing process, because it is considered a key factor to ensure the quality of software products. However, the staff usually has problems developing testing activities because they do not have the appropriate competences to carry out these activities effectively; this results in low performance in organisations and increased costs of software products. A way to reduce this gap is developing a competence model that defines the roles who participate in the software testing activities as well as the general and technical competences required for them. Therefore this model could be applied to train staff in software testing activities and to recruit the appropriate profiles, which contribute to improving their performance. Considering that there is no published competence model specifically addressed to software testing, this study presents one that has been developed, analysing the literature and testing jobs and validated by experts in the software testing field using a survey as a validation method. So, as a result of this work, a competence model for software testing close to the software industry has been obtained.},
 duplicado = {false},
 inserir = {false},
 title = {Design of a competence model for testing teams},
 year = {2012}
}

@article{149,
 abstract = {Abstract: In the development of many safety-critical systems, test cases are still created on the basis of experience rather than systematic methods. As a consequence, many redundant test cases are created and many aspects remain untested. One of the most important questions in testing dependable systems is: which are the right test techniques to obtain a test set that will detect critical errors in a complex system? In this paper, we provide an overview of the state-of-practice in designing test cases for dependable event-based systems regulated by the IEC 61508 and DO-178B standards. For example, the IEC 61508 standard stipulates modelbased testing and systematic test-case design and generation techniques such as transition-based testing and equivalence-class partitioning for software verification. However, it often remains unclear in which situation these techniques should be applied and what information is needed to select the right technique to obtain the best set of test cases. We propose an approach that selects appropriate test techniques by considering issues such as specification techniques, failure taxonomies and quality risks. We illustrate our findings with a case study for an interlocking system for Siemens transportation systems.},
 duplicado = {false},
 inserir = {},
 title = {Testing of Safety-Critical Systems a Structural Approach to Test Case Design},
 year = {2011}
}

@article{150,
 abstract = {Abstract: We propose a set of functional test patterns for testing a class of repository-style information systems. This class, which we call workflow-based, implements not only functions reading and writing from/to the database, but also workflows (business processes) that may be constrained by states of application objects. Patterns are based on models (functional abstractions) of the software product. The testing strategy combines testing of different models (hierarchy of functions, models of business processes and state/transition automata) using different testing techniques for each type of model. We discuss the patterns application to a large health care software product. We analyze the results (about 2700 test cases) and the effect of testing multiple patterns on the discovered failures and types of faults. We also describe the use of the method for testing about thirty software applications of an information system of a large retail company.},
 duplicado = {false},
 inserir = {false},
 title = {Functional abstractions for testing repository-style information systems},
 year = {2009}
}

@article{151,
 abstract = {Abstract: This position paper argues that fault classification provides vital information for software analytics, and that machine learning techniques such as clustering can be applied to learn a project- (or organization-) specific fault taxonomy. Anecdotal evidence of this position is presented as well as possible areas of research for moving toward the posited goal.},
 duplicado = {false},
 inserir = {false},
 title = {Toward a learned project-specific fault taxonomy: application of software analytics},
 year = {2015}
}

@article{152,
 abstract = {Objectives: To study on software testing pertaining to the trends, techniques, problems and challengers from 2006 to 2009 using systematic literature reviews (SLRs) approach. Method: We used the standard systematic literature review method employing a manual online search of 41 journals, Conference, Article and Books. Conclusions: Currently, the topic areas covered are trends, techniques, problems and challengers on software testing from 2006 to 2009.},
 duplicado = {false},
 inserir = {false},
 title = {Systematic literature reviews pertaining to the trends, techniques, problems, and challenges in software testing (2006-2009) A systematic literature review},
 year = {2015}
}

@article{154,
 abstract = {Abstract: System testing based on a black box approach is a common industrial practice in information systems. Despite its widespread use, however, little guidance is available for testing engineers facing the problem of selecting the best test strategy. In previous work, we proposed to adopt functional models and related testing patterns according to the architectural style of the application under test. In this paper, we present an industrial study that applies this technique to system testing of repository based applications. We define a set of functional models abstracting different concerns of software applications: hierarchy of functions, business processes and states/transitions of application objects. The models are used to derive the functional test cases through the definition of test patterns. We applied this approach in an industrial context for over 5 years. In this paper, we analyze a data set of 37 test projects including about 22000 test cases and 1500 failures. We relate failures to the originating defect types. The study confirms that a system test strategy that uses multiple functional models according to the architectural style of the software application generates a better cost/benefit ratio than the use of just one model. The explanation is that - despite a small overlap - each model detects specific types of software defects. The results of the study can guide testing engineers in selecting the best system test strategy and significantly improve the efficiency of their work.},
 duplicado = {false},
 inserir = {false},
 title = {System Testing of Repository-Style Software: An Experience Report},
 year = {2016}
}

@article{155,
 abstract = {Abstract: As software becomes more important to society, the number, age, and complexity of systems grow. Software organizations require continuous process improvement to maintain the reliability, security, and quality of these software systems. Software organizations can utilize data from manual fault classification to meet their process improvement needs, but organizations lack the expertise or resources to implement them correctly. This dissertation addresses the need for the automation of software fault classification. Validation results show that automated fault classification, as implemented in the MiSFIT tool, can group faults of similar nature. The resulting classifications result in good agreement for common software faults with no manual effort. To evaluate the method and tool, I develop and apply an extended change taxonomy to classify the source code changes that repaired software faults from an open source project. MiSFIT clusters the faults based on the changes. I manually inspect a random sample of faults from each cluster to validate the results. The automatically classified faults are used to analyze the evolution of a software application over seven major releases. The contributions of this dissertation are an extended change taxonomy for software fault analysis, a method to cluster faults by the syntax of the repair, empirical evidence that fault distribution varies according to the purpose of the module, and the identification of project-specific trends from the analysis of the changes.},
 duplicado = {false},
 inserir = {false},
 title = {MiSFIT: Mining Software Fault Information and Types},
 year = {2015}
}

@article{158,
 abstract = {Abstract: Any e-organization, whether it is commercial or governmental, requires a knowledge management support in order to achieve optimal performance. Many of the technologies that serve the operations of such organizations can also support knowledge management to facilitate efficient knowledge sharing and reuse. Thus, e-organizations should be at the forefront in the use of knowledge management. This paper examines systems of knowledge management used in large organizations. The limitations of traditional organizational schemes are examined, including the tie to the traditional pre-digital knowledge unit, the multi-page document. An action research approach is taken towards the question of how we improve upon traditional approaches using the technology available in conjunction with approaches arising from organizational research. A new framework is described where knowledge is packaged into objects and classified by organizational performance roles and goals. A prototype implementation of the framework was developed in order to test its feasibility. Evaluation of the prototype suggests that the system could result in a more intuitive organizational framework that enables workers to obtain appropriate knowledge support in a timely manner without the need for extensive search, and also facilitates greater reuse and sharing of knowledge.},
 duplicado = {false},
 inserir = {false},
 title = {An Object and Performance Framework for Implementation of Web-based Knowledge Sharing Technology},
 year = {2009}
}

@article{159,
 abstract = {This paper will highlight the global spread of usability expertise by presenting data on the location of usability testing centers and laboratories around the world. The possibilities for future expansion of the database and global networking of usability expertise and knowledge will be discussed.},
 duplicado = {false},
 inserir = {false},
 title = {Global mapping of usability labs and centers},
 year = {2009}
}

@article{160,
 abstract = {Concern with usability in the design of technology has developed rapidly over the last decade. Although this concern has been mainly located in the U.S. and Western Europe, there is evidence of a global spread, and moves to outsource this activity. As marketplaces become global, it is necessary to adapt testing procedures for different cultures. This paper will illustrate the global spread of usability expertise by presenting data collected from usability testing centers and laboratories around the world. It will discuss the possibilities for future expansion and global networking of usability expertise and knowledge.},
 duplicado = {false},
 inserir = {false},
 title = {Global spread of usability expertise},
 year = {2008}
}

@article{162,
 abstract = {Information from usability engineering activities can be useful in different contexts beyond the scope of a specific development project. This dissertation project aims at developing a model of usability information in order to support corresponding information needs of usability engineers in organizations with a usability information system. Results of interviews with usability professionals are presented, indicating that usability related information is already put to use in practice and that access to this information can be improved.},
 duplicado = {false},
 inserir = {false},
 title = {Sustainable management of usability information},
 year = {2011}
}

@article{165,
 abstract = {Software testing is a crucial phase of the software development lifecycle, responsible for assuring that the system under test meets quality standards, requirements, and consumer needs. Unfortunately, software testing is not without flaws. Some problems are timeless while others are brought on by new technologies and methodologies. As software systems grow in size and complexity, quality becomes significantly more difficult to ensure. With recent advancements in cloud computing, the internet's vast and elastic resources are available for testing. Testing as a Service (TaaS) offers accessible services that handle testing activities to consumers on a pay-as-you-test basis in hopes of providing a more efficient and effective way of guaranteeing software quality.
This thesis presents the top industry issues and concerns as identified through the Hard Problems in Software Testing survey, followed by a thorough overview of the current state of TaaS based on an exploration of existing commercial offerings and a survey of academic research. These problems are then examined to determine where TaaS can be applied to overcome the issue or offer improvements. The remaining shortcomings are analyzed to generate a roadmap for enhancing TaaS by addressing the hard problems plaguing the industry.
The evaluation of three existing tools against academic research and the hard problems indicated by the survey revealed a gap that must be overcome before TaaS can be fully embraced by the industry. While many of the industry concerns were reduced or eliminated by TaaS tools, a few still remain. These challenges appeared the most prominent in the areas of tester education and training, and a need for better tools, including issues such as incorporating fully-automated test case generation, offering greater compatibility and extensibility for external tools, promoting more types of testing, and enhanced security.},
 duplicado = {false},
 inserir = {false},
 title = {Overcoming Hard Problems in Software Testing with Testing as a Service},
 year = {2014}
}

@article{166,
 abstract = {Abstract
In the last 15 years, software architecture has emerged as an important software engineering field for managing the development and maintenance of large, software-intensive systems. Software architecture community has developed numerous methods, techniques, and tools to support the architecture process (analysis, design, and review). Historically, most advances in software architecture have been driven by talented people and industrial experience, but there is now a growing need to systematically gather empirical evidence about the advantages or otherwise of tools and methods rather than just rely on promotional anecdotes or rhetoric. The aim of this paper is to promote and facilitate the application of the empirical paradigm to software architecture. To this end, we describe the challenges and lessons learned when assessing software architecture research that used controlled experiments, replications, expert opinion, systematic literature reviews, observational studies, and surveys. Our research will support the emergence of a body of knowledge consisting of the more widely-accepted and well-formed software architecture theories.},
 duplicado = {false},
 inserir = {false},
 title = {Applying empirical software engineering to software architecture: challenges and lessons learned},
 year = {2010}
}

@article{167,
 abstract = {Abstract

Defining organization-specific process standards by integrating, harmonizing, and standardizing heterogeneous and often implicit processes is an important task, especially for large development organizations. On the one hand, such a standard must be generic enough to cover all of the organization's development activities; on the other hand, it must be as detailed and precise as possible to support employees' daily work. Today, organizations typically maintain and advance a plethora of individual processes, each addressing specific problems. This requires enormous effort, which could be spent more efficiently. This article introduces an approach for developing a Software Process Line that, similar to a Software Product Line, promises to reduce the complexity and thus, the effort required for managing the processes of a software organization. We propose Scoping, Modeling, and Architecting the Software Process Line as major steps, and describe in detail the Scoping approach we recommend, based on an analysis of the potential products to be produced in the future, the projects expected in the future, and the respective process capabilities needed. In addition, the article sketches experience from determining the scope of space process standards for satellite software development. Finally, it discusses the approach, draws conclusions, and gives an outlook on future work.},
 duplicado = {false},
 inserir = {false},
 title = {Scoping software process lines},
 year = {2009}
}

@article{168,
 abstract = {Abstract
The paper attempts to provide a comprehensive view of the field of software testing. The objective is to put all the relevant issues into a unified context, although admittedly the overview is biased towards my own research and expertise. In view of the vastness of the field, for each topic problems and approaches are only briefly tackled, with appropriate references provided to dive into them. I do not mean to give here a complete survey of software testing. Rather I intend to show how an unwieldy mix of theoretical and technical problems challenge software testers, and that a large gap exists between the state of the art and of the practice.},
 duplicado = {false},
 inserir = {false},
 title = {Software Testing Research and Practice},
 year = {2003}
}

@article{169,
 abstract = {Abstract. Minimizing risks in the development and introduction of new technologies
in the software industry can be considered an important factor to evolve software
engineering as science. The using of evidence provided by experimental studies
allows the characterization of a technology before its using in software projects in
the industry. Thus, it is possible to determine with reasonable precision the
feasibility in to use a software technology in specific scenarios. Based on this
context, this paper presents an approach that uses secondary and primary studies to
support the conception of new software technologies. Its applicability is evaluated
by the description of two experience cases. As consequence, some discussions are
presented regarding the difficulties, risks, and benefits in the using of the proposed
approach.},
 duplicado = {false},
 inserir = {false},
 title = {Abordagem para Desenvolver Tecnologia de Software com Apoio de Estudos Secundarios e Primarios},
 year = {2008}
}

@article{170,
 abstract = {Abstract
Selecting software technologies for software projects represents a challenge to software engineers. It is known that software projects differ from each other by presenting different characteristics that can complicate the selection of such technologies. This is not different when considering model-based testing. There are many approaches with different characteristics described in the technical literature that can be used in software projects. However, there is no indication as to how they can fit a software project. Therefore, a strategy to select model-based testing approaches for software projects called Porantim is fully described in this paper. Porantim is based on a body of knowledge describing model-based testing approaches and their characterization attributes (identified by secondary and primary experimental studies), and a process to guide by adequacy and impact criteria regarding the use of this sort of software technology that can be used by software engineers to select model-based testing approaches for software projects.},
 duplicado = {false},
 inserir = {false},
 title = {Model-based testing approaches selection for software projects},
 year = {2009}
}

@article{171,
 abstract = {Abstract
Model-Based Testing (MBT) represents a feasible and interesting testing strategy where test cases are generated from formal models describing the software behavior/structure. The MBT field is continuously evolving, as it could be observed in the increasing number of MBT techniques published at the technical literature. However, there is still a gap between researches regarding MBT and its application in the software industry, mainly occasioned by the lack of information regarding the concepts, available techniques, and challenges in using this testing strategy in real software projects. This chapter presents information intended to support researchers and practitioners reducing this gap, consequently contributing to the transfer of this technology from the academia to the industry. It includes information regarding the concepts of MBT, characterization of 219 MBT available techniques, approaches supporting the selection of MBT techniques for software projects, risk factors that may influence the use of these techniques in the industry together with some mechanisms to mitigate their impact, and future perspectives regarding the MBT field.},
 duplicado = {false},
 inserir = {false},
 title = {A Picture from the Model-Based Testing Area: Concepts, Techniques, and Challenges},
 year = {2010}
}

@article{172,
 abstract = {Abstract:
Software testing is expensive for the industry, and always constrained by time and effort. Although there is a multitude of test techniques, there are currently no scientifically based guidelines for the selection of appropriate techniques of different domains and contexts. For large complex systems, some techniques are more efficient in finding failures than others and some are easier to apply than others are. From an industrial perspective, it is important to find the most effective and efficient test design technique that is possible to automate and apply. In this paper, we propose an experimental framework for comparison of test techniques with respect to efficiency, effectiveness and applicability. We also plan to evaluate ease of automation, which has not been addressed by previous studies. We highlight some of the problems of evaluating or comparing test techniques in an objective manner. We describe our planned process for this multi-phase experimental study. This includes presentation of some of the important measurements to be collected with the dual goals of analyzing the properties of the test technique, as well as validating our experimental framework},
 duplicado = {false},
 inserir = {false},
 title = {A Framework for Comparing Efficiency, Effectiveness and Applicability of Software Testing Techniques},
 year = {2006}
}

@article{173,
 abstract = {Abstract
Defining process standards by integrating, harmonizing, and standardizing heterogeneous and often implicit processes is an important task, especially for large development organizations. However, many challenges exist, such as limiting the scope of process standards, coping with different levels of process model abstraction, and identifying relevant process variabilities to be included in the standard. On the one hand, eliminating process variability by building more abstract models with higher degrees of interpretation has many disadvantages, such as less control over the process. Integrating all kinds of variability, on the other hand, leads to high process deployment costs. This article describes requirements and concepts for determining the scope of process standards based on a characterization of the potential productzs to be produced in the future, the projects expected for the future, and the respective process capabilities needed. In addition, the article sketches experience from determining the scope of space process standards for satellite software development. Finally, related work with respect to process model scoping, conclusions, and an outlook on future work are presented.},
 duplicado = {false},
 inserir = {false},
 title = {Scoping Software Process Models - Initial Concepts and Experience from Defining Space Standards},
 year = {2008}
}

@article{174,
 abstract = {Abstract:
Classification makes a significant contribution to advancing knowledge in both science and engineering. It is a way of investigating the relationships between the objects to be classified and identifies gaps in knowledge. Classification in engineering also has a practical application; it supports object selection. They can help mature software engineering knowledge, as classifications constitute an organized structure of knowledge items. Till date, there have been few attempts at classifying in software engineering. In this research, we examine how useful classifications in software engineering are for advancing knowledge by trying to classify testing techniques. The paper presents a preliminary classification of a set of unit testing techniques. To obtain this classification, we enacted a generic process for developing useful software engineering classifications. The proposed classification has been proven useful for maturing knowledge about testing techniques, and therefore, SE, as it helps to: 1) provide a systematic description of the techniques, 2) understand testing techniques by studying the relationships among techniques (measured in terms of differences and similarities), 3) identify potentially useful techniques that do not yet exist by analyzing gaps in the classification, and 4) support practitioners in testing technique selection by matching technique characteristics to project characteristics.},
 duplicado = {false},
 inserir = {true},
 title = {Maturing Software Engineering Knowledge through Classifications: A Case Study on Unit Testing Techniques},
 year = {2009}
}

@article{175,
 abstract = {Abstract:
The rapid development of verification and validation (V&V) has resulted in a multitude of V&V technologies, making V&V selection difficult for practitioners. Since most V&V technologies will be combined it is important to be aware of how they should be combined and the cost-effectiveness of these combinations. This paper presents a strategy for selecting and evaluating particular V&V combinations that focuses on maximising completeness and minimising effort. The strategy includes a systematic approach for applying empirical information regarding the costs and capabilities of V&V technologies.},
 duplicado = {false},
 inserir = {false},
 title = {An Iterative Empirical Strategy for the Systematic Selection of a Combination of Verification and Validation Technologies},
 year = {2007}
}

@article{176,
 abstract = {Abstract. To make software technologies available to support software development
process with quality in the industry represents one of the most important goals in
software engineering. However, the mere providing of the software technology is
not enough. It is necessary to produce evidence on the software technology
feasibility and applicability in the different development contexts. Therefore, for a
technology to be made available it is necessary that it goes through three stages:
conception, construction and evaluation. Approaches to construct and evaluate
software technologies are easily found in the technical literature. However, this is
not the case with the conception stage. This paper describes an approach concerned
with the conception of software technologies based on evidence collected from
secondary and primary studies to support their development. The types of study and
decision points are presented to allow their evaluation and application by software
engineers.},
 duplicado = {false},
 inserir = {false},
 title = {Developing Software Technologies through Experimentation: Experiences from the Battlefield },
 year = {2010}
}

@article{178,
 abstract = {Abstract
Software testing techniques differ in the type of faults they are more prone to detect, and their performance varies depending on the features of the application being tested. Practitioners often use informally their knowledge about the software under test in order to combine testing techniques for maximizing the number of detected faults.

This work presents an approach to enable practitioners to select testing techniques according to the features of the software to test. A method to build a testing-related base of knowledge for tailoring the techniques selection process to the specific application(s) is proposed. The method grounds upon two basic steps: (i) constructing, on an empirical basis, models to characterize the software to test in terms of fault types it is more prone to contain; (ii) characterizing testing techniques with respect to fault types they are more prone to detect in the given context.
Using the created base of knowledge, engineers within an organization can define the mix of techniques so as to maximize the effectiveness of the testing process for their specific software.

Highlights

? We address the issue of techniques selection in software testing process. ? We define a method to select testing techniques according to software features. ? The method grounds upon two steps: software, and technique characterization. ? We show how to apply the method in practice, using real-world software applications.},
 duplicado = {false},
 inserir = {false},
 title = {Testing techniques selection based on ODC fault types and software metrics},
 year = {2013}
}

@article{179,
 abstract = {Abstract:
One of the major problems within the software testing area is how to get a suitable set of test cases to test a software system. This set should assure maximum effectiveness with the least possible number of test cases. There are nowadays numerous testing techniques available for generating test cases. However, many of them are never used, while a few are used over and over again. Testers have little (if any) information about the available techniques, their usefulness and, generally, how suited they are to the project at hand. This lack of information means less tuned decisions on which testing techniques to use. This paper presents the results of developing an artefact (called a characterisation schema) to assist with testing technique selection. When instantiated for a variety of techniques, the schema provides developers with a catalogue containing enough information for them to select the best suited techniques for a given project. The schema, and its associated catalogue, assure that the decisions developers make are based on grounded knowledge of the techniques rather than on perceptions, suppositions and assumptions.},
 duplicado = {false},
 inserir = {true},
 title = {Identifying the relevant information for software testing technique selection},
 year = {2004}
}

@article{18,
 abstract = {Abstract: Software organisations have been experiencing software development failures since the start of software development. These failures include among others abandoned and runaway projects, cost overruns, and low quality software. One of the major causes of software development failure is the inability of software organisations to learn from past mistakes. Many intervention strategies have been tried by software organisations to address this issue. Such strategies include software process improvement (SPI) models such as the Capability Maturity Model Integration (CMMI) and new software development methodologies such as agile methods. These intervention strategies don't seem to be working because software development projects continue to fail. Current statistics from surveys indicate that less than 50% of projects are successful. To dress this issue, today, software organisations are turning to knowledge management. This is because the software development process is a knowledge intensive task. Knowledge management is a series of processes that seek to acquire, create, capture and store, transfer and apply knowledge to organisational routines and processes. It aims to make knowledge available to the right people and processes at the right times in the right presentation for the right cost. Knowledge management is believed to promote learning in software organisations so that they become learning software organisations. In software organisations, knowledge workers use the captured knowledge in their daily tasks thus learning from it. Organisational learning increases efficiency and prevents past mistakes from happening in the future. This paper presents a theoretical framework that shows how knowledge management facilitates learning of individuals, teams and the organisation. Specifically, it argues that knowledge management plays a vital role in facilitating learning in software organisations.},
 duplicado = {false},
 inserir = {false},
 title = {The Role of Knowledge Management in Facilitating Learning in Software Organisations},
 year = {2015}
}

@article{180,
 abstract = {Abstract
Testing is the dominating method for quality assurance of industrial software. Despite its importance and the vast amount of resources invested, there are surprisingly limited efforts spent on testing research, and the few industrially applicable results that emerge are rarely adopted by industry. At the same time, the software industry is in dire need of better support for testing its software within the limited time available.

Our aim is to provide a better understanding of how test cases are created and applied, and what factors really impact the quality of the actual test. The plethora of test design techniques (TDTs) available makes decisions on how to test a difficult choice. Which techniques should be chosen and where in the software should they be applied? Are there any particular benefits of using a specific TDT? Which techniques are effective? Which can you automate? What is the most beneficial way to do a systematic test of a system?

This thesis attempts to answer some of these questions by providing a set of guidelines for test design, including concrete suggestions for how to improve testing of industrial software systems, thereby contributing to an improved overall system quality. The guidelines are based on ten studies on the understanding and use of TDTs. The studies have been performed in a variety of system domains and consider several different aspects of software test. For example, we have investigated some of the common mistakes in creating test cases that can lead to poor and costly testing. We have also compared the effectiveness of different TDTs for different types of systems. One of the key factors for these comparisons is a profound understanding of faults and their propagation in different systems. Furthermore, we introduce a taxonomy for TDTs based on their effectiveness (fault finding ability), efficiency (fault finding rate), and applicability. Our goal is to provide an improved basis for making well-founded decisions regarding software testing, together with a better understanding of the complex process of test design and test case writing. Our guidelines are expected to lead to improvements in testing of complex industrial software, as well as to higher product quality and shorter time to market.},
 duplicado = {false},
 inserir = {false},
 title = {On Test Design},
 year = {2009}
}

@article{181,
 abstract = {Research in verification and validation (V&V) for concurrent programs can be guided by practitioner information. A survey was therefore run to gain state-of-practice information in this context. The survey presented in this paper collected state-of-practice information on V&V technology in concurrency from 35 respondents. The results of the survey can help refine existing V&V technology by providing a better understanding of the context of V&V technology usage. Responses to questions regarding the motivation for selecting V&V technologies can help refine a systematic approach to V&V technology selection.},
 duplicado = {false},
 inserir = {false},
 title = {A state-of-practice questionnaire on verification and validation for concurrent programs},
 year = {2006}
}

@article{183,
 abstract = {This paper describes the third round of the Java Unit Testing Tool Competition. This edition of the contest evaluates no less than seven automated testing tools! And, like during the second round, test suites written by human testers are also used for comparison. This paper contains the full results of the evaluation.},
 duplicado = {false},
 inserir = {false},
 title = {Unit testing tool competition: round three},
 year = {2015}
}

@article{184,
 abstract = {Abstract
One major issue in managing software engineering knowledge is the construction of information repositories for software development artifacts (techniques, products, processes, tools, and so on). But how does one package each artifact so that the package contains the appropriate information to understand and use the artifact? What is the appropriate characterization schema? This chapter proposes an empirical and iterative process to identify the information that should be used to characterize a software engineering artifact, using theoretical knowledge, practical experience, and expert opinion to generate a schema. The ultimate goal is to improve the schema and the package contents based upon it experience in their application. The proposed process has been applied to define a characterization schema for testing techniques. Nowadays, there are numerous testing techniques available for generating test cases. However, many of them are never used, while a few are used over and over again. Testers have little (if any) information about the available techniques, their usefulness and, generally, how suited they are to the project at hand. This lack of information means less appropriate decisions on which testing techniques to use. This chapter also shows this characterization schema and discusses the information it contains and why it is included in the schema.},
 duplicado = {false},
 inserir = {true},
 title = {A Process for Identifying Relevant Information for a Repository: A Case Study for Testing Techniques},
 year = {2003}
}

@article{185,
 abstract = {Abstract:
There exists a real need in industry to have guidelines on what testing techniques use for different testing objectives, and how usable (effective, efficient, satisfactory) these techniques are. Up to date, these guidelines do not exist. Such guidelines could be obtained by doing secondary studies on a body of evidence consisting of case studies evaluating and comparing testing techniques and tools. However, such a body of evidence is also lacking. In this paper, we will make a first step towards creating such body of evidence by defining a general methodological evaluation framework that can simplify the design of case studies for comparing software testing tools, and make the results more precise, reliable, and easy to compare. Using this framework, (1) software testing practitioners can more easily define case studies through an instantiation of the framework, (2) results can be better compared since they are all executed according to a similar design, (3) the gap in existing work on methodological evaluation frameworks will be narrowed, and (4) a body of evidence will be initiated. By means of validating the framework, we will present successful applications of this methodological framework to various case studies for evaluating testing tools in an industrial environment with real objects and real subjects.},
 duplicado = {false},
 inserir = {false},
 title = {A Methodological Framework for Evaluating Software Testing Techniques and Tools},
 year = {2012}
}

@article{186,
 abstract = {Abstract:
The selection of software technologies represents a risk factor to a software project. Therefore, using tailored software technologies to support this task can contribute to reduce the risk of inadequate choices made by software engineers. This paper presents the results of an experimental study conducted to evaluate if three dependent variables (selection time, % of correct choices, and used information) can be significantly affected by two different approaches to support the selection of Model-Based Testing techniques. This study consists of an external replication of an experiment conducted to evaluate two approaches for the selection of testing techniques. Therefore, we present the adaptations performed in the planning and design of the experiment and the challenges observed in conducting a study replication. The results indicated that the time spent to select MBT techniques and the percentage of right selections can be affected by the approach used to select MBT techniques for different software project categories.},
 duplicado = {false},
 inserir = {false},
 title = {Evaluation of {model-based} testing techniques selection approaches: An external replication},
 year = {2009}
}

@article{188,
 abstract = {Software technologies, such as model-based testing approaches, have specific characteristics and limitations that can affect their use in software projects. To make available knowledge regarding such technologies is important to support the decision regarding their use in software projects. In particular, a choice of model-based testing approach can influence testing success or failure. Therefore, this paper aims at describing knowledge acquired from a systematic review regarding model-based testing approaches and proposing an infrastructure towards supporting their selection for software projects.},
 duplicado = {false},
 inserir = {false},
 title = {Supporting the selection of model-based testing approaches for software projects},
 year = {2008}
}

@article{189,
 abstract = {Abstract:
This paper describes the Java Unit Testing Tool Competition that ran in the context of the Search Based Software Testing (SBST) workshop at ICST 2013. It describes the main objective of the benchmark, the Java classes that were selected, the data that was collected, the tools that were used for data collection, the protocol that was carried out to execute the benchmark and how the final benchmark score for each participating tool can be calculated.},
 duplicado = {false},
 inserir = {false},
 title = {Unit Testing Tool Competition},
 year = {2015}
}

@article{190,
 abstract = {Abstract
The success of a measurement initiative in a software company depends on the quality of the links between metrics programs and organizational business goals. GQM+Strategies is a new approach designed to help in establishing links between the organizational business goals and measurement programs. However, there are no reported industrial experiences that demonstrate the practical value of the method. We designed a five-step research approach in order to assess the method's practical value. The approach utilized revised Bloom's taxonomy as a framework for assessing the practitioners cognition level of the concepts. The results of our study demonstrated that the method has practical value for the case company, and it is capable of addressing real-world problems. In the conducted empirical study, the cognition level of the practitioners was sufficient for a successful implementation of the method.},
 duplicado = {false},
 inserir = {false},
 title = {Early Empirical Assessment of the Practical Value of GQM+Strategies},
 year = {2010}
}

@article{191,
 abstract = {Abstract
This paper reports about the two rounds of the Java Unit Testing Tool Competition that ran in the context of the Search Based Software Testing (SBST) workshop at ICST 2013 and the first Future Internet Testing (FITTEST) workshop at ICTSS 2013. It describes the main objectives of the benchmark, the Java classes that were selected in both competitions, the data that was collected, the tools that were used for data collection, the protocol that was carried out to execute the benchmark and how the final benchmark scores for each participating tool were calculated. Eventually, we discuss the challenges encountered during the events, what we learned and how we plan to improve our framework for future competitions.},
 duplicado = {false},
 inserir = {false},
 title = {Unit Testing Tool Competitions Lessons Learned},
 year = {2013}
}

@article{192,
 abstract = {Abstract
Defect taxonomies collect and organize the domain knowledge and project experience of experts and are a valuable instrument of system testing for several reasons. They provide systematic backup for the design of tests, support decisions for the allocation of testing resources and are a suitable basis for measuring the product and test quality. In this paper, we propose a method of system testing based on defect taxonomies and investigate how these can systematically improve the efficiency and effectiveness, i.e. the maturity of requirements-based testing. The method is evaluated via an industrial case study based on two projects from a public health insurance institution by comparing one project with defect taxonomy-supported testing and one without. Empirical data confirm that system testing supported by defect taxonomies (1) reduces the number of test cases, and (2) increases of the number of identified failures per test case.},
 duplicado = {false},
 inserir = {true},
 title = {Using Defect Taxonomies to Improve the Maturity of the System Test Process: Results from an Industrial Case Study},
 year = {2013}
}

@article{193,
 abstract = {Abstract
Context

Software products have requirements on software quality attributes such as safety and performance. Development teams use various specific techniques to achieve these quality requirements. We call these Quality Attribute Techniques (QATs). QATs are used to identify, analyse and control potential product quality problems. Although QATs are widely used in practice, there is no systematic approach to represent, select, and integrate them in existing approaches to software process modelling and tailoring.

Objective

This research aims to provide a systematic approach to better select and integrate QATs into tailored software process models for projects that develop products with specific product quality requirements.

Method

A selection method is developed to support the choice of appropriate techniques for any quality attribute, across the lifecycle. The selection method is based on three perspectives: (1) risk management; (2) process integration; and (3) cost/benefit using Analytic Hierarchy Process (AHP). An industry case study is used to validate the feasibility and effectiveness of applying the selection method.

Results

The case study demonstrates that the selection method provides a more methodological and effective approach to choose QATs for projects that target a specific quality attribute, compared to the ad hoc selection performed by development teams.

Conclusion

The proposed selection method can be used to systematically choose QATs for projects to target specific product qualities throughout the software development lifecycle.},
 duplicado = {false},
 inserir = {false},
 title = {Applying a selection method to choose Quality Attribute Techniques},
 year = {2013}
}

@article{194,
 abstract = {Empirical Software Engineering research has achieved considerable results in building our knowledge about selecting and applying appropriate empirical methods for technology evaluation. Empirical studies in general and empirical studies in industrial settings in particular have played an important role in successful transition of many Software Engineering technologies to industry, for example, defect detection techniques and automated test cases. However, conducting empirical research in industrial settings remains a challenging undertaking for a variety of reasons. There is no substantial literature reporting on the challenges and complexities involved in conducting empirical studies in an industry in general and in settings whose business models are built around global sourcing. This paper reports some of our experiences and lessons learned from conducting empirical research in industry. Some of the observed challenges include short time horizon for research, high expectations, limited research skills, and the 'acceptable' research rigor. The paper discusses some of these issues with relevant examples and provides some strategies for overcoming these issues. We also stress that researchers and practitioners should share their experiences of conducting empirical research in order to help build a body of knowledge to guide the future efforts.},
 duplicado = {false},
 inserir = {false},
 title = {Conducting empirical studies in industry: balancing rigor and relevance},
 year = {2013}
}

@article{195,
 abstract = {This work presents a method to combine testing techniques adaptively during the testing process. It intends to mitigate the sources of uncertainty of software testing processes, by learning from past experience and, at the same time, adapting the technique selection to the current testing session. The method is based on machine learning strategies. It uses offline strategies to take historical information into account about the techniques performance collected in past testing sessions; then, online strategies are used to adapt the selection of test cases to the data observed as the testing proceeds. Experimental results show that techniques performance can be accurately characterized from features of the past testing sessions, by means of machine learning algorithms, and that integrating this result into the online algorithm allows improving the fault detection effectiveness with respect to single testing techniques, as well as to their random combination.},
 duplicado = {false},
 inserir = {true},
 title = {A learning-based method for combining testing techniques},
 year = {2013}
}

@article{196,
 abstract = {Abstract:
Numerous software verification and validation (V&V) techniques and tools exist to analyse requirements, designs and implementations of software systems. These V&V technologies range from relatively lightweight ones, such as inspection and testing, to more heavyweight technologies based on formal methods and theorem proving. For complex systems, a significant part of the cost and effort for development and maintenance is associated with V&V activities, and this almost always involves selecting and applying a mix of V&V technologies. Unfortunately, little is known about the cost-effectiveness of individual technologies or how to derive the most cost-effective combination. As such, combinations for particular projects are typically selected in an ad-hoc way. In this paper, several issues related to the selection and evaluation of combinations of V&V technologies are explored, based on personal experiences with the V&V of concurrent Java components. A V&V method is presented that was systematically derived through an analysis of the possible failures that can occur in concurrent Java components. This method combines inspection, static analysis, and dynamic testing. In addition, empirical methods that use analysis of fault data and experiments to evaluate V&V combinations are presented. Finally, ideas are presented for an iterative method that can be used to assist with both the selection and evaluation of cost-effective V&V combinations in a given context.},
 duplicado = {false},
 inserir = {false},
 title = {Selecting V&V Technology Combinations: How to Pick a Winner?},
 year = {2003}
}

@article{197,
 abstract = {There are many different approaches to testing software, with different benefits for software quality and the development process. Yet, it is not well understood what developers struggle with when getting started with testing - and why some do not test at all or not as much as would be good for their project. This missing understanding keeps us from improving processes and tools to help novices adopt proper testing practices. We conducted a qualitative study with 97 computer science students. Through interviews, we explored their experiences and attitudes regarding testing in a collaborative software project. We found enabling and inhibiting factors for testing activities, the different testing strategies they used, and novices perceptions and attitudes of testing. Students push test automation to the end of the project, thus robbing themselves from the advantages of having a test suite during implementation. Students were not convinced of the return of investment in automated tests and opted for laborious manual tests - which they often regretted in the end. Understanding such challenges and opportunities novices face when confronted with adopting testing can help us improve testing processes, company policies, and tools. Our findings provide recommendations that can enable organizations to facilitate the adoption of testing practices by their members.},
 duplicado = {false},
 inserir = {false},
 title = {Enablers, inhibitors, and perceptions of testing in novice software teams},
 year = {2014}
}

@article{198,
 abstract = {Abstract:
The technical literature on model-based testing (MBT) offers us several techniques with different characteristics and goals. Contemporary software projects usually need to make use of different software testing techniques. However, a lack of empirical information regarding their scalability and effectiveness is observed. It makes their application difficult in real projects, increasing the technical difficulties to combine two or more MBT techniques for the same software project. In addition, current software testing selection approaches offer limited support for the combined selection of techniques. Therefore, this paper describes the conception and evaluation of an approach aimed at supporting the combined selection of MBT techniques for software projects. It consists of an evidence-based body of knowledge with 219 MBT techniques and their corresponding characteristics and a selection process that provides indicators on the level of adequacy (impact indicator) amongst MBT techniques and software projects characteristics. Results from the data analysis indicate it contributes to improve the effectiveness and efficiency of the selection process when compared to another selection approach available in the technical literature. Aiming at facilitating its use, a computerized infrastructure, evaluated into an industrial context and evolved to implement all the facilities needed to support such selection approach, is presented.},
 duplicado = {false},
 inserir = {false},
 title = {Supporting the Combined Selection of Model-Based Testing Techniques},
 year = {2014}
}

@article{199,
 abstract = {Abstract:
Defect taxonomies collect and organize the domain knowledge and project experience of experts and are a valuable instrument of system testing for several reasons. They provide systematic backup for the design of tests, support decisions for the allocation of testing resources and provide a suitable basis for measuring the product and test quality. In this paper, we present a method of system testing based on defect taxonomies and an appropriate estimation procedure for its return on investment depending on several parameters like the average test design time or the number of test cycles and experience values of a test organization. The estimated return on investment provides decision support whether to apply defect taxonomy supported system testing for a specific product or not. We develop the estimation procedure in the context of an industrial project from a public health insurance institution where the return on investment was positive after the first test cycle. From the experience of this project we extract guidelines and heuristics for precise estimation and interpretation of the return on investment of defect taxonomy supported system testing in the context of other projects.},
 duplicado = {false},
 inserir = {true},
 title = {Estimating the Return on Investment of Defect Taxonomy Supported System Testing in Industrial Projects},
 year = {2012}
}

@article{2,
 abstract = {Abstract: A mapping study provides a broad overview of a research area in order to determine whether there is research evidence on a particular topic. Results of a systematic mapping may identify suitable areas for performing future research. In this paper, we discuss our experience in using the findings of a mapping study on Knowledge Management (KM) in Software Testing for performing a real research project, which also applied other empirical approaches. The main goals of this paper are: (i) to reinforce the importance of a systematic mapping in the conduction of a research project by discussing a real case of such application, and (ii) to present the results of our survey on the most important aspects of KM when applied to software testing.},
 duplicado = {false},
 inserir = {true},
 title = {Using the Findings of a Mapping Study to Conduct a Research Project: A Case in Knowledge Management in Software Testing},
 year = {2015}
}

@article{20,
 abstract = {Background: Software organisations have been experiencing software development failures since the beginning of software development. Globally, latest Standish Group CHAOS reports indicate that only 29% of projects are successful. In South Africa (SA), the ITWeb report (2013) indicates that only 11% of all projects are successful. Intervention strategies such as software process improvement (SPI) frameworks and new software development methodologies such as agile methods have been introduced to address this issue. These intervention strategies do not seem to be effective because software development projects continue to fail. To address this issue, software organisations are turning to knowledge management (KM). This is because software development is a knowledge-intensive task. Objectives: The study aimed to investigate KM practices in small, medium and micro (SMMEs) software development organisations in SA and to determine if KM has benefited the organisations. Method: Fifteen software development project managers from 15 software development SMMEs were interviewed. Content analysis was used to analyse the data. Results: The study found six KM practices in the organisations: knowledge acquisition, creation, storage, sharing, organisation and application. The study found that KM has benefited organisations by making them effective, efficient and productive. Conclusions: The study concluded that software development organisation have adopted KM, although informally, and that KM practices have improved organisational routines and processes.},
 duplicado = {false},
 inserir = {false},
 title = {Knowledge management in small software development organisations : a South African perspective},
 year = {2017}
}

@article{200,
 abstract = {The lack of information limits component consumers to understand candidate components sufficiently in a way they can check if a given component fulfills its goal. Thus, this paper presents an approach to support component testing aiming to reduce the lack of information between component producers and component consumers. Additionally, the approach is covered by a CASE tool integrated in the development environment. An experimental study was performed in order to evaluate its efficiency and difficulties of its use. The experimental study indicates that the approach is viable and the tool support provides effort reduction to component producers and component consumers.},
 duplicado = {false},
 inserir = {false},
 title = {An approach for component testing and its empirical validation},
 year = {2009}
}

@article{201,
 abstract = {Abstract:
The combination of testing techniques is considered an effective strategy to evaluate a software product. However, the selection of which techniques to combine in a software project has been an interesting challenge in the Software Engineering field. This paper presents a proposal extending an approach developed to support the combined selection of model-based testing (MBT) techniques, named Porantim, applying Multiobjective Combinatorial Optimization strategy by determining the smallest dominating set in a bipartite and weighted graph. Thus, a local search strategy algorithm is proposed generating solutions aiming at maximizing the coverage of software project characteristics and skills required by the testing team to use the techniques and minimizing the eventual effort to construct models used for test cases generation. A preliminary evaluation analyzes this new approach when compared to the Porantim's original version, and the results indicate improvements in the MBT techniques selection.},
 duplicado = {false},
 inserir = {false},
 title = {Porantim-Opt: Optimizing the Combined Selection of Model-Based Testing Techniques},
 year = {2011}
}

@article{202,
 abstract = {Abstract: We study software testing as a distributed component of the software life cycle, developing a technique for estimating testing validity. The goal of this paper is to build measures for testing result estimation, to find how tested properties influence software quality, and to develop means (methodology and algorithms) to achieve better results in testing. To achieve these goals, we suggest using system approach to testing developed in this paper and introduce a system of test efficiency measures, as well as discuss a methodology of how to use these measures. Properties of test efficiency measures oriented at the software engineering domain are studied. We also show (Proposition 1, Theorem 1 and their Corollaries) how logic can increase testing efficiency.},
 duplicado = {false},
 inserir = {false},
 title = {Measuring testing as a distributed component of the software life cycle},
 year = {2009}
}

@article{204,
 abstract = {Abstract
Having software processes that fit technological, project, and business demands is one important prerequisite for software-developing organizations to operate successfully in a sustainable way. However, many such organizations suffer from processes that do not fit their demands, either because they do not provide the necessary support, or because they provide features that are no longer necessary. This leads to unnecessary costs during the development cycle, a phenomenon that worsens over time. This paper presents the SCOPE approach for systematically determining the process demands of current and future products and projects, for analyzing existing processes aimed at satisfying these demands, and for subsequently selecting those processes that provide the most benefit for the organization. The validation showed that SCOPE is capable of adjusting an organization's process scope in such a way that the most suitable processes are kept and the least suitable ones can be discarded.},
 duplicado = {false},
 inserir = {false},
 title = {Determining Organization-Specific Process Suitability},
 year = {2010}
}

@article{205,
 abstract = {This dissertation presents an empirical approach for building and storing knowledge about software engineering through human-subject research. It is based on running empirical studies in stages, where previously held hypotheses are supported or refuted in different contexts, and new hypotheses are generated. The approach is both mixed-methods based and opportunistic, and focuses on identifying a diverse set of potential sources for running studies. The output produced is an experience base which contains a set of these hypotheses, the empirical evidence which generated them, and the implications for practitioners and researchers. This experience base is contained in a software system which can be navigated by stakeholders to trace the chain of evidence of hypotheses as they evolve over time and across studies. This approach has been applied to the domain of high-end computing, to build knowledge related to programmer productivity. The methods include controlled experiments and quasi-experiments, case studies, observational studies, interviews, surveys, and focus groups. The results of these studies have been stored in a proof-of-concept system that implements the experience base.},
 duplicado = {false},
 inserir = {false},
 title = {Development of An Empirical Approach to Building Domain-Specific Knowledge Applied to High-End Computing},
 year = {2006}
}

@article{206,
 abstract = {Abstract. This paper describes a computerized infrastructure to support the
combined selection of Model-based Testing (MBT) Techniques to be jointly
used for a software project. The decision making can be supported by a
selection approach providing the generation of indicators and charts which MBT
techniques could be considered regarding the software project's characteristics
and requirements. Results of an observational study performed with software
engineers suggest the proposed infrastructure could be able to guide the
selection of the best-suited combination of MBT techniques for a software
project, can reduce about 50% of the selection time in comparison with manual
selection, and can approximate the time spent by software engineers with
different experience backgrounds while performing the MBT selection task.},
 duplicado = {false},
 inserir = {false},
 title = {Evolving a Computerized Infrastructure to support the Selection of Model-Based Testing Techniques},
 year = {2010}
}

@article{207,
 abstract = {Abstract:
Conducting verification and validation (V&V) of modeling and simulation (M&S) requires systematic and structured application of different V&V techniques throughout the M&S life cycle. Whether an existing technique is appropriate to a particular V&V activity depends not only on the characteristics of the technique but also on the situation where it will be applied. Although there already exit several guidance documents describing a variety of V&V techniques and their application potential, accessible findings or experiences on the effective selection of suitable V&V techniques for a given M&S context are still lacking. This paper presents: (1.) a characterization approach to developing a V&V techniques catalog that packages the available techniques together with the information about their application conditions; and (2.) a planning and tailoring strategy for project-specific selection of the appropriate V&V techniques from the established catalog according to the goals and characteristics of a simulation study.},
 duplicado = {false},
 inserir = {false},
 title = {Selecting verification and validation techniques for simulation projects: A planning and tailoring strategy},
 year = {2013}
}

@article{208,
 abstract = {Abstract
Software testing is a very delicate aspect of software development, since designing good test sets is a non-trivial task. In this article, we describe a testing technique for testing business rules using property-based testing and the property-based automatic testing tool QuickCheck. Systematic, effective, and efficient testing of business rules increases the confidence on the validation of business concepts and domain rules which are specifically critical to data consistency. The approach is presented on the basis of small but representative examples in order to facilitate the readers understanding, but it has been successfully evaluated in a number of different industrial examples, demonstrating that it generalises to much larger systems and is, thus, broadly applicable.},
 duplicado = {false},
 inserir = {false},
 title = {Advanced management of data integrity: property-based testing for business rules},
 year = {2015}
}

@article{209,
 abstract = {Abstract:
Different quality assurance techniques can be applied in the software development lifecycle to improve the quality of a product. Today, no holistic approach for planning and customizing inspection and testing techniques in a given context exists. In this paper, we present a framework which consists of the main aspects necessary for a systematic planning of quality assurance activities in different development steps. We present the core elements of the framework: influence and variation factors, characteristics of techniques and a role concept. The framework is a first step towards a systematic planning approach for quality assurance, which has to be refined in future research. Based on the elements, we recommend an initial process how to apply the framework. With this, a basis is developed to support quality managers planning quality assurance techniques in a more systematic way.},
 duplicado = {false},
 inserir = {false},
 title = {A Comprehensive Planning Framework for Selecting and Customizing Quality Assurance Techniques  Sign In or Purchase},
 year = {2007}
}

@article{210,
 abstract = {Abstract
Context

An accepted fact in software engineering is that software must undergo verification and validation process during development to ascertain and improve its quality level. But there are too many techniques than a single developer could master, yet, it is impossible to be certain that software is free of defects. So, it is crucial for developers to be able to choose from available evaluation techniques, the one most suitable and likely to yield optimum quality results for different products. Though, some knowledge is available on the strengths and weaknesses of the available software quality assurance techniques but not much is known yet on the relationship between different techniques and contextual behavior of the techniques.

Objective

This research investigates the effectiveness of two testing techniques equivalence class partitioning and decision coverage and one review technique code review by abstraction, in terms of their fault detection capability. This will be used to strengthen the practical knowledge available on these techniques.

Method

The results of eight experiments conducted over 5 years to investigate the effectiveness of three techniques code reading by stepwise abstraction, equivalence class partitioning and decision (branch) coverage were aggregated using a less rigorous aggregation process proposed during the course of this work.
Results

It was discovered that the equivalence class partitioning and the decision coverage techniques behaved similarly in terms of fault detection capacity (and type of faults caught) based on the programs and fault classification used in the experiments. They both behaved better than the code reading by stepwise abstraction technique.

Conclusion

Overall, it can be deducted from the aggregation results that the equivalence class partitioning and the decision coverage techniques used are actually equally capable in terms of the type and number of faults detected. Nevertheless, more experiments is still required in this field so that this result can be verified using a rigorous aggregation technique.},
 duplicado = {false},
 inserir = {false},
 title = {Determining the effectiveness of three software evaluation techniques through informal aggregation},
 year = {2013}
}

@article{211,
 abstract = {Abstract-- Classification makes a vital role to advancing
knowledge in both science and engineering. It is a process of
investigating the relationships between the objects to be
classified and identifies gaps in knowledge. Classification in
engineering also has a practical application. They can help
maturing Software Engineering knowledge, as classifications
constitute an organized structure of knowledge items. Till date,
in existing system, there have been few attempts at classifying in
test cases. In this research, we examine how useful classifications
in Software Engineering are for advancing knowledge by trying
to classify testing techniques. This paper presents a preliminary
classification of a set of unit testing techniques. To obtain this
classification, we enacted a generic process for developing useful
Software Engineering classifications. The proposed
classification has been proven useful for maturing knowledge
about testing techniques. SE helps to: 1) provide a systematic
description of the techniques,2) understand testing techniques
by studying the relationships among techniques (measured in
terms of differences and similarities), 3) identify potentially
useful techniques that do not yet exist by analyzing gaps in the
classification, and 4) support practitioners in testing technique
selection by matching technique characteristics to project
characteristics.},
 duplicado = {false},
 inserir = {true},
 title = {Achieving Software Engineering Knowledge Items with an Unit Testing Approach},
 year = {2012}
}

@article{212,
 abstract = {Software quality assurance (SQA) is an important part of software development that ensures the software quality. The effectiveness of the process is highly dependent on the technique(s) and tools(s) that are employed for the execution of the process. However, the choice of technique or tool to use is a hard and complicated decision that involves many factors. Research has been done to formalize the method of defining and characterizing these factors. In this paper we have analyzed the framework proposed by Frank Elberzhanger and Christian Denger. The shortcomings and limitations are highlighted and possible improvements are given. We further propose a new model/framework on the basis of above discussion.},
 duplicado = {false},
 inserir = {false},
 title = {An analysis of a comprehensive planning framework for customizing SQA},
 year = {2010}
}

@article{213,
 abstract = {Conducting verification and validation (V&V) of modeling and simulation (M&S) requires systematic and structured application of different V&V techniques throughout the M&S life cycle. Whether an existing technique is appropriate to a particular V&V activity depends not only on the characteristics of the technique but also on the situation where it will be applied. This work proposes a characterization approach to identifying and specifying the information relevant for selecting V&V techniques by means of an M&S-specific characterization schema. Based on the proposed schema, an application catalog that works as an information repository for V&V techniques selection is established. This characterization is applicable to any simulation study with well defined and structured model development and V&V processes.},
 duplicado = {false},
 inserir = {false},
 title = { A characterization approach to selecting verification and validation techniques for simulation projects},
 year = {2012}
}

@article{214,
 abstract = {Abstract
In a world where technology plays a major, increasing role day after day,
efforts devoted to develop better software are never too much. Both industry
and academia are well aware of this, and keep on working to face the new
problems and challenges that arise, more efficiently and effectively each time.
Companies show their interest in cutting-edge methods, techniques, and tools,
especially when they are backed up with empirical results that show practical
benefits. On the other hand, academia is more than ever aware of real-world
problems, and it is succeeding in connecting its research efforts to actual case
studies.
This thesis follows the mentioned trend, as it presents a study on software
applications development based on a real case. As its main novelty and contribution,
the integral process of software development is addressed from the
functional paradigm point of view. In contrast with the traditional imperative
paradigm, the functional paradigm represents not only a different way of developing
applications, but also a distinct manner of thinking about software
itself. This work goes through the characteristics and properties that functional
technology gives to both software and its development process, from
the early analysis and design development phases, up to the final and no less
critical verification and validation stages. In particular, the strengths and opportunities
that emerge in the broad field of testing, thanks to the use of the
functional paradigm, are explored in depth.
From the analysis of this process being put into practise in a real software
development experience, we draw conclusions about the convenience of applying
a functional approach to complex domains. At the same time, we extract
a reusable engineering methodology to do so.},
 duplicado = {false},
 inserir = {false},
 title = {On the development life cycle of distributed functional applications: a case study},
 year = {2010}
}

@article{215,
 abstract = {Abstract
Software quality has become and persistently remains a big issue among software users and developers. So, the importance of software evaluation cannot be overemphasized. An accepted fact in software engineering is that software must undergo evaluation process during development to ascertain and improve its quality level. In fact, there are too many techniques than a single developer could master, yet, it is impossible to be certain that software is free of defects. Therefore, it may not be realistic or cost effective to remove all software defects prior to product release. So, it is crucial for developers to be able to choose from available evaluation techniques, the one most suitable and likely to yield optimum quality results for different products - it bogs down to choosing the most appropriate for different situations. However, not much knowledge is available on the strengths and weaknesses of the available evaluation techniques. Most of the information related to the techniques available is focused on how to apply the techniques but not on the applicability conditions of the techniques practical information, suitability, strengths, weaknesses etc. This research focuses on contributing to the available applicability knowledge of software evaluation techniques. More precisely, it focuses on code reading by stepwise abstraction as representative of the static technique, as well as equivalence partitioning (functional technique) and decision coverage (structural technique) as representatives of the dynamic technique. The specific focus of the research is to summarize the results of a series of experiments conducted to investigate the effectiveness of these techniques among other factors. By effectiveness in this research, we mean the potential of each of the techniques to generate test cases capable of revealing software faults in the case of the dynamic techniques or the ability of the static technique to generate abstractions that will aid the detection of faults. The experiments used two versions of three different programs with seven different faults seeded into each of the programs. This work uses the results of the eight different experiments performed and analyzed separately, to explore this fact. The analysis results were pooled together and jointly summarized in this research to extract a common knowledge from the experiments using a qualitative deduction approach created in this work as it was decided not to use formal aggregation at this stage. Since the experiments were performed by different researchers, in different years and in some cases at different site, there were several problems that have to be tackled in order to be able to summarize the results. Part of the problems is the fact that the data files exist in different languages, the structure of the files are different, different names is used for data fields, the analysis were done using different confidence level etc. The first step, taken at the inception of this research was to apply all the techniques to the programs used during the experiments in order to detect the faults. This purpose of this personal experience with the experiment is to be familiarized and get acquainted to the faults, failures, the programs and the experiment situations in general and also, to better understand the data as recorded from the experiments. Afterwards, the data files were recreated to conform to a uniform language, data meaning, file style and structure. A well structured directory was created to keep all the data, analysis and experiment files for all the experiments in the series. These steps paved the way for a feasible results synthesis. Using our method, the technique, program, fault, program technique, program fault and technique fault were selected as main and interaction effects having significant knowledge relevant to the analysis summary result. The result, as reported in this thesis, indicated that the functional technique and the structural technique are equally effective as far as the programs and faults in these experiments are concerned. Both perform better than the code review. Also, the analysis revealed that the effectiveness of the techniques is influenced by the fault type and the program type. Some faults were found to exhibit better behavior with certain programs, some were better detected with certain techniques and even the techniques yield different result in different programs.},
 duplicado = {false},
 inserir = {false},
 title = {Summarizing the Results of a Series of Experiments: Application to the Effectiveness of Three Software Evaluation Techniques},
 year = {2009}
}

@article{216,
 abstract = {This work proposes a procedure to support the stopping criterion selection for
software testing regarding the matching of characteristcs presented by a software
project and the stopping criteria organized into a body of knowledge. Such body of
knowledge has been organized to contain relevant information acquired through a
quasi-systematic literature review concerned with 74 stopping criteria for software
testing. The stored information includes the attributes used for comparison between
software projects and stopping criteria. By considering such attributes, the stopping
criteria can be filtered according to the defined characteristics of the software project.
Next, for each filtered criterion, an adequacy degree suggesting the level of adequacy
(conceptual distance) between a stopping criterion and the software project is
calculated by comparing the equivalente attributes characterizing both the software
project and stopping criterion. Aiming to make easier its use, the proposed procedure
has been implemented and integrated into a test management and monitoring CASE
tool called Maraka. Its use has been exemplified by a proof of concept using different
projects contexts including one real application. },
 duplicado = {false},
 inserir = {false},
 title = {PROCEDIMENTO PARA APOIO A SELECAO DE CRITERIOS DE PARADA PARA TESTES DE SOFTWARE},
 year = {2013}
}

@article{217,
 abstract = {This work proposes an approach developed by following a scientific
methodology based on the conduction of secondary and primary studies with the
purpose of supporting the combined selection of Model Based Testing (MBT)
techniques for a given software project considering two aspects: (1) the adequacy level
between MBT techniques and the software project characteristics and (2) impact of
more than one MBT technique in some testing process variables. Results of an
experimental evaluation indicate this approach contributes to improve the MBT
techniques selection process effectiveness and efficiency when compared to another
selection approach available in the technical literature. Finally, it is presented a
computerized infrastructure to support the proposed selection approach, evaluated by
software engineers of an international software organization that indicated the
infrastructure can contribute positively for the MBT selection process.},
 duplicado = {false},
 inserir = {false},
 title = {SELECAO DE TECNICAS DE TESTE BASEADO EM MODELOS},
 year = {2009}
}

@article{218,
 abstract = {The lack of information limits component consumers to understand
candidate components suficiently in a way they can check if a given
component fulfills its goal. Thus, this work presents an approach to
support component testing aiming to reduce the lack of information
between component producers and component consumers. Additionallu,
the approach is covered by a CASE tool integrated in the development
environment. An experimental study was performad in order to evaluate
is efficiency and difficulties of its use. The experimental study
indicates that the approach is viable and the tool support provides
effort reduction to component producers and component consumers.},
 duplicado = {false},
 inserir = {false},
 title = {Uma abordagem para testes de componentes apoiada por uma ferramenta CASE},
 year = {2009}
}

@article{22,
 abstract = {Abstract: Software engineering is a knowledge-intensive task. Software development organisations depend on the
knowledge they have at their disposal to develop high-quality software and become competitive in the software industry.
Knowledge held by employees and in the organisation's repositories, routines and processes is important in the software
development process. A literature review was conducted to investigate knowledge management (KM) practices, their
benefits and challenges in software engineering. Four main areas of KM research were discovered: KM in software
organisations, KM in the software development process which includes KM in development teams, KM in testing, and KM in
development methodologies, KM in mitigating risk and the role of KM in fostering learning in software development
organisations. KM benefits and tools were also discovered in the literature review. Software organisations and teams create,
share, acquire, store and share knowledge during software development. KM activities such as knowledge sharing, storage
and reuse are applied in software development risk mitigation. A strong link between KM practices and learning in software
development was found. The conclusion drawn from the literature review is that KM activities have improved product quality
and delivery time, and execution of tasks. However, there are gaps that were identified too. The literature does not identify
KM challenges in software engineering, it does not state all the phases of the systems development lifecycle (SDLC) these
KM activities occur, and does not tell us if KM practices have helped mitigate risk in software development. },
 duplicado = {false},
 inserir = {false},
 title = {Knowledge Management in Software Development: A Literature Review},
 year = {2016}
}

@article{220,
 abstract = {Abstract. Small and large scale Web applications have been built along the
years influencing the conventional software engineering approaches and
demanding care with additional quality issues such as verification, validation
and testing. Regarding software testing, the results of a secondary study have
pointed out at least 71 different testing techniques specifically applicable to
Web software projects. Hence, the diversity of Web applications and
development methodologies combined with the great number of available
software testing techniques make hard the decision about which technique
should be used in the project. Therefore, the understanding of testing techniques
characteristics can support the decision making and represents an interesting
research challenge. Based on this, this paper presents a set of Web software
testing techniques characteristics synthesized from the results of a secondary
study and evaluated through a survey with specialists in the field. It represents
an initial result towards the organization of a Web application testing techniques
characterization schema, which intends to support the choosing of suitable
software testing techniques for a Web software project. },
 duplicado = {false},
 inserir = {false},
 title = {CaracterIsticas de Tecnicas de Teste de Software para Uso em Projetos WEB},
 year = {2011}
}

@article{221,
 abstract = {Abstract. Software technologies, such as model-based testing approaches,
have specific characteristics and limitations that can affect their use in
software projects. It is very important to make available knowledge regarding
such technologies aiming at to support its applicability in software projects. In
particular, a choice of model-based testing approach can influence testing
success or failure. Therefore, this paper presents the knowledge acquired from
some secondary and primary studies regarding model-based testing
approaches and the proposal of a strategy towards supporting their selection
for software projects. },
 duplicado = {false},
 inserir = {false},
 title = {Estrategia para Apoiar a Selecao de Abordagens de Teste Baseado em Modelos para Projetos de Software},
 year = {2008}
}

@article{222,
 abstract = {Abstract:
The aim of this paper is to provide a review on verification and validation of embedded software. An embedded software is a product that contains a microprocessor and software to perform certain function. The growing demand for new features and functionalities of embedded systems makes the design and implementation reach a higher level of complexity. We focused on identifying research trends in order to provide a review of the challenges that emerge during the verification and validation processes of embedded software. The research works are grouped by related themes in order to find research problems that persist nowadays.},
 duplicado = {false},
 inserir = {false},
 title = {A review on verification and validation for embedded software},
 year = {2016}
}

@article{223,
 abstract = {Abstract. During the course of a strategic cooperation, a method was developed that supports
an organization in adjusting its process scope. The approach systematically analyzes process
needs of products and projects, and then evaluates the organization's processes with respect to
the fulfillment of these needs. The result is a recommendation as to which processes to keep
and maintain, which ones to discard, and which process to apply in which situation. The
method was successfully applied during the development of software development standards
for the development of satellite software at the Japan Aerospace Exploration Agency (JAXA).
It could be shown that the method can significantly reduce the effort required for creating and
maintaining software process standards, while at the same time providing projects with exactly
the processes that are needed. },
 duplicado = {false},
 inserir = {false},
 title = {Which Processes Are Needed in Five Years? Strategic Process Portfolio Management at the Japan Aerospace Exploration Agency (JAXA)},
 year = {2010}
}

@article{224,
 abstract = {Abstract Over more than two decades, Empirical
Software Engineering (ESE) research has achieved
considerable results in building our knowledge about selecting
and applying appropriate empirical methods for technology
evaluation. Empirical studies in general and empirical studies
in industrial settings in particular have played an important
role in successful transition of many Software Engineering
(SE) technologies to industry, for example, defect detection
techniques and automated test cases. However, conducting
empirical research in industrial settings remains a challenging
undertaking for a variety of reasons. There is no substantial
literature reporting on the challenges and complexities
involved in conducting empirical studies in an industrial
setting in general and in settings whose business models are
built around global sourcing. This paper reports some of the
tales of our experiences and lessons learned from conducting
empirical research in industry as corporate as well as academic
researchers. Some of the observed challenges include short
time horizon for research, high expectations, limited research
skills, and the acceptable research rigor. The paper explores
some of these issues with relevant examples and provides some
common strategies for overcoming these issues. We also stress
that researchers and practitioners should share their
experiences of conducting empirical research in order to help
build a body of knowledge to guide the future efforts.},
 duplicado = {false},
 inserir = {false},
 title = {Tales and Lessons from the Front Lines of Conducting Empirical Studies in (or with) Industry},
 year = {2013}
}

@article{226,
 abstract = {Aspect-oriented programming is a relatively new programming paradigm and it builds on the basis of object oriented programming paradigm. It deals with those concerns that cross-cut the modularity of traditional programming mechanisms and it aims at reduction of code and to provide higher cohesion. As with any new technology aspect oriented programming provides some benefits and also there are some costs associated with it. In this thesis we have done a systematic review on aspect oriented software testing in the context of testing challenges. Detailed analysis have been made to show that how effective are the structural test techniques to handle these challenges. We have given the analysis of Aspect-oriented test techniques effectiveness, based on research literature.},
 duplicado = {false},
 inserir = {false},
 title = {Systematic Review on Testing Aspect-orientedPrograms: Challenges, Techniques and Their Effectiveness},
 year = {2008}
}

@article{227,
 abstract = {Software testing is a vital activity and a determining factor for
success of a given project since testing allow stakeholders to know
if the product meets their expectations and requirements,
additionally, the cost of testing is very significant in relation to
development cost, therefore is important to select the most suitable
testing techniques and tools in a given project to find defects at
less cost possible; however in many projects, the practitioners
adopt the same testing technique/tool used in past projects or any
available without knowing the testing technique attributes. As a
theoretical basis for future research, in this article we present an art
state about the testing techniques evaluation and selection which
contain a set of characterization schemas and a comparison
between approaches for lead primary studies leading to build
knowledge bases or repositories and in other way we present the
existing methods and approaches about the informed selection of
testing techniques for a given project based on repositories
information.},
 duplicado = {false},
 inserir = {false},
 title = {Evaluacion y seleccion de tecnicas de prueba de software},
 year = {2010}
}

@article{228,
 abstract = {ABSTRACT
Conducting verification and validation (V&V) of modeling and simulation (M&S) requires systematic
and structured application of different V&V techniques throughout the M&S life cycle. Whether an
existing technique is appropriate to a particular V&V activity depends not only on the characteristics of
the technique but also on the situation where it will be applied. This work proposes a characterization
approach to identifying and specifying the information relevant for selecting V&V techniques by means
of an M&S-specific characterization schema. Based on the proposed schema, an application catalog that
works as an information repository for V&V techniques selection is established. This characterization is
applicable to any simulation study with well defined and structured model development and V&V processes.},
 duplicado = {false},
 inserir = {fale},
 title = {A CHARACTERIZATION APPROACH TO SELECTING VERIFICATION AND VALIDATION TECHNIQUES FOR SIMULATION PROJECTS},
 year = {2012}
}

@article{229,
 abstract = {Abstract
Concurrent software testing is a challenging activity due to factors that are not present in sequential programs, such as communication, synchronization, and non-determinism, and that directly affect the testing process. When we consider multithreaded programs, new challenges for the testing activity are imposed. In the context of structural testing, an important problem raised is how to deal with the coverage of shared variables in order to establish the association between def-use of shared variables. This paper presents results related to the structural testing of multithreaded programs, including testing criteria for coverage testing, a supporting tool, called ValiPthread testing tool and results of an experimental study. This study was conducted to evaluate the cost, effectiveness, and strength of the testing criteria. Also, the study evaluates the contribution of these testing criteria to test specific aspects of multithreaded programs. The experimental results show evidence that the testing criteria present lower cost and higher effectiveness when revealing some kinds of defects, such as deadlock and critical region block. Also, compared to sequential testing criteria, the proposed criteria show that it is important to establish specific coverage testing for multithreaded programs.},
 duplicado = {false},
 inserir = {false},
 title = {Contributions for the structural testing of multithreaded programs: coverage criteria, testing tool, and experimental evaluation},
 year = {2018}
}

@article{23,
 abstract = {Abstract: Purpose - The purpose of this paper is to provide organizations in the Chinese cultural context with a conceptual model for an integrated adoption of existing knowledge management (KM) methods and to improve the effectiveness of their KM activities. Design/methodology/approaches. A comparative analysis is conducted between China and the western world based on a comprehensive document analysis from key databases available. In total, four critical dimensions, which are identified through the review of the related literature including observation, methodology, systems and applications, are used in the analysis for addressing the questions of why, what, how, and where in KM. Findings The paper rationalizes the need for this study in conformity with the emerging trend of an integrated use of diverse KM methods and approaches for effective KM in organizations. Further, a reference model for the integrated adoption of existing KM methods is developed with respect to the characteristics of the Chinese culture for improving the effectiveness of KM activities in the organizations. Such a model can adequately address the questions of why, what, how, and where in KM in Chinese organizations to the best of Chinese collective interest, which provides more comprehensive and unified KM views, activities, processes and technologies for collaborative innovation in Chinese organizations. Originality/value  This paper is the first of its kind to systematically review existing KM methods and approaches with respect to the perspectives of observations, methodologies, systems and applications for effective KM in Chinese organizations.},
 duplicado = {false},
 inserir = {false},
 title = {An integrated model for effective knowledge management in Chinese organizations},
 year = {2013}
}

@article{230,
 abstract = {Abstract:
This paper presents a framework to instantiate software technologies selection approaches by using search techniques. The software technologies selection problem (STSP) is modeled as a Combinatorial Optimization problem aiming attending different real scenarios in Software Engineering. The proposed framework works as a top-level layer over generic optimization frameworks that implement a high number of metaheuristics proposed in the technical literature, such as JMetal and OPT4J. It aims supporting software engineers that are not able to use optimization frameworks during a software project due to short deadlines and limited resources or skills. The framework was evaluated in a case study of a complex real-world software engineering scenario. This scenario was modeled as the STSP and some experiments were executed with different metaheuristics using the proposed framework. The results indicate its feasibility as support to the selection of software technologies.},
 duplicado = {false},
 inserir = {false},
 title = {A Framework to Support the Selection of Software Technologies by Search-Based Strategy},
 year = {2014}
}

@article{231,
 abstract = {Abstract
The combination of testing techniques is considered an effective strategy to evaluate the quality of a software product. However, the selection of which techniques to combine in a software project has been an interesting challenge in the software engineering field because the high number of techniques available at the technical literature. This paper presents an approach developed to support the combined selection of model-based testing techniques, applying multiobjective combinatorial optimization strategies, by determining the minimum dominating set in a bipartite and bi-weighted graph. Thus, an evolutionary strategy based on a multiobjective genetic algorithm is proposed to generate trade-off techniques subsets between the maximum coverage of software project characteristics and the minimum eventual effort to construct models used for test cases generation. In an empirical evaluation, our evolutionaryalgorithmstrategygavebetterresultsthanthepreviousapproaches.},
 duplicado = {false},
 inserir = {False},
 title = {Providing Trade-Off Techniques Subsets to Improve Software Testing Effectiveness: Using Evolutionary Algorithm to Support Software Testing Techniques Selection by a Web Tool},
 year = {2012}
}

@article{232,
 abstract = {Abstract
Experiments in computing share many characteristics with the traditional experimental method, but also present significant differences from a practical perspective, due to their aim at producing software artifacts and the central role played by human actors and organizations (e.g., programmers, project teams, software houses) involved in the software development process. By analyzing some of the most significant experiments in the subfield of software engineering, we aim at showing how the conceptual framework that supports experimental methodology in this context needs an extension in a socio-technical perspective.},
 duplicado = {false},
 inserir = {False},
 title = {Rethinking Experiments in a Socio-Technical Perspective: The Case of Software Engineering},
 year = {2016}
}

@article{233,
 abstract = {Testing can still be considered a bottleneck for software product line engineering. The variability implemented in the source artifacts increases its complexity. Due to its key role for product line quality, testing requires cost-effective practices, such as techniques for test selection should be produced to enable companies to experience the substantial production cost savings. In this paper, we present the outline of a Ph.D. research aimed at developing a reasoning framework to improve SPL testing practices. Based on multiple sources of evidence, the framework intends to provide testers with an automated reasoner for determining which techniques may be suitable for a given variability implementation mechanism, and how these should be employed in order to makes testing in a SPL a more effective and efficient practice. We plan to perform empirical evaluations in order to assess the proposal effectiveness.},
 duplicado = {false},
 inserir = {false},
 title = {Towards a reasoning framework for software product line testing},
 year = {2012}
}

@article{234,
 abstract = {This research for this thesis was conducted to develop a framework which supports the automatic configuration of project-specific software development processes by selecting and combining different technologies: the Process Configuration Framework. The research draws attention to the problem that while the research community develops new technologies, the industrial companies continue only using their well-known ones. Because of this, technology transfer takes decades. In addition, there is the fact that there is no solution which solves all problems in a software development project. This leads to a number of technologies which need to be combined for one project.
The framework developed and explained in this research mainly addresses those problems by building a bridge between research and industry as well as by supporting software companies during the selection of the most appropriate technologies combined in a software process. The technology transformation gap is filled by a repository of (new) technologies which are used as a foundation of the Process Configuration Framework. The process is configured by providing SPEM process pattern for each technology, so that the companies can build their process by plugging into each other.
The technologies of the repository were specified in a schema including a technology model, context model, and an impact model. With context and impact it is possible to provide information about a technology, for example, its benefits to quality, cost or schedule. The offering of the process pattern as output of the Process Configuration Framework is performed in several stages:
I Technology Ranking:
1 Ranking based on Application Domain, Project & Impact
2 Ranking based on Environment
3 Ranking based on Static Context
II Technology Combination:
4 Creation of all possible Technology Chains
5 Restriction of the Technology Chains
6 Ranking based on Static and Dynamic Context
7 Extension of the Chains by Quality Assurance
III Process Configuration:
8 Process Component Diagram
9 Extension of the Process Component Diagram
10 Instantiation of the Components by Technologies of the Technology Chain
11 Providing process patterns
12 Creation of the process based on Patterns
The effectiveness and quality of the Process Configuration Framework have additionally been evaluated in a case study. Here, the Technology Chains manually created by experts were compared to the chains automatically created by the framework after it was configured by those experts. This comparison depicted that the framework results are similar and therefore can be used as a recommendation.
We conclude from our research that support during the configuration of a process for software projects is important especially for non-experts. This support is provided by the Process Configuration Framework developed in this research. In addition our research has shown that this framework offers a possibility to speed up the technology transformation gap between the research community and industrial companies.},
 duplicado = {false},
 inserir = {false},
 title = {A Framework for Goal-oriented Process Configuration},
 year = {2012}
}

@article{235,
 abstract = {Abstract
The article analyses the theoretical aspects relevant to designing business software testing models, describing
the evolution of software, and argumenting the case in favour of the need for continuous improvement of software
testing. The results of prominent studies in the domain of software testing, featuring as the starting point
for business software testing model design are analysed. The final part of the article defines a research model
which will serve as a basis for research whose outcome will define the business software testing model design.},
 duplicado = {false},
 inserir = {false},
 title = {Business Software Testing Model Design: A Theoretical Framework},
 year = {2013}
}

@article{236,
 abstract = {Abstract:
Regression testing is a vast field of research. It is very costly and time consuming process but on the other hand very important process in software testing. Retest all, Test case Selection, Hybrid and Test Case Prioritization are its various techniques which are used to reduce the efforts in maintenance phase. In technical literature several techniques are present with their different and vast number of goals which can be applied in software projects despite of that they have not proven their true efficiency in the testing process. The major problem in regression testing area is to select the test case prioritization technique/s that is effective in such a way that maximum project characteristics should be cover in a minimum time span. However, consideration of this decision be carefully done so that loss of resources can be avoided in a software project. Based on the above scenario, author proposes a selection schema to support the selection of TCP techniques for a given software project aiming at maximizing the coverage of software project characteristics considering aspect of prioritization of software project characteristics. At the end, preliminary results of an experimental evaluation are presented. The purpose of this research is decision should be based on the objective knowledge of the techniques rather than considering some perception and assumptions.},
 duplicado = {false},
 inserir = {false},
 title = {A Schema Support for Selection of Test Case Prioritization Techniques},
 year = {2015}
}

@article{237,
 abstract = {Tourism is one of the biggest industry branches with billions of tourists
traveling every year around the world. Therefore, solutions providing
tourist information have to be up to date with both changes in the industry
and the world's technological progress. The aim of this thesis is
to present a design and a prototype of a tourist mobile service which is
individual-oriented, cost-free for the end user, and secure.
i
On the information providers' side, the solution is implemented as a Webbased
database. The end users access the information through a Bluetooth
application on their mobile devices. The Bluetooth-based solution allows
to avoid any costs for the end users, that is tourists. The study shows that,
even with small data transfers, the tourists could save significantly when
compared to possible roaming charges for data transfer. Also, the proposed
mobile service is not intrusive, as it is provided through an application
installed by tourists voluntarily on their mobile devices.
Through design and implementation this work shows that it is possible to
build a system which can be used to provide information services to tourists
through mobile phones. The work achieved a successful ongoing synchronization
between the client and the server databases. Implementation and
usage were limited to smart phones only, as they provide better technological
support for the solution having features like maps, GPS, Wi-Fi,
Bluetooth and Databases. Moreover, the design of this system shows how
Bluetooth technology can be used effectively as a means of communication
while minimizing its shortcomings and risks, such as security, by bypassing
Bluetooth server service discovery protocol (SDP) and connecting directly
to the device.
Apart from showing the design and implementation of the end-user costfree
mobile information service, the results of this work also highlight the
possible business opportunities to the provider of the service.},
 duplicado = {false},
 inserir = {false},
 title = {A DESIGN AND A PROTOTYPE FOR A MOBILE INFORMATION SERVICE FOR TOURISTS},
 year = {2012}
}

@article{239,
 abstract = {Testing techniques have been widely used as a method to help software engineers in detecting defects in a software system in order to develop high-quality software system and achieve customer satisfaction. Different techniques reveal different quality aspects of a software system. This paper proposes a model-based methodology of the major accepted categories of testing techniques to evaluate many aspects like functional, structural, reliability, usability requirements and check their consistency. Evaluating all these aspects in a software project will help ensure the success of such software development project and will also assist software testers in error handling in order to achieve the desired quality for software customers.},
 duplicado = {false},
 inserir = {false},
 title = {A Generic Model-Based Methodology of Testing Techniques to Obtain High Quality Software},
 year = {2015}
}

@article{24,
 abstract = {Abstract: Software testing is the process on how to identify and deliver the software as a product based on the specification that has been given and required by the users. In order to ensure that the product is working based on the user specification, there are many people who are working together for that purposes as a community of practice (CoP). The CoP in software testing environment is including the system designer, programmer, and system tester as well as the user by himself. Based on this scenario of working together or collaboratively in order to avoid a lot of mistake or errors and causes the software failure, which may be found during the processes of software testing process, so that there is a need for CoP to have a tool called knowledge management system (KMS) in managing the knowledge of best practice and lesson learnt. The paper will discuss the concept on how the KMS is offering of its processes through knowledge life cycle which starting from knowledge acquisition, knowledge storing, knowledge dissemination, and knowledge application. Therefore, by using the model of KMS in managing knowledge of software testing, CoP can utilize the knowledge in KMS and it will reduces the mistake or errors, so that they can delivered a good product besides to enhance the quality of software of the particular users.},
 duplicado = {false},
 inserir = {false},
 title = {A model of knowledge management system in managing knowledge of software testing environment},
 year = {2011}
}

@article{240,
 abstract = {Abstract:
The technical literature regarding Model-based Testing (MBT) has several techniques with different characteristics and goals available to be applied in software projects. Besides the lack of information regarding these techniques, they could be applied together in a software project aiming at improving the testing coverage. However, this decision needs to be carefully analyzed to avoid loss of resources in a software project. Based on this scenario, this paper proposes an approach with the purpose of supporting the unique or combined selection of MBT techniques for a given software project considering two aspects: the adequacy level between MBT techniques and the software project characteristics and impact of more than one MBT technique in some testing process variables. At the end, preliminary results of an experimental evaluation are presented.},
 duplicado = {false},
 inserir = {false},
 title = {Porantim: An approach to support the combination and selection of Model-based Testing techniques  Sign In or Purchase},
 year = {2009}
}

@article{241,
 abstract = {Software architecture has emerged as an important field of software
engineering for managing the realm of large-system development and
maintenance. The main intent of software architecture is to provide
intellectual control over a sophisticated system enormous complexity.
The key idea of this dissertation is that there is no silver bullet in
software engineering, each method has pros and cons; the goodness of a
method (tool, technique, etc.) varies based on the peculiarities of the
application context.
According to a famous idiom: i) a poor craftsman blames his tool,
ii) a really bad craftsman chooses the wrong tool for the job and then he
blames the tool, iii) a good craftsman chooses and uses the tool well.
While the software engineering community has been mainly focused on
providing new methods, which usage are aimed/supposed to provide
better results than older methods, there is a lack in helping the software
practitioners in selecting/adapting the available tools. Hence, in this
view, the contribution of the present dissertation, instead of being a new
method for architectural design, which would have been easily forgotten
in a bookcase, is a characterization of the existing methods. In other
words, this dissertation provides a toolbox for software architecture
design, from which software architects can select the best method to
apply, according to the application context.
In this dissertation, the available architectural methods have been
characterized by means of empirical studies. Unfortunately, the
application of empirical methods on software architecture includes some
troubles. A contribution of the present dissertation is a characterization
of the available empirical methods by exposing their levels of
challenges that researchers have to face when applying empiricism to
software architecture. Such a proposed characterization should help to
increase both the number and validity of software architecture empirical
studies by allowing researchers to select the most suitable empirical
method(s) to apply (i.e. the one with minor challenges), based on the
application contexts (e.g. available software applications, architects,
reviewers). However, in our view, in order to provide high levels of
conclusion and internal validity, empirical methods for software
6
architecture should be oriented to take advantage of both quantitative
and qualitative data. Moreover, based on the results from two
experiments, the challenges, in conducting evidence-based software
architecture investigations, might 1) highly influence the results of the
empirical studies, and 2) be faced by empiricists cleverness.
Architecting software system is a complex job and it encompasses
several activities; this dissertation focuses on the following families of
activities: software architecture design, resolving architectural tradeoffs,
documenting design decisions, and enacting empirical studies on
software architecture (as just described).
Regarding the resolution of architectural tradeoffs, based on our
review of already proposed decision making techniques, we realized that
no one of the available decision-making technique can be considered in
general better than another; each technique has intrinsically its own
level of complexity and proneness to specific problems. Since we
cannot decide in advance what degree of complexity of modeling is
sufficient, instead of proceeding by trial and error, we offered guidelines
on which complexity to emphasize for avoiding specific problem(s).
Our key idea is to expose and organize in a useful way, namely by a
characterization schema, in what extent each decision-making technique
is prone to specific problems. In this way, the level of proneness of
specific technique to specific problems becomes a quality attribute of
the decision-making technique. Furthermore, we situated in the
proposed characterization schema eighteen different decision-making
techniques already proposed by the literature in the domains of
architecture design, COTS selection, and release planning. Such
localization validates the completeness of the proposed characterization
schema, and it provides a useful reference for analyzing the state of the
art
Regarding software architecture design, this dissertation tried to
answer to following question: Do actual software architecture design
methods meet architects needs? To do so, we provide a
characterization of the available methods by defining nine categories of
software architects needs, proposing an ordinal scale for evaluating the
degree to which a given software architecture design method meets the
needs, and then applying this to a set of software architecture design
methods. Based on results from the present study, we argue that there
7
are two opposite but valid answers to the aforementioned question: a)
Yes, they do. In fact, we showed that one or more software architecture
design methods are able to meet each individual architect needs that we
considered. b) No, they do not. In fact, we showed that there is no
software architecture design method that is able to meet any tuple of
seven or more needs, which means that there is still some work to do to
improve software architecture design methods to actually help
architects. In order to provide directions for software architecture design
method improvement, we presented couples of needs, and triplets of
needs that actual software architecture design methods are unable to
meet. Moreover, an architect can use such characterization to choose the
software architecture design method which better meets his needs.
Regarding design decision documentation, we conducted a
controlled experiment for analyzing the impact of documenting design
decisions rationale on effectiveness and efficiency of individual/team
decision-making in presence of requirement changes. Main results show
that, for both individual and team-based decision-making, effectiveness
significantly improves, while efficiency remains unaltered, when
decision-makers are allowed to use, rather not use, the proposed design
rationale documentation technique. Being sure that documenting designdecisions
rationale does help, we argued why it is not used in practice
and what we can do to facilitate its usage. Older design decisions
rationale documentation methods aimed at maximizing the consumer
(documentation reader) benefits by forcing the producer (documentation
writer) to document all the potential useful information; they eventually
ran into too many inhibitors to be used in practice. In this dissertation
we propose a value-based approach for documenting the reasons behind
design decision, which is based on a priori understanding of who will
benefit later on, from what set of information, and in which amount.
Such a value-based approach for documenting the reasons behind design
decision offers means to mitigate all the known inhibitors and it is based
on the hypothesis that the set of required information depends on the
purpose (use case) of the documentation. In order to validate such a
hypothesis we ran an experiment in a controlled environment,
employing fifty subjects, twenty-five decisions, and five different
purposes (documentation use case) of the documentation. Each subjects
practically used the documentation to enact all the five documentation
use case(s) by providing an answer and a level of utility for each
8
category of information in the provided documentation. Both descriptive
and statistical results confirm our expectancies that the level of utility,
related to the same category of information in the design decision
rationale documentation, significantly changes according to the purpose
of the documentation. Such result is novel and implies that the consumer
of the rationale documentation requires, or not, a specific category of
information according the specific purpose of the documentation.
Consequently, empirical results suggest that the producer can tailor the
design decision rationale documentation by including only the
information required for the expected purposes of the documentation.
This result demonstrates the feasibility of our proposed value-based
approach for documenting the reasons behind design decision. },
 duplicado = {false},
 inserir = {false},
 title = {A Toolbox for Software Architecture Design },
 year = {2008}
}

@article{25,
 abstract = {Abstract: The development of open source software (OSS), and their deployment by general public as well as by different types of organizations, has increased manifold over the past decade or so. In spite of the ubiquity of OSS, the quality of many OSS remains questionable. Testing provides a curative approach for OSS quality assurance, and a comprehensive approach to testing is a knowledge-intensive endeavor. The management of knowledge in the OSS test process forms a perpetual cycle of creation, dissemination, and acquisition of test knowledge.},
 duplicado = {false},
 inserir = {true},
 title = {A knowledge management approach for testing open source software systems},
 year = {2014}
}

@article{28,
 abstract = {Abstract: Software testing (ST) is the process of identifying and delivering the software as a product
based on the specification that has been given and required by the users. In order to ensure the product is
working properly based on the user specification, there are many people who are working together and
provide their services for a community of practice (CoP) purposes. The CoP in ST of cloud computing
environment are including the software designer, programmer, and system tester as well as the software
users by themselves. Based on this scenario of working together or working collaboratively in order to
avoid a lot of mistakes or errors and causes the software failure, which may be found during the ST as a
service (STaaS) process, so that there is a need for CoP to have a tool called Collaborative Knowledge
Management System (CKMS) in managing the ST knowledge of best practice and lesson learnt. The
paper will discuss a model on how the ST of CoP is offering its service of processes called STaaS through
knowledge life cycle which starting from knowledge acquisition, knowledge storing, knowledge
dissemination, and knowledge application in order to overcome any shortcoming faulty or failure
especially during the software development and it implementation in a cloud computing environment.
Therefore, by utilizing the CKMS model in managing knowledge of STaaS, the CoP can maximize the
STaaS knowledge in the CKMS and furthermore to overcome the mistakes or errors, so that they can also
delivered a good product as part of well services besides in enhancing the quality of software of the
particular users. },
 duplicado = {false},
 inserir = {false},
 title = {Towards Developing Software Testing As a Service (Staas) Model in Cloud Computing: A Case of Collaborative Knowledge Management System},
 year = {2012}
}

@article{36,
 abstract = {Abstract: Software Maintenance (SM) community of practice (CoP) is including the system maintainer as a service provider and the users as its service recipient. Based on this scenario, they are working together or work collaboratively in order to optimize the capabilities of the software, which we called it as SM as a service (SMaaS) process. In this context, The CoP can make use knowledge management system (KMS) as a tool in managing the SM knowledge as a best practice and lesson learnt. SM is the process of identifying and delivering the software as a product based on service level agreement (SLA) that has been made between service provider and the users. The paper will discuss the model on how the SM is offering its service of processes through knowledge life cycle which starting from knowledge acquisition, knowledge storing, knowledge dissemination, and knowledge application in order to avoid any shortcoming fault or failure especially during the software development (SD) in a private cloud computing environment. Therefore, by using the KMS model in managing knowledge of SM, CoP can utilize the SM knowledge in the KMS and it will reduces the mistake or errors, so that they can also maintain a good service besides in enhancing the return of investment (ROI) as well as the quality of software to the particular users. },
 duplicado = {false},
 inserir = {false},
 title = {A Model of Managing Knowledge for Software Maintenance As a Service (SMaaS) in a Private Cloud Computing Environment},
 year = {2012}
}

@article{37,
 abstract = {Abstract: Context: Knowledge management technologies have been employed across software engineering activities for more than two decades. Knowledge-based approaches can be used to facilitate software architecting activities (e.g., architectural evaluation). However, there is no comprehensive understanding on how various knowledge-based approaches (e.g., knowledge reuse) are employed in software architecture. Objective: This work aims to collect studies on the application of knowledge-based approaches in software architecture and make a classification and thematic analysis on these studies, in order to identify the gaps in the existing application of knowledge-based approaches to various architecting activities, and promising research directions. Method: A systematic mapping study is conducted for identifying and analyzing the application of knowledge-based approaches in software architecture, covering the papers from major databases, journals, conferences, and workshops, published between January 2000 and March 2011. Results: Fifty-five studies were selected and classified according to the architecting activities they contribute to and the knowledge-based approaches employed. Knowledge capture and representation (e.g., using an ontology to describe architectural elements and their relationships) is the most popular approach employed in architecting activities. Knowledge recovery (e.g., documenting past architectural design decisions) is an ignored approach that is seldom used in software architecture. Knowledge-based approaches are mostly used in architectural evaluation, while receive the least attention in architecture impact analysis and architectural implementation. Conclusions: The study results show an increased interest in the application of knowledge-based approaches in software architecture in recent years. A number of knowledge-based approaches, including knowledge capture and representation, reuse, sharing, recovery, and reasoning, have been employed in a spectrum of architecting activities. Knowledge-based approaches have been applied to a wide range of application domains, among which Embedded software has received the most attention.},
 duplicado = {false},
 inserir = {false},
 title = {Application of knowledge-based approaches in software architecture: A systematic mapping study},
 year = {2013}
}

@article{39,
 abstract = {Abstract: Software development is a knowledge intensive and collaborative activity. The success of the project totally depends on knowledge and experience of the developers. Increasing knowledge creation and sharing among software engineers are uphill tasks in software development environments. The field of knowledge management has emerged into this field to improve the productivity of the software by effective and efficient knowledge creation, sharing and transferring. In other words, knowledge management for software engineering aims at facilitating knowledge flow and utilization across every phases of a software engineering process. Therefore, adaptation of various knowledge management practices by software engineering organizations is essential. This survey identified the knowledge management involvement in software engineering in different perspectives in the recent literature and guide future research in this area.},
 duplicado = {false},
 inserir = {false},
 title = {A Survey on Knowledge Management in Software Engineering},
 year = {2015}
}

@article{4,
 abstract = {Abstract: As software organizations try to mitigate operational and technical risk that occurs when using software, there is need to develop a knowledge intensive system to assist team members in mitigating both operational and technical risk. Knowledge mapping in risk mitigation context is in its infancy and has the potential to address both operational and technical risk faced by software organizations. However, as the amount and depth of organizational knowledge increases, it poses some challenges to software organizations. The key challenges for knowledge intensive organizations are how to identify, assimilate, disseminate, and apply these risk knowledge; particularly between different team members in same software development project. Thus this paper proposes a knowledge mapping process model to assist in mitigating risk (operational and technical) that occurs in software organizations. Knowledge mapping is the field within Knowledge Management (KM) that aims to optimize the efficient and effective use of the organization's knowledge. The mapping process model can support software management teams in measuring and treating risk, thus aiding decision making in software management. Data was collected using semi-structured interview through case study. The interview transcripts were coded and categorized using Nvivo software.},
 duplicado = {false},
 inserir = {false},
 title = {KNOWLEDGE MAPPING PROCESS MODEL FOR RISK MITIGATION IN SOFTWARE MANAGEMENT},
 year = {2017}
}

@article{40,
 abstract = {Abstract: Software testing is a sub area of software engineering which is also a knowledge intensive and collaborative activity. Our previous study results revealed that knowledge in the repositories were outdated, internal documents are unstructured and varied formats, less accessing facilities and lack of targeted delivery methods, such that software testers from software companies are highly affected by not being able to get vital information required to carryout their software testing activities. Ontologies emerge as one of the more appropriate knowledge management tools for supporting knowledge representation, processing, storage and retrieval. A Software testing ontology is designed to represent the necessary software testing knowledge within the software testers' context. The ontology-based Knowledge Sharing Portal is introduced into the semantic representation of software testing knowledge. SPARQL is used as the query language to retrieve software testing knowledge from the semantic storage. Both Ontology experts and non-experts evaluated the developed ontology.},
 duplicado = {false},
 inserir = {true},
 title = {An Ontology-Based Knowledge Sharing Portal for Software Testing},
 year = {2017}
}

@article{41,
 abstract = {Technical debt (TD) has received significant attention in the past few years. The
concept of TD was initially concerned with internal quality issues in coding, and
currently it has been extended to the whole software lifecycle, such as software
architecture and testing. At the architecture level, architectural technical debt (ATD)
is mainly incurred by architecture decisions that intentionally or unintentionally
compromise system-wide quality attributes, particularly maintainability and
evolvability. Considering the fundamental influence of software architecture on
quality attributes, including maintainability and evolvability, TD at the architecture
level (i.e., ATD) has greater and wider impact on these quality attributes than other
TD types, such as code-level TD. Thus, ATD needs to be systematically managed in
an appropriate manner, in order to improve the health of the software architecture
and optimize the cost of maintenance and evolution of the system in the long term.
Most research on TD focuses on TD at the source code level while TD at the
architecture level and its management remain under-explored. Currently, there is a
lack of an overall process for systematically managing ATD, as well as approaches
for concrete ATD management activities, particularly for ATD identification,
measurement, and documentation.
Before investigating the key problems in ATD management, we first needed to
obtain a comprehensive understanding on the concept of TD and the current state of
research on TD management (TDM). This could help us to build a solid
understanding on ATD and inspire us to come up with appropriate approaches for
ATD management. To this end, we conducted a systematic mapping study on TD
and its management. The main results are summarized as follows. (1) TD is classified
into ten types according to the stages of the software development lifecycle, and code
TD was the most studied TD type. (2) Interest, principal, and risk are the most
frequently-used notions to describe and explain the TD concept. (3) Most studies
argue that TD negatively affects the maintainability of the software system. (4) Eight
TDM activities were identified; among the activities, TD identification, measurement,
and repayment received the most attention, while TD representation/documentation
received the least. (5) Among the 29 tools used for managing TD, only four are
dedicated tools for TDM; most tools only support managing code and design TD,
while few tools support the management of other types of TD, e.g., ATD.
After having gained an understanding on the state of the art on TD research, we
tried to explore how to manage ATD. First we proposed a conceptual model of ATD
based on the understanding on TD, and an ATD item template based on this model;
then we developed an ATD management process that utilizes this conceptual model,
in order to facilitate decision-making and decision-evaluation in a value-oriented
perspective in architecture design. The ATD management process is comprised of six
activities: ATD identification, measurement, prioritization, repayment, monitoring,
and documentation. An industrial example using the proposed approach of ATD management in architecture synthesis and evaluation
shows how ATD can be managed in architecting. The contribution of this work
provides a controllable and predictable balance between the value and cost of
architecture design in the long term.
In our ATD management process, the first step is to identify ATD. Existing ATD
identification approaches are mainly based on source code analysis and thus suffer
from certain shortcomings: (1) they can only identify issues at the system
implementation; (2) they can only be employed after the systems is implemented in
code; and (3) they lack a mechanism to confirm whether the identified ATD is real
ATD or not. To address these issues, we proposed an ATD identification approach
based on architecture decisions and change scenarios. We evaluated the effectiveness
and usability of this approach, through an industrial case study in a large
telecommunications company. The results show that the proposed approach is useful
and easy to use for ATD identification, and it also supports release planning and
ATD interest measurement.
It is difficult to precisely measure ATD, but it makes sense to indicate the amount
of the total ATD in a software system. ATD indicators can show the change direction
of the ATD in sequential versions of the software system. One indicator of ATD, is
the average number of modified components per commit (ANMCC): a higher
ANMCC indicates more ATD in a software system. However, it is difficult and
sometimes impossible to calculate ANMCC, because the data (i.e., the log of commits)
are not always available or accessible. We proposed to use software modularity
metrics, which can be directly calculated based on source code, as a substitute of
ANMCC to indicate ATD. We validated the correlation between ANMCC and
modularity metrics through a holistic multiple case study on thirteen open source
software projects. The results of this study suggested that two modularity metrics,
namely Index of Package Changing Impact (IPCI) and Index of Package Goal Focus
(IPGF), have significant correlation with ANMCC, and therefore can be used as
alternative ATD indicators.
After ATD is identified and measured, the knowledge about ATD needs to be
explicitly documented thereby facilitating the rest of the activities in ATD
management. Existing work on ATD documentation is rather limited and it cannot
address all stakeholders concerns on ATD. We proposed six architecture viewpoints
related to ATD (ATD viewpoints in short). Each viewpoint frames a number of
concerns on ATD. All these concerns were systematically extracted from literature in
the aforementioned systematic mapping study on TD. The six ATD viewpoints
together help to get a comprehensive understanding of ATD in a software system,
thereby providing support for architecture decision-making. To evaluate the
effectiveness of the ATD viewpoints in documenting ATD, we conducted an
industrial case study in the same company where the aforementioned ATD
identification approach was validated. The case study results show that the
documented ATD views can effectively facilitate the documentation of ATD.
The aforementioned approaches for ATD management have their limitations while
they do not cover all the activities in the ATD management process. We explored the
application of knowledge-based approaches in software architecture through a
systematic mapping study, in order to improve the current management of ATD,
which is a type of architectural knowledge. We found that (1) the representations of
ATD and architecture in a formal form and further applying reasoning techniques
based on these formal representations can support ATD identification, measurement,
prioritization, and monitoring, and (2) the sharing and reuse of the knowledge on
ATD and related architectural knowledge can support ATD identification,
documentation, prioritization, repayment, and monitoring.},
 duplicado = {false},
 inserir = {false},
 title = {Managing technical debt in software architecture},
 year = {2015}
}

@article{42,
 abstract = {Abstract Software testing is an important activity in quality assurance and it generates large amount of knowledge. Software testers need to gather domain knowledge to be able to successfully conduct a software testing activity. Not having a proper knowledge base within its own context by software testing environments cause software testers to query limited knowledge available or consult peer software testers, which would greatly impact on their decision-making process. Ontologies emerge as one of the more appropriate knowledge management tools for supporting knowledge representation, processing, storage and retrieval. Given great importance to knowledge for software testing, and the potential benefits of managing software testing knowledge, using semantic web technologies, ontology based knowledge management system is developed. A Software testing knowledge sharing ontology is designed to describe software testing domain knowledge. SPARQL is used as the query language to retrieve software testing knowledge from the semantic storage. Both Ontology experts and non-experts evaluated the developed ontology. We believe our software testing ontology can support other software organizations to improve the sharing of knowledge and learning practices.},
 duplicado = {false},
 inserir = {true},
 title = {An Ontology-based Knowledge Management System for Software Testing},
 year = {2017}
}

@article{46,
 abstract = {Abstract: Context: In Web applications, the Software vulnerability can be reduced by applying security testing in all phases of the software development life cycle (SDLC). Lot of vulnerabilities might occur if the security testing is applied in the last phase of SDLC. In order to mitigate these vulnerabilities, a lot of rework is required that involves reverse engineering in the development and design phases. To overcome this situation, organizations are shifting from security testing (performed in last phase) towards security testing in the early phases of SDLC. Objectives: The main objectives of this thesis are to gather the benefits and challenges of security testing in the last phase versus security testing in every phase of the SDLC. After gathering, authors want to compare both implementations because these days most organizations are shifting from last phase to every phase of SDLC. Justification to the reason can be achieved by this comparison. Methods: In order to satisfy the objectives of this thesis, a literature review and interviews were conducted. The literature review was conducted by gathering benefits and challenges of last phase and every phase of SDLC. Authors have applied coding technique to the data gathered from literature review. By using the results from literature review, a set of questions were framed. Based on these questions, interviews in various organizations were performed. To analyze the practitioner's data we used Sorting and Coding technique. Then, we conducted a comparative analysis to compare both results. Results: Application of security testing in the last phase of the SDLC results in a lot of rework which in turn leads to instability in managing the cost, time and resources in an organisation. In order to overcome this, more and more organisations are introducing security testing at each and every phase of SDLC. Conclusions: It can be concluded that every phase of security testing in SDLC has more benefits than applying in last phase of SDLC. To evaluate this process more research is needed to acquire more knowledge of security testing in all phases of SDLC. Through literature review and interviews conducted, it is evident that security testing at early phases causes a reduction in rework which in turn leads to more efficient management of cost, time and resources of a project.},
 duplicado = {false},
 inserir = {false},
 title = {Security Testing for Web Applications in SDLC},
 year = {2011}
}

@article{47,
 abstract = {Abstract: Software test process improvement (STPI) approaches are frameworks that guide software development organizations to improve their software testing process. We have identified existing STPI approaches and their characteristics (such as completeness of development, availability of information and assessment instruments, and domain limitations of the approaches) using a systematic literature review (SLR). Furthermore, two selected approaches (TPI NEXT and TMMi) are evaluated with respect to their content and assessment results in industry. As a result of this study, we have identified 18 STPI approaches and their characteristics. A detailed comparison of the content of TPI NEXT and TMMi is done. We found that many of the STPI approaches do not provide sufficient information or the approaches do not include assessment instruments. This makes it difficult to apply many approaches in industry. Greater similarities were found between TPI NEXT and TMMi and fewer differences. We conclude that numerous STPI approaches are available but not all are generally applicable for industry. One major difference between available approaches is their model representation. Even though the applied approaches generally show strong similarities, differences in the assessment results arise due to their different model representations.},
 duplicado = {false},
 inserir = {false},
 title = {Software test process improvement approaches: A systematic literature review and an industrial case study},
 year = {2016}
}

@article{49,
 abstract = {Evolutions are happening in everyday life including teaching and learning process. With the used of e-learning, it was an innovative technology that was implemented in many different fields regarding to its benefits. This paper presents motivation assessment models that were used for constructivism learning theory. It also proposed for learning programming through an e-learning. This learning theory hopefully will help to improve level of motivation among novice. },
 duplicado = {false},
 inserir = {false},
 title = {Motivation Assessment Model for Constructivism Learning},
 year = {2013}
}

@article{5,
 abstract = {Abstract: Software testing is a complex and critical process for achieving product quality. Its importance has been increasing and well recognized, and there is a growing concern in improving the accomplishment of this process. In this context, Knowledge Management (KM) emerged as an important supporting approach to improve the software testing process. However, managing relevant testing knowledge requires effective means to represent and to associate semantics to a large volume of testing information. To address this concern, we have developed a Reference Ontology on Software Testing (ROoST). ROoST establishes a common conceptualization about the software testing domain, which can serve several KM-related purposes, such as defining a common vocabulary for knowledge workers with respect to the testing domain, structuring testing knowledge repositories, annotating testing knowledge items, and for making search for relevant information easier. In this paper, we present ROoST, and we discuss how it was developed using two ontology pattern languages. Moreover, we discuss how we evaluated ROoST following four complementary approaches: assessment by humans, data-driven evaluation, ontology testing, and application-based evaluation.},
 duplicado = {false},
 inserir = {true},
 title = {ROoST: Reference Ontology on Software Testing},
 year = {2017}
}

@article{50,
 abstract = {Abstract Computer programming is known for its complexity and difficulty among novice. Developing good computer programming skills requires students to do a lot of exercises. Besides, high self motivation is the only types of person that required in performing better in programming development. In this research, an exploration for PDCA cycle from manufacturing area will be transform into computer programming learning for continuous process improvements. Expected results from this study will reflect to the research questions. This paper contributed on the motivations of applying TQM to the programming earning using Problem-based learning through web-based environment. Therefore, we believe that constructive development through PBL and web-based programming learning tends to support cognitive development among novice.},
 duplicado = {false},
 inserir = {false},
 title = {A Framework for Learning Programming Using TQM},
 year = {2012}
}

@article{51,
 abstract = {Abstract: Evolutions are happening in teaching and learning process with the used of e-learning. It was an innovative technology that was implemented in many different fields regarding to its benefits. This paper presents a constructivism learning theory for programming through an e-learning with the assumption that constructivism is an important branch of cognitivism. This learning theory that proposed in learning programming hopefully will help to improve level of motivation among novice.},
 duplicado = {false},
 inserir = {false},
 title = {Constructivism learning theory for programming through an e-learning},
 year = {2012}
}

@article{52,
 abstract = {Context: Test Process Improvement (TPI) approaches are frameworks or models that guide software development organizations to investigate, assess and improve their software testing process. Objectives: We extend existing work in the area of Test Process Improvement by identifying available approaches and by evaluating them in regards to their characteristics. Furthermore, two selected approaches are evaluated with respect to their content and assessment results. Methods: In the first part of this study we use a systematic literature review to identify the existing TPI approaches which are then used in the second part of the study. The second part of the study is an industrial case study in which two TPI approaches are applied in an industrial setting. Results: We contribute in providing (1) a complete, in our opinion, list of 16 existing TPI approaches and their characteristics, (2) a detailed comparison of the content and the results of the two applied approaches (TPI Next and TMMi) and (3) experience in applying them in industry. As a result of this research we found that the content as well as the assessment results of the two approaches are similar to a great extent. Conclusions: Numerous Test Process Improvement approaches are available, but not all are generally applicable for industry. One major difference between available approaches is their model representation. Even though, the applied approaches generally show strong similarities, differences in the assessment results are noticeable due to their different model representations.},
 duplicado = {false},
 inserir = {false},
 title = {Evaluation of Test Process Improvement approaches: An industrial case study},
 year = {2015}
}

@article{53,
 abstract = {Software Framework is a universal software platform in software application. A framework proposes to provide generic functionality of software. Best practice of framework will be used in very software application. A specific software application changes a framework and reuses it. With test framework improves the reusability of test environment. This paper reports a survey of recent research to test framework. These present in tow category: functional testing and nonfunctional testing. Functional testing is in unit testing frameworks, integration testing, regression testing and system testing. Some of researches present categorize to automatically test framework, these will be needed to research will report.},
 duplicado = {false},
 inserir = {true},
 title = {A Survey of Test Framework},
 year = {2014}
}

@article{57,
 abstract = {Background/Objectives: Research in recent years has probed integration amongst research field of Software Engineering & Semantic Web technology, addressing the advantages of applying Semantic techniques to the field of Software Engineering. Prolifically published studies have further substantiated the benefits of ontologies to the field of Software Engineering, which clearly motivate us to explore further opportunities available in this collaborated field. This paper is a survey expounding such opportunities while discussing the role of ontologies as a Software Life-Cycle support technology. Method/Statistical Analysis: Survey centred on providing an overview of the state-of-art of all the ontologies available for Software Engineering followed by their categorization based on software life cycle phases and their application scope. Findings: Characterization of ontologies as a Software Life-cycle support technology, instigated by the increasing need to investigate the interplay between Semantic Web & Software Engineering with the ultimate goal of enabling & improving Software Engineering capabilities. Application/Improvements: This paper discusses the practical and potential applications of ontologies in the field of Software Engineering followed by the issues and challenges that will keep this field dynamic and lively for years to come.},
 duplicado = {false},
 inserir = {false},
 title = {Ontologies for Software Engineering: Past, Present and Future},
 year = {2016}
}

@article{58,
 abstract = {Abstract: Software testing is a critical process for achieving product quality. Its importance is more and more recognized, and there is a growing concern in improving the accomplishment of this process. In this context, Knowledge Management emerges as an important supporting tool. However, managing relevant knowledge to reuse is difficult and it requires some means to represent and to associate semantics to a large volume of test information. In order to address this problem, we have developed a Reference Ontology on Software Testing (ROost). ROost is built reusing ontology patterns from the Software Process Ontology Pattern Language (SP-OPL). In this paper, we discuss how ROost was developed, and present a fragment of Roost that concerns with software testing process, its activities, artifacts, and procedures.},
 duplicado = {false},
 inserir = {true},
 title = {Using Ontology Patterns for Building a Reference Software Testing Ontology},
 year = {2013}
}

@article{59,
 abstract = {Software test is a technique to obtain information about software systems quality. Performance test is a type of software test that aims at evaluating software performance at a given load scenario, but it requires specialized knowledge about tools, activities and metrics of the domain. Since ontology is a promising knowledge representation technique, this paper presents a literature review to identify trends and compare researches of ontologies in the fields of software testing and software performance. Also, to investigate this issue from a practical perspective, it was developed an ontology for representing the core knowledge of performance testing. This paper presents the ontology and compare it with related ones. Then, semantic technologies are explored to demonstrate the practical feasibility of developing ontology-based applications for assisting testers with performance test planning and management.},
 duplicado = {false},
 inserir = {true},
 title = {An Ontology for Guiding Performance Testing},
 year = {2014}
}

@article{6,
 abstract = {Globalization has had a lot of impact on the management of various organizations, and the
health care sector is not exempt. In the era of knowledge economy, management of
knowledge has become a significant tool for enhancing the competitiveness of firms. The
Aga Khan University Hospital, Nairobi is the tertiary, teaching and referral healthcare facility
in Kenya. Their approach to care is guided by their core principles of Quality, Access, Impact
and Relevance. Knowledge management at The AKUH is primarily facilitated by the
University hospital library and The Continuous Medical Education (CME) department. The
libraries provide the university community access to comprehensive and multi-disciplinary
information resources in print and digital formats. This access is provided through innovative
services and state-of-the-art systems. Despite all the above there still exist an empirical gap
in the knowledge management practices of the healthcare institution.
This study investigated the knowledge management practices implemented in the university
hospital. Simple random sampling technique was applied to select a sample size of
respondents picked from the university hospital setting. The research applied both
quantitative and qualitative data analysis. Out of 201 questionnaires distributed, 188
responded giving a response rate of 93%. 8 of the respondents were section heads and 10
were program directors, 64 were consultants, 28 were registrars, 24 were senior house
officers and 54 were residents. Data was collected from the respondents using structured
questionnaires. For qualitative study, semi structured interviews were conducted on 13 senior
managers out of which 8 were chairs of different clinical departments,1ICT officer, 1
regional librarian and 1 head librarian, 1 dean of the medical college and 1 CME coordinator
to make a total of thirteen giving a response rate of 81%. Descriptive and causal research
designs were used in analysis of the data. The design was ideal in describing the
characteristics of the large targeted sample used in the study.
The results of the study established that the use of Electronic medical records, Continuous
medical education, Communities of Practice, Knowledge cafes, and Web based system are
the key knowledge management practices that are in place in the university hospital. The
leadership role played by top management emerged as being a key facilitator of knowledge
management practices in the healthcare facility. The results from this study will inform the
hospital management on the knowledge management practices in the hospital, suggest that
knowledge management practices directly influence the performance of clinical staff.
Key recommendations from the study were that an alignment of knowledge management
policy to the organizational strategy would act as guideline on how knowledge should be
disseminated within the organization, secondly the role of leadership in managing KM
activities in terms of management support is key and finally, incentive programs which
reward knowledge sharing are important so as to encourage employees to actively share
knowledge both in the departments and across department knowledge sharing. Further
research efforts can investigate the impact of social media in the improving implementation
of knowledge management within organizations or between sector partners. The study was
limited to the Aga Khan University Hospital which is a private health care facility, further
studies can be carried out in public health care facilities.},
 duplicado = {false},
 inserir = {false},
 title = {An investigation of clinical knowledge management practices at the Aga Khan University Hospital, Nairobi},
 year = {2016}
}

@article{60,
 abstract = {Abstract. Ontologies have been widely recognized as an important instrument for supporting Knowledge Management (KM). In order to look for a domain ontology that can be used in KM in software testing, in this paper, we investigate, by means of a Systematic Literature Review (SLR), ontologies in the software testing domain, including questions related to their coverage of the software testing domain, and how they were developed.},
 duplicado = {false},
 inserir = {false},
 title = {Ontologies in Software Testing: A Systematic Literature Review },
 year = {2013}
}

@article{62,
 abstract = {Abstract: To solve the trustworthiness of reusable test cases, a trustworthiness framework of reusable test cases is proposed in this paper, including the analysis of trustworthiness attributes and trustworthiness evidence. Furthermore, the trustworthiness assurance processes related are presented to support management activities of reusable test case.},
 duplicado = {false},
 inserir = {true},
 title = {Trustworthiness framework of reusable test case},
 year = {2013}
}

@article{64,
 abstract = {Abstract: With the advent of Component-based software engineering (CBSE), large software systems are being built by integrating pre-built software components. The Semantic Web in association with CBSE has shown to offer powerful representation facilities and reasoning techniques to enhance and support querying, reasoning, discovery, etc. of software components. The goal of this paper is to research the applicability of Semantic Web technologies in performing the various tasks of CBSE and review the experimental results of the same in an easy and effective manner. To the best of our knowledge, this is the first study which provides an extensive review of the application of Semantic Web in CBSE from different perspectives. A systematic literature review of the Semantic Web approaches, employed for use in CBSE, reported from 2001 until 2015, is conducted in this research article. Empirical results have been drawn through the question-answer based analysis of the research, which clearly tells the year wise trend of the research articles, with the possible justification of the usage of Semantic Web technology and tools for a particular phase of CBSE. To conclude, gaps in the current research and potential future prospects have been discussed.},
 duplicado = {false},
 inserir = {false},
 title = {Software component and the semantic Web: An in-depth content analysis and integration history},
 year = {2017}
}

@article{67,
 abstract = {Abstract: Reuse is an important mechanism to increase productivity and to reduce time and costs during software development. Although source code is the most commonly reusable asset, other types of assets can also be reused, such as requirements, business processes, analysis and design models, etc. In this context, it is important that the knowledge about reusable assets and its management are available to potential stakeholders. This work presents the development of an ontology of reusable assets specification and management, named ONTO-ResAsset. This ontology is evaluated under two points-of-view: domain experts and non-experts.},
 duplicado = {false},
 inserir = {false},
 title = {ONTO-ResAsset Development: An Ontology for Reusable Assets Specification and Management},
 year = {2014}
}

@article{7,
 abstract = {This systematic mapping study investigates the modeling and automatic code generation initiatives for wireless sensor network applications based on the IEEE 802.15.4 standard, trying to understand the reasons, characteristics and methods used in the approaches available in the scientific literature, identifying research gaps and potential approaches that can be better exploited, indicating new possibilities of research. The focus is on studies that follow the Model-Driven or Business Process approaches.},
 duplicado = {false},
 inserir = {false},
 title = {Modeling and automatic code generation for wireless sensor network applications using model-driven or business process approaches: A systematic mapping study},
 year = {2017}
}

@article{71,
 abstract = {Abstract - With the growth of data from several different sources of knowledge within an organization, it becomes necessary to provide computerized support for tasks of acquiring, processing, analyzing and disseminating knowledge. In the software process, testing is a critical factor for product quality, and thus there is an increasing concern in how to improve the accomplishment of this task. In software testing, finding relevant knowledge to reuse can be a difficult and complex task, due to the lack of a strategy to represent or to associate semantics to a large volume of test data, including test cases, testing techniques to be applied and so on. This paper aims to investigate, through a Systematic Mapping of the Literature, some aspects associated with applying Knowledge Management to Software Testing.},
 duplicado = {false},
 inserir = {false},
 title = {Knowledge Management Applied to Software Testing: A Systematic Mapping},
 year = {2013}
}

@article{72,
 abstract = {Abstract: The aim of this article is to introduce the survey results of the current state of testing and quality management in software companies in the Czech Republic. The article includes answers to questions about the view of quality management in these companies and their employees' level of knowledge of software testing as well as an analysis of these results and a proposal for solving the identified issues. Moreover it describes the current software testing education system and its characteristics as it has a direct impact on this perception.},
 duplicado = {false},
 inserir = {false},
 title = {The perception of software quality and testing in Czech software companies},
 year = {2012}
}

@article{73,
 abstract = {In this article an approach to process improvement is established through the validation process area of CMMI, by focusing on making a metamodel. Validation process area was considered specifically, SG1 prepare for validation. By the metamodel, a taxonomy of projects, the testing characterization, the testing templates for the product to validate, for the validation environment, for the testing procedures and criteria, and a test plan are defined. The focus was subjected to evaluation by a case study. The case study was conducted in the IT Department of a public higher education institution. The approach proved its validity, since testers feel that it gives them specific tests for the development of the validation process and allows them to prepare validation for a particular project.},
 duplicado = {false},
 inserir = {false},
 title = {Una aproximacion basada en metamodelado del area de proceso de Validacion del CMMI: Un caso de estudio},
 year = {2016}
}

@article{75,
 abstract = {Abstract : In this article an approach to process improvement is established through the validation process area of CMMI, by focusing on making a metamodel. Validation process area was considered specifically, SG1 prepare for validation. By the metamodel, a taxonomy of projects, the testing characterization, the testing templates for the product to validate, for the validation environment, for the testing procedures and criteria, and a test plan are defined. The focus was subjected to evaluation by a case study. The case study was conducted in the IT Department of a public higher education institution. The approach proved its validity, since testers feel that it gives them specific tests for the development of the validation process and allows them to prepare validation for a particular project. Keywords: CMMI; process improvement; metamodel; maturity model; software validation. En el presente articulo se establece una aproximacion para la mejora de procesos a traves del area de proceso de validacion del CMMI, mediante un enfoque basado en metamodelado. Se considero el area de proceso de Validacion, especificamente la meta SGi preparar la validacion. Mediante el metamodelo, se definen una taxonomia de proyectos, la caracterizacion de pruebas, plantillas de pruebas para el producto a validar, para el entorno de validacion, para los procedimientos y criterios de prueba, y un plan de pruebas. La aproximacion fue sometida a evaluacion, por medio de un caso de estudio. El caso de estudio se llevo a cabo en la Direccion de Informatica, de una institucion de educacion superior publica. La aproximacion demostro su validez, ya que, los probadores consideran que les aporta las pruebas especificas para el desarrollo del proceso de validacion y les permite preparar la validacion para un proyecto determinado. Palabras-clave: CMMI; mejora de procesos; metamodelo; modelo de madurez; validacion de software.},
 duplicado = {false},
 inserir = {false},
 title = {An approach based on metamodelling for the validation CMMI process area: a case study/Una aproximacion basada en metamodelado del area de proceso de validacion del CMMI: un caso de estudio},
 year = {2016}
}

@article{77,
 abstract = {This research has explored the relationship between system test complexity and tacit knowledge. It is proposed as part of this thesis, that the process of system testing (comprising of test planning, test development, test execution, test fault analysis, test measurement, and case management), is directly affected by both complexity associated with the system under test, and also by other sources of complexity, independent of the system under test, but related to the wider process of system testing. While a certain amount of knowledge related to the system under test is inherent, tacit in nature, and therefore difficult to make explicit, it has been found that a significant amount of knowledge relating to these other sources of complexity, can indeed be made explicit. While the importance of explicit knowledge has been reinforced by this research, there has been a lack of evidence to suggest that the availability of tacit knowledge to a test team is of any less importance to the process of system testing, when operating in a traditional software development environment. The sentiment was commonly expressed by participants, that even though a considerable amount of explicit knowledge relating to the system is freely available, that a good deal of knowledge relating to the system under test, which is demanded for effective system testing, is actually tacit in nature (approximately 60% of participants operating in a traditional development environment, and 60% of participants operating in an agile development environment, expressed similar sentiments). To cater for the availability of tacit knowledge relating to the system under test, and indeed, both explicit and tacit knowledge required by system testing in general, an appropriate knowledge management structure needs to be in place. This would appear to be required, irrespective of the employed development methodology.},
 duplicado = {false},
 inserir = {true},
 title = {Exploration of the relationship between tacit knowledge and software system test complexity},
 year = {2016}
}

@article{8,
 abstract = {Abstract: Software testing is a major V&V activity that revolves around quality test cases. Generating quality test cases is inherently knowledge intensive, tedious and expensive task that is traditionally done by humans. Therefore, a great deal of research has been done to facilitate and automate test case generation as much as possible. Given the knowledge intensity of software test case generation, various knowledge management techniques, such as semantic-based techniques, are applicable. The main focus of our research is automatic generation of quality test cases using available knowledge from the early stages of software development, i.e. the RE. Our goal is to develop a semantic web enabled framework for integrating knowledge from various requirement models to generate effective and efficient test cases automatically. We are going to apply semantic technology to facilitate the test case generation process by means of ontologies and Web of Data.},
 duplicado = {false},
 inserir = {true},
 title = {Semantic-Based Test Case Generation},
 year = {2016}
}

@article{81,
 abstract = {Abstract: This report focuses on the design of a university knowledge management system (KMS), and how such a system can provide the framework for students and faculty to partake in, share, and collaborate on research activities. The collaborative environment of a university KMS, or research network, must be established in order for a university to maintain its reputation as a competitive research institution and to develop its faculty and students outside the classroom. A research network may contain a variety of features, some of which include faculty profiles, forums, communication portals, publication links, biographies, faculty keywords, interest areas and contact information. Several web-based systems provide the capabilities required for a well-functioning research network, including creating a customized system in-house. Thus, the existence of various alternatives often leads to great difficulty selecting, designing, and/or customizing a feasible system. The systems engineering process allows a university to evaluate and select the most desirable research network to suit its needs while maintaining decision objectivity. Once a system is selected, the university must integrate the research network into its existing research organization through a structured implementation plan, which includes system implementation schedule, functionality, required resources, and cost analysis. In addition, this report includes a case study performed at California Polytechnic State University, San Luis Obispo (Cal Poly). In this case study, the KMS design and systems engineering process are applied to Cal Poly, in an attempt to satisfy Cal Poly's need for a robust research network. An implementation plan developed for Cal Poly is presented, as well.},
 duplicado = {false},
 inserir = {false},
 title = {DESIGN OF A UNIVERSITY RESEARCH NETWORK: ANALYSIS, SELECTION, AND IMPLEMENTATION PLANNING},
 year = {2011}
}

@article{83,
 abstract = {Abstract: Context: New technologies such as social networks, wikis, blogs and other social software enable collaborative work and are important facilitators of the learning process. They provide a simple mechanism for people to communicate and collaborate and thus support the creation of knowledge. In software-development companies they are used to creating an environment in which communication and collaboration between workers take place more effectively. Objective: This paper identifies the main tools and technologies used by software-development companies in Brazil to manage knowledge and attempts to determine how these tools and technologies relate to important knowledge-sharing and learning theories and how they support the concepts described by these theories. Method: A survey was conducted in a group of Brazilian software development companies with high levels of process software maturity to see how they implement the Brazilian Software Processes Improvement model (MPS.Br) and use new tools and technologies. The survey used a qualitative analysis to identify which tools are used most and how frequently employees use them. The results of the analysis were compared with data from the literature on three knowledge-sharing and learning theories to understand how the use of these tools relates to the concepts proposed in these theories. Results:The results show that some of the tools used by the companies do not apply the concepts described in the theories as they do not help promote organizational learning. Furthermore, although the companies have adopted the tools, these are not often used, mainly because they are felt not to organize information efficiently. Conclusion: The use of certain tools can help promote several concepts described in the theories considered. Moreover, the use of these tools can help reduce the impact of, some common organizational problems. However, companies need to improve existing organizational policies that encourage employees to use these tools more regularly.},
 duplicado = {false},
 inserir = {false},
 title = {Old theories, New technologies: Understanding knowledge sharing and learning in Brazilian software development companies},
 year = {2015}
}

@article{84,
 abstract = {Organizational learning assists the companies to improve significantly their processes by means of experiences reuse, making knowledge accessible to the whole organization. In software engineering it is important that the acquired knowledge is stored and systematically reused. This paper aims to present a systematic review, by identifying in which software engineering areas are the organizational learning studies concentrated, and how the organizational learning concepts are being applied in software engineering. This systematic review identified 2496 papers. After eliminating the duplicate titles and those not related to the review, 1184 papers remained. Applying the exclusion criteria, the number of papers was reduced to 68. These papers were analyzed and classified according to the software engineering areas defined in the SWEBOK, and the main organizational learning theories and techniques. It was observed that many software engineering researches apply organizational learning concepts without being aware of it.},
 duplicado = {false},
 inserir = {false},
 title = {ORGANIZATIONAL LEARNING APPLIED TO SOFTWARE ENGINEERING: A SYSTEMATIC REVIEW},
 year = {2013}
}

@article{86,
 abstract = {Abstract: A Decision Support System (DSS) based on Knowledge Discovery from Data (KDD) process is used to give confident knowledge to the final users in order to help them making right decisions. Such systems can be underused if the mined knowledge is unconfident, or if the system is hardly usable or unusable. Our target is to supply out a Quality Model (QM) ensuring a global evaluation of DSS based on KDD process (DSS/KDD). In our point of view, a QM should involve three dimensions: the evaluation of the DSS as a Software Product, as a User Interface and as a DSS. We should also take into account ISO recommendations. We intend to build a model which defines a set of criteria and allows measurement of a DSS/KDD quality evaluation using Goal-Question Method (GQM) and Analytic Hierarchy Process (AHP).},
 duplicado = {false},
 inserir = {false},
 title = {Towards a quality model for the evaluation of DSS based on KDD process},
 year = {2013}
}

@article{88,
 abstract = {Test cases generation based on Finite State Machines (FSMs) has been addressed for quite some time. Model-based testing has drawn attention from researchers and practitioners as one of the approaches to support software verification and validation. Several test criteria have been proposed in the literature to generate test cases based on formal methods, such as FSM. However, there is still a lot to be done on this aspect in order to clearly direct a test designer to choose a test criterion most suitable to generate test cases for a certain application domain. This work presents a new test criterion for model-based test case generation based on FSM, H-Switch Cover. H-Switch Cover relies on the traditional Switch Cover test criterion, but H-Switch Cover uses new heuristics to improve its performance, for example, adoption of rules to optimize graph balancing and traverse the graph for test cases generation. We conducted an investigation of cost and efficiency of this new test criterion by comparing it with unique input/output and distinguishing sequence. We used two embedded software products (space application software products) and mutation analysis for assessing efficiency. In general, for the case studies proposed in this paper in terms of cost (amount of events) and efficiency (mutation score), H-Switch Cover test criterion presented an average and a standard deviation better than the other two test criteria.},
 duplicado = {false},
 inserir = {false},
 title = {H-Switch Cover: a new test criterion to generate test case from finite state machines},
 year = {2017}
}

@article{89,
 abstract = {Providing final users with confident knowledge to help them make the right decisions is the goal of using a decision support system based on a knowledge discovery from data process (DSS/KDD). Some failures can be found in such systems especially when the mined knowledge is unconfident, or if the system is hardly usable. The objective of this study is to define a quality model (QM) which ensures a global evaluation of DSS/KDD that generates association rules. The proposed QM evaluates the DSS/KDD regarding three dimensions: utility, usability and interestingness. It defines a set of criteria and allows the measurement of a DSS/KDD quality evaluation. To validate the proposed approach, a prototype has been developed. Weka and a DSS/KDD in the healthcare domain were assessed drawing on 20 users who participated in the evaluation process. Results have shown that a user-centred QM leads to a better quality of such systems.},
 duplicado = {false},
 inserir = {false},
 title = {A quality model for the evaluation of decision support systems based on a knowledge discovery from data process},
 year = {2016}
}

@article{9,
 abstract = {Abstract: Context: Software testing is a knowledge intensive process and the use of Knowledge Management (KM) methods and principles makes software testing even more beneficial. Thus there is a need of adapting KM into software testing core process and attain the benefits that it provides in terms of cost, quality etc. There has been an extensive literature published in the context of KM in software testing. But it is still unclear about the importance of KM with respect to testing techniques as well as testing aspects i.e. each activity that takes part during testing and the outcomes that they result such as test artifacts is considered as testing aspect. Thus there is a requisite for studies to focus on identifying the challenges faced due to lack of KM along with the importance of KM with respect to testing aspects, testing techniques and thus can provide recommendations to apply Knowledge Management to those that get benefited from it. Objectives: In this thesis, we investigate the usage and implementation of KM in Software testing. The major objectives of current thesis include, To identify various software testing aspects that receive more attention while applying KM.
To analyze the software testing techniques i.e. test design, test execution and test result analysis and evaluate them and highlight which of these have more involvement of KM.
To identify the software testing techniques where tacit or explicit knowledge is currently used.
To gather challenges faced by industry due to lack of KM initiatives in software testing. Methods: We conducted a Systematic Literature Review (SLR) through a snowballing method based on the guidelines from Wohlin in order to identify various software testing aspects and testing techniques that have more involvement of KM and challenges that are faced due to lack of KM. A questionnaire intended for web-based survey was prepared from the gathered literature results to complement and further supplement them and to categorize the testing techniques based on the type of knowledge they utilize. The studies were analyzed in relation to their rigor and relevance to assess the quality of the results. The data obtained from survey were statistically analyzed using descriptive statistics and Chi-square test of significance. Results: We identified 35 peer reviewed papers among which 31 were primary and 4 were secondary studies. The literature review results indicated 9 testing aspects being in focus when applying KM within various adaptation contexts. In addition, few testing techniques were found to get benefited from the application of KM. Several challenges were identified from the literature review such as improper selection and application of better suited techniques, low reuse rate of Software Testing knowledge, barriers in Software testing knowledge transfer, impossible to quickly achieve the most optimum distribution of human resources during testing etc. 54 full answers were received to the survey. The survey showed that Knowledge Management was being applied in software testing in most of the industries. It was observed that test result analysis, test case design, test planning and testing techniques stood out as the most important testing aspects being focused while KM is applied. Regarding software testing techniques, 17 test design techniques, 5 test execution techniques and 5 test result analysis techniques gain more attention in the context of KM. Moreover, the results suggest that tacit knowledge was utilized for most of these techniques. Several new challenges are obtained from the survey such as lacking quality in terms of testing results or outcomes, difficulties in finding relevant information and resources during testing, applying more effort than required during testing, having a huge loss of know-how by neglecting explicit and tacit knowledge during test design etc. Conclusions. To conclude, various challenges are being faced due to the lack of KM. Our study also brings supporting evidence that applying KM in Software Testing is necessary i.e. to increase test effectiveness, selection and application of better suited techniques and so on. It was also observed that perceptions vary between the literature and the survey results obtained from the practitioners regarding testing aspects and testing techniques, as few aspects and techniques which are being categorized as the most important in the literature are not given the same priority by the respondents. Thus the final list of testing aspects and testing techniques is provided and empirical findings can likewise help practitioners to specifically apply KM more for those that are very much in need of it. Besides, it was found that most of the techniques require and utilize tacit knowledge to apply them and techniques such as shadowing, observing, training and recording sessions can help to store tacit knowledge for those that are in need of it. Thus researchers can recognize the advantages from this thesis and can further extend to various software life cycle models.},
 duplicado = {false},
 inserir = {true},
 title = {Knowledge Management in Software Testing},
 year = {2009}
}

@article{91,
 abstract = {Abstract: Software development is a highly knowledge intensive activity. During the software life cycle, knowledge and experiences are accumulated over time. One of the main knowledge sources in software development is lessons learned. Even though lessons learned have been a common practice for process improvement, however, there are arguments that lessons learned are not used effectively and organizations continuously fail to learn from past projects. In this paper, we propose utilizing lessons learned by transforming it into an experience base incorporated with software engineering life cycle for the purpose of sharing and future reuse. An initial model is formulated based on literature analysis and a preliminary study. The goal of the study is to assess the experts' perception on the model formulation in terms of its importance in each software development phase, and which knowledge element is more valuable as future reuse. Additionally, results from the study show that such model can bring positive impact to individuals as well as organizations.},
 duplicado = {false},
 inserir = {false},
 title = {Towards developing lessons learned and experience based factory in software development},
 year = {2015}
}

@article{93,
 abstract = {Abstract: Context: Software testing practices and processes in many companies are far from being mature and are usually conducted in ad-hoc fashions. Such immature practices lead to various negative outcomes, e.g., ineffectiveness of testing practices in detecting all the defects, and cost and schedule overruns of testing activities. To conduct test maturity assessment (TMA) and test process improvement (TPI) in a systematic manner, various TMA/TPI models and approaches have been proposed. Objective: It is important to identify the state-of-the-art and the practice in this area to consolidate the list of all various test maturity models proposed by practitioners and researchers, the drivers of TMA/TPI, the associated challenges and the benefits and results of TMA/TPI. Our article aims to benefit the readers (both practitioners and researchers) by providing the most comprehensive survey of the area, to this date, in assessing and improving the maturity of test processes. Method: To achieve the above objective, we have performed a Multivocal Literature Review (MLR) study to find out what we know about TMA/TPI. A MLR is a form of a Systematic Literature Review (SLR) which includes the grey literature (e.g., blog posts and white papers) in addition to the published (formal) literature (e.g., journal and conference papers). We searched the academic literature using the Google Scholar and the grey literature using the regular Google search engine. Results: Our MLR and its results are based on 181 sources, 51 (29%) of which were grey literature and 130 (71%) were formally published sources. By summarizing what we know about TMA/TPI, our review identified 58 different test maturity models and a large number of sources with varying degrees of empirical evidence on this topic. We also conducted qualitative analysis (coding) to synthesize the drivers, challenges and benefits of TMA/TPI from the primary sources. Conclusion: We show that current maturity models and techniques in TMA/TPI provides reasonable advice for industry and the research community. We suggest directions for follow-up work, e.g., using the findings of this MLR in industry-academia collaborative projects and empirical evaluation of models and techniques in the area of TMA/TPI as reported in this article.},
 duplicado = {false},
 inserir = {false},
 title = {Software test maturity assessment and test process improvement: A multivocal literature review},
 year = {2017}
}

@article{94,
 abstract = {Abstract: High software quality is a very important outcome of software development practices for business customers (Mairiza, Zowghi, & Nurmuliani, 2010). This annotated bibliography is developed for software testers who want to improve the quality of software and customer satisfaction in the Agile development cycle. Selected references published between 2006 to 2013 are reviewed to examine software quality requirements, appropriate amounts of software tester readiness, test planning, verification of business test cases, and additional testing activities.},
 duplicado = {false},
 inserir = {false},
 title = {The Role of Testers in an Agile Software Development Life Cycle within B2B Companies},
 year = {2013}
}

@article{97,
 abstract = {The adoption of innovative Software Engineering (SE) processes by an organization implies that engineers have to learn new processes which they might not be familiar with. Social software can support and enhance this adoption process, so research needs to focus on how the exchange of knowledge among software engineers using these tools can help to perform training more effectively. We propose a framework based on social software to support the collaborative learning, adoption and improvement of SE processes through the exchange of experiences among individuals. This article examines factors influencing the adoption of new SE processes and the quality of the experiences shared using the proposed framework in comparison with similar ones. Two case studies were carried out involving junior engineers in a training course on agile software development. Anonymous surveys collected data on the perceived quality of the experiences shared during the research, their usefulness, and the simplicity of the mechanisms provided to contribute experiences. Results show that the adoption of new SE processes is influenced by several factors such as the commitment of software engineers to collaborate in the adoption of the new process, the perceived level of usefulness of the tacit knowledge elicited during the adoption process, the diversity of the topics covered by the shared knowledge, the simplicity of the mechanisms to contribute new tacit knowledge, and the amount of learning achieved by software engineers.},
 duplicado = {false},
 inserir = {false},
 title = {Study of Factors Influencing the Adoption of Agile Processes When Using Wikis},
 year = {2014}
}

