@article{1848,
 abstract = {We analyze studies that aim to simplify code generation for WSN applications.The predominant approaches used were MDE (74%), BPM (19%) and BPM4WSN (7%).Few studies addressed any type of high-level aspect in their code generation process.There were few studies concerned with energy consumption or SOA support.Automatic Reprogramming support was not a consideration in any of studies analyzed. This systematic mapping study investigates the modeling and automatic code generation initiatives for wireless sensor network applications based on the IEEE 802.15.4 standard, trying to understand the reasons, characteristics and methods used in the approaches available in the scientific literature, identifying research gaps and potential approaches that can be better exploited, indicating new possibilities of research. The focus is on studies that follow the Model-Driven or Business Process approaches. Display Omitted},
 duplicado = {false},
 inserir = {false},
 title = {Modeling and automatic code generation for wireless sensor network applications using model-driven or business process approaches: A systematic mapping study},
 year = {2017}
}

@article{1849,
 abstract = {Context: concept Maps (CMs) enable the creation of a schematic representation of a domain knowledge. For this reason, CMs have been applied in different research areas, including Computer Science. Objective: the objective of this paper is to present the results of a systematic mapping study conducted to collect and evaluate existing research on CMs initiatives in Computer Science. Method: the mapping study was performed by searching five electronic databases. We also performed backward snowballing and manual search to find publications of researchers and research groups that accomplished these studies. Results: from the mapping study, we identified 108 studies addressing CMs initiatives in different subareas of Computer Science that were reviewed to extract relevant information to answer a set of research questions. The mapping shows an increasing interest in the topic in recent years and it has been extensively investigated due to support in teaching and learning. Conclusions: based on our results we conclude that the use of CMs as an educational tool has been widely accepted in Computer Science.},
 duplicado = {false},
 inserir = {false},
 title = {Analyzing the Use of Concept Maps in Computer Science: A Systematic Mapping Study},
 year = {2017}
}

@article{1850,
 abstract = {The aim of the paper is to summarize the experience gained from the research and the constructions of the Knowledge Management Systems (KMS) based on the ontology engineering. Computational ontologies are the means to formally model the structure of a system, to discover the entities and relations that emerge from its observation, and which are useful to the system purposes. In our projects, we utilize the Topic Maps Theory and the tools based on this technology (e.g. ATOM2). The key point of the solution is the ontology development; our methodology procedure is presented; the improvement of the methodology and the future research goals are oriented to automate the procedure steps. During the process, the text mining tools (e.g. TOVEK) and other analytical tools are used. At the end of the paper, the summary of the experience with the methodology application is presented.},
 duplicado = {false},
 inserir = {false},
 title = {Knowledge Management Systems Based on Topic Maps Theory},
 year = {2017}
}

@article{1855,
 abstract = {Test cases generation based on Finite State Machines (FSMs) has been addressed for quite some time. Model-based testing has drawn attention from researchers and practitioners as one of the approaches to support software verification and validation. Several test criteria have been proposed in the literature to generate test cases based on formal methods, such as FSM. However, there is still a lot to be done on this aspect in order to clearly direct a test designer to choose a test criterion most suitable to generate test cases for a certain application domain. This work presents a new test criterion for model-based test case generation based on FSM, H-Switch Cover. H-Switch Cover relies on the traditional Switch Cover test criterion, but H-Switch Cover uses new heuristics to improve its performance, for example, adoption of rules to optimize graph balancing and traverse the graph for test cases generation. We conducted an investigation of cost and efficiency of this new test criterion by comparing it with unique input/output and distinguishing sequence. We used two embedded software products (space application software products) and mutation analysis for assessing efficiency. In general, for the case studies proposed in this paper in terms of cost (amount of events) and efficiency (mutation score), H-Switch Cover test criterion presented an average and a standard deviation better than the other two test criteria.},
 duplicado = {false},
 inserir = {false},
 title = {H-Switch Cover: a new test criterion to generate test case from finite state machines},
 year = {2017}
}

@article{1856,
 abstract = {Abstract
Context

Software testing practices and processes in many companies are far from being mature and are usually conducted in ad-hoc fashions. Such immature practices lead to various negative outcomes, e.g., ineffectiveness of testing practices in detecting all the defects, and cost and schedule overruns of testing activities. To conduct test maturity assessment (TMA) and test process improvement (TPI) in a systematic manner, various TMA/TPI models and approaches have been proposed.

Objective

It is important to identify the state-of-the-art and the �practice in this area to consolidate the list of all various test maturity models proposed by practitioners and researchers, the drivers of TMA/TPI, the associated challenges and the benefits and results of TMA/TPI. Our article aims to benefit the readers (both practitioners and researchers) by providing the most comprehensive survey of the area, to this date, in assessing and improving the maturity of test processes.

Method

To achieve the above objective, we have performed a Multivocal Literature Review (MLR) study to find out what we know about TMA/TPI. A MLR is a form of a Systematic Literature Review (SLR) which includes the grey literature (e.g., blog posts and white papers) in addition to the published (formal) literature (e.g., journal and conference papers). We searched the academic literature using the Google Scholar and the grey literature using the regular Google search engine.

Results

Our MLR and its results are based on 181 sources, 51 (29%) of which were grey literature and 130 (71%) were formally published sources. By summarizing what we know about TMA/TPI, our review identified 58 different test maturity models and a large number of sources with varying degrees of empirical evidence on this topic. We also conducted qualitative analysis (coding) to synthesize the drivers, challenges and benefits of TMA/TPI from the primary sources.

Conclusion

We show that current maturity models and techniques in TMA/TPI provides reasonable advice for industry and the research community. We suggest directions for follow-up work, e.g., using the findings of this MLR in industry-academia collaborative projects and empirical evaluation of models and techniques in the area of TMA/TPI as reported in this article.},
 duplicado = {false},
 inserir = {false},
 title = {Software test maturity assessment and test process improvement: A multivocal literature review},
 year = {2017}
}

@article{1858,
 abstract = {Abstract
Providing final users with confident knowledge to help them make the right decisions is the goal of using a decision support system based on a knowledge discovery from data process (DSS/KDD). Some failures can be found in such systems especially when the mined knowledge is unconfident, or if the system is hardly usable. The objective of this study is to define a quality model (QM) which ensures a global evaluation of DSS/KDD that generates association rules. The proposed QM evaluates the DSS/KDD regarding three dimensions: utility, usability and interestingness. It defines a set of criteria and allows the measurement of a DSS/KDD quality evaluation. To validate the proposed approach, a prototype has been developed. Weka and a DSS/KDD in the healthcare domain were assessed drawing on 20 users who participated in the evaluation process. Results have shown that a user-centred QM leads to a better quality of such systems.},
 duplicado = {false},
 inserir = {false},
 title = {A quality model for the evaluation of decision support systems based on a knowledge discovery from data process},
 year = {2016}
}

@article{1859,
 abstract = {Abstract:
Software development is a highly knowledge intensive activity. During the software life cycle, knowledge and experiences are accumulated over time. One of the main knowledge sources in software development is lessons learned. Even though lessons learned have been a common practice for process improvement, however, there are arguments that lessons learned are not used effectively and organizations continuously fail to learn from past projects. In this paper, we propose utilizing lessons learned by transforming it into an experience base incorporated with software engineering life cycle for the purpose of sharing and future reuse. An initial model is formulated based on literature analysis and a preliminary study. The goal of the study is to assess the experts' perception on the model formulation in terms of its importance in each software development phase, and which knowledge element is more valuable as future reuse. Additionally, results from the study show that such model can bring positive impact to individuals as well as organizations.},
 duplicado = {false},
 inserir = {false},
 title = {Towards developing lessons learned and experience based factory in software development},
 year = {2015}
}

@article{1861,
 abstract = {Abstract
Numerous advancements in semantic web technologies initiate the new research area in Software Engineering (SE) applications. Various methods, tools available to create the systematic, disciplined structure of software. SE contains three processes, namely, construction, testing and maintenance. The systematic creation of meaningful software by using coding, debugging and verification refers to construction. The maintenance process analyzes and modifies the software products to improve the performance of attributes. The crucial step in between construction and maintenance is software testing. A process that identifies the bugs, errors and defects in software refer testing. This paper presents the detailed survey of the testing process and influence of testing in SE applications. Also, this highlights the cloud deployment, clustering, generation/selection/prioritization of test cases in automated software testing. The survey addresses the testing order of test cases during grouping process in distributed server testing. Moreover, the case selection for the complete testing process is the complex task. Clustering mechanism improves the grouping processes based on dependency analysis in test case generation and selection. This paper also addresses the prediction of testing sequence based on dependency information and fitness function through test prioritization techniques. This survey focuses how to create an optimal distributed test bed for real world software applications. },
 duplicado = {false},
 inserir = {false},
 title = {A survey: Optimal approaches for distributed software testing},
 year = {2014}
}

@article{1862,
 abstract = {Abstract
Context

New technologies such as social networks, wikis, blogs and other social software enable collaborative work and are important facilitators of the learning process. They provide a simple mechanism for people to communicate and collaborate and thus support the creation of knowledge. In software-development companies they are used to creating an environment in which communication and collaboration between workers take place more effectively.

Objective

This paper identifies the main tools and technologies used by software-development companies in Brazil to manage knowledge and attempts to determine how these tools and technologies relate to important knowledge-sharing and learning theories and how they support the concepts described by these theories.

Method

A survey was conducted in a group of Brazilian software development companies with high levels of process software maturity to see how they implement the Brazilian Software Processes Improvement model (MPS.Br) and use new tools and technologies. The survey used a qualitative analysis to identify which tools are used most and how frequently employees use them. The results of the analysis were compared with data from the literature on three knowledge-sharing and learning theories to understand how the use of these tools relates to the concepts proposed in these theories.

Results

The results show that some of the tools used by the companies do not apply the concepts described in the theories as they do not help promote organizational learning. Furthermore, although the companies have adopted the tools, these are not often used, mainly because they are felt not to organize information efficiently.

Conclusion

The use of certain tools can help promote several concepts described in the theories considered. Moreover, the use of these tools can help reduce the impact of, some common organizational problems. However, companies need to improve existing organizational policies that encourage employees to use these tools more regularly.},
 duplicado = {false},
 inserir = {false},
 title = {Old theories, New technologies: Understanding knowledge sharing and learning in Brazilian software development companies},
 year = {2015}
}

@article{1863,
 abstract = {The adoption of innovative Software Engineering (SE) processes by an organization implies that engineers have to learn new processes which they might not be familiar with. Social software can support and enhance this adoption process, so research needs to focus on how the exchange of knowledge among software engineers using these tools can help to perform training more effectively. We propose a framework based on social software to support the collaborative learning, adoption and improvement of SE processes through the exchange of experiences among individuals. This article examines factors influencing the adoption of new SE processes and the quality of the experiences shared using the proposed framework in comparison with similar ones. Two case studies were carried out involving junior engineers in a training course on agile software development. Anonymous surveys collected data on the perceived quality of the experiences shared during the research, their usefulness, and the simplicity of the mechanisms provided to contribute experiences. Results show that the adoption of new SE processes is influenced by several factors such as the commitment of software engineers to collaborate in the adoption of the new process, the perceived level of usefulness of the tacit knowledge elicited during the adoption process, the diversity of the topics covered by the shared knowledge, the simplicity of the mechanisms to contribute new tacit knowledge, and the amount of learning achieved by software engineers.},
 duplicado = {false},
 inserir = {false},
 title = {Study of factors influencing the adoption of agile processes when using Wikis},
 year = {2014}
}

@article{1864,
 abstract = {Abstract:
A Decision Support System (DSS) based on Knowledge Discovery from Data (KDD) process is used to give confident knowledge to the final users in order to help them making right decisions. Such systems can be underused if the mined knowledge is unconfident, or if the system is hardly usable or unusable. Our target is to supply out a Quality Model (QM) ensuring a global evaluation of DSS based on KDD process (DSS/KDD). In our point of view, a QM should involve three dimensions: the evaluation of the DSS as a Software Product, as a User Interface and as a DSS. We should also take into account ISO recommendations. We intend to build a model which defines a set of criteria and allows measurement of a DSS/KDD quality evaluation using Goal-Question Method (GQM) and Analytic Hierarchy Process (AHP).},
 duplicado = {false},
 inserir = {false},
 title = {Towards a quality model for the evaluation of DSS based on KDD process},
 year = {2013}
}

@article{1865,
 abstract = {Abstract - With the growth of data from several different sources
of knowledge within an organization, it becomes necessary to
provide computerized support for tasks of acquiring, processing,
analyzing and disseminating knowledge. In the software process,
testing is a critical factor for product quality, and thus there is an
increasing concern in how to improve the accomplishment of this
task. In software testing, finding relevant knowledge to reuse can
be a difficult and complex task, due to the lack of a strategy to
represent or to associate semantics to a large volume of test data,
including test cases, testing techniques to be applied and so on.
This paper aims to investigate, through a Systematic Mapping of
the Literature, some aspects associated with applying Knowledge
Management to Software Testing.},
 duplicado = {false},
 inserir = {false},
 title = {Knowledge Management Applied to Software Testing: A Systematic Mapping},
 year = {2013}
}

@article{1866,
 abstract = {Abstract
Ontologies have been widely recognized as an important instrument for supporting Knowledge Management (KM). In order to look for a domain ontology that can be used in KM in software testing, in this paper, we investigate, by means of a Systematic Literature Review (SLR), ontologies in the software testing domain, including questions related to their coverage of the software testing domain, and how they were developed},
 duplicado = {false},
 inserir = {false},
 title = {Ontologies in software testing: A Systematic Literature Review},
 year = {2013}
}

@article{1867,
 abstract = {Abstract�The present study aims to discuss the role of the information technology (IT) components (hardware, software and network) in knowledge management for product design. Data were collected through questionnaires, which were distributed 220 questionnaires to industrial companies in China. Simple regression analyses were used to determine the relationship between hardware and knowledge management , software and knowledge management, network and knowledge management, knowledge management and product design , also Multiple regression analyses were used to determine the relationship among hardware, software, network, knowledge management and product design, Results of the statistical analysis showed that the components of information management, knowledge management, and product design had a significant positive correlation, which suggests a strong relationship. Moreover, information management components, which support knowledge management, plays a vital role in product design to achieve competitive advantage. Therefore, companies must use information and knowledge management in obtaining their objectives to achieve a competitive advantage in their product design and be consistent with customer requirements to achieve consumer satisfaction. Product design does not only involve field product, and operations management or simple management participation. The foundation of product design the proper use of IT components are valuable knowledge.},
 duplicado = {false},
 inserir = {false},
 title = {Information Technology Components and Their Role in Knowledge Management for Product Design},
 year = {2017}
}

@article{1869,
 abstract = {This paper seeks to contribute to the on-going research in knowledge management (KM) by presenting a study conducted in six public service agencies in Singapore. The study was guided by three research foci, namely, (1) to elucidate the nebulous nature of KM initiatives, (2) to uncover the motivation behind KM measurement and (3) to identify the various elements of a KM initiative that can be measured. Data collected from the public service agencies revealed that KM initiatives were generally top-down and technology-focused. Project management and the need to quantify the value of KM initiatives drove KM measurement. The measurement indicators adopted by the agencies encompassed four elements of measurement: activities, knowledge assets, organizational processes and business outcomes. In conclusion, this paper highlights two practical implications for the design of a KM measurement regime and suggests a number of possible directions for further research.},
 duplicado = {false},
 inserir = {false},
 title = {Untying the knot of knowledge management measurement: a study of six public service agencies in Singapore},
 year = {2008}
}

@article{1870,
 abstract = {Abstract
This chapter discusses the basic properties of corporate Wikis that make them an effective learning and knowledge management tool. Wikis offer a user-friendly environment that enhances informal knowledge sharing and the collaborative creation of new knowledge. Enterprise-wide adoption of Wikis promotes the reuse of existing know-hows and prevents employee re-invention of the wheel. Four cases of successful implementations of Wikis in large, hi-tech global organizations are described in detail including their goals, design considerations, implementation and actual use for formal and informal knowledge creation and sharing. The adoption and long-term sustainability of Wikis is attributed to perceived business outcomes by managers and to perceived usefulness and ease of use by individual contributors and users. Good practices based on one or more of these use-cases can provide practical guidance to organizations that wish to use a Wiki for KM purposes.},
 duplicado = {false},
 inserir = {false},
 title = {Principles and Good Practices for Using Wikis within Organizations},
 year = {2017}
}

@article{1871,
 abstract = {Professional services represent a specific type of business service, one that is strongly based on the knowledge of individual professionals and organizational processes to harness that knowledge. Modularization can help in coordinating creation of the offering and enhancing the cost-effectiveness of processes. However, utilizing modularity in services, and particularly in professional service firms, is still a field that deserves specific studies. While the product modularity literature has developed definitions, metrics, and a number of hypotheses about modularity, in the field of professional services, we still have to clarify how firms can use modularity.

While professional services are often generated from expert-embedded and even tacit knowledge, which is hard to transfer, implementing modularity may offer one way to facilitate knowledge sharing related to service offerings, organizational processes, and practices. However, when considering the strategic importance of modularity for a professional service firm, it is also necessary to define what the characteristics are in the knowledge base of the organization. Not all professional service firms are alike. The purpose of the present study is to examine how service modularity can be employed within the context of professional services. More specifically, (1) what knowledge-related characteristics vary between different types of professional service firms? (2) How do these characteristics influence modularization implementation? Two company cases are analyzed to reveal how modularity can be implemented in varying professional service firm environments. },
 duplicado = {false},
 inserir = {false},
 title = {Implementing Modularization in Professional Services The Influence of Varied Knowledge Environments},
 year = {2017}
}

@article{1872,
 abstract = {Purpose
� Knowledge is a main resource of any organization. Knowledge management (KM) is identified by four processes: creating, capturing, distributing and sharing of knowledge. Technology can enable successful KM. The purpose of this paper is to propose a technology knowledge management (TKM) taxonomy, which lists popular electronic tools that can enhance KM processes and shows which tool can contribute to which processes.

Design/methodology/approach
� The taxonomy was developed by an extensive literature review of electronic KM tools and a three-year extensive analysis of different knowledge sources at the Jeddah Municipality (JM) in Saudi Arabia.

Findings
� The taxonomy can be used by practitioners developing an organizational KM system to guide them to choose a sufficient subset of tools that covers all four processes in order to ensure that no process is overlooked.

Research limitations/implications
� The result of using the TKM taxonomy and its effect on KM success is an interesting area for further research. However, the current value underlies in it offering practitioners a rough roadmap to an electronic KM system and aids in giving at least a starting point.

Practical implications
� The TKM taxonomy can be used by large scale organizations to guide in developing a KM system effectively and more efficiently. Furthermore, the JM KC is a good model for similar organizations to use, with all the tools explained in the paper.

Social implications
� The paper addresses some of the social elements related to successful KM in organizations. However, it is more technically targeted.

Originality/value
� Researchers have investigated either the holistic effect of IT on KM or described certain tools. The types of IT tools and their effect on KM have not been investigated. Furthermore, limited research addresses the design of effective KM systems and no tools exist to guide designers. The TKM taxonomy is a tool that can help KM practitioners and strategists to design effective KM systems efficiently, by guiding them in choosing tools that are suitable for certain KM processes. The paper also describes the JM Knowledge Center as a KMS model for organizations which addresses all four KM processes.},
 duplicado = {false},
 inserir = {false},
 title = {Technology knowledge management (TKM) taxonomy: Using technology to manage knowledge in a Saudi municipality},
 year = {2014}
}

@article{1874,
 abstract = {
Purpose
� The knowledge of inhibitors of internal customer knowledge transfer in b?to?b professional service organizations is still in its infancy. Previous literature on professional service organizations has focused on knowledge processes on a general level without paying closer attention to inhibitors of internal knowledge transfer. This study aims to contribute by increasing the knowledge of various inhibitors of customer?related knowledge transfer and their influence on customer?related knowledge utilization in collaborative customer relationships.

Design/methodology/approach
� The present empirical article is based on a case study of two professional service organizations in the field of business?to?business education and consultancy services. An in?depth analysis of organizations developing collaborative relationships was conducted.

Findings
� This paper shows that internal fragmentation seems to be inherent in this type of organization, and may cause many problems in customer?related knowledge transfer among individuals, collegial groups and hierarchical levels in a professional service organization. All these problems in collective knowledge utilization influence both the service offering creation and general relationship coordination in the collaborative relationship.

Originality/value
� This paper provides managerial suggestions for how to deal with the inhibitors of customer knowledge transfer. This includes developing unified goals, strengthening cultural cohesion and cooperation in the organization, building forums of dialogue between individuals and subgroups, and structuring relationship coordination systems (i.e. key account management systems), keeping customer?related knowledge transfer in mind.},
 duplicado = {false},
 inserir = {false},
 title = {Loose coupling as an inhibitor of internal customer knowledge transfer: findings from an empirical study in B?to?B professional services},
 year = {2008}
}

@article{1875,
 abstract = {Abstract
This article is based on a case study of a professional service organisation in the field of business-to-business education and consultancy services. This study contributes by increasing the knowledge of organisational inhibitors of customer knowledge utilisation in collaborative customer relationships by describing four organisational aspects inhibiting internal customer knowledge utilisation. The first is a professional service organisation's dominant logic, which refers to a barrier between organisation and customer. The second relates to cultural characteristics, referring to barriers between individuals and groups. The third barrier is the organisational structure of the professional service firm and the fourth barrier relates to systems and administrative routines.},
 duplicado = {false},
 inserir = {false},
 title = {What prevents effective utilisation of customer knowledge in professional B-to-B services? An empirical study},
 year = {2008}
}

@article{1876,
 abstract = {Abstract

The aim of this paper is to identify the benefits of an improved Knowledge Management (KM) strategy for a small/medium-sized professional services firm. The research examined how the three epistemological views (cognitivistic, connectionistic and autopoietic) influenced the management and use of organizational knowledge via the company intranet.

The Intranet Evaluation Model (Skok and Kalmanovitch, 2005) was selected to evaluate the company's KM strategy and was extended to emphasize group, cultural and external aspects. Knowledge Evaluation Maps were used to present findings in a revealing graphical display. The results indicated that the company's KM strategy should focus on the high-value tacit knowledge of experts, develop the intranet in a connectionistic form, build upon existing knowledge activities and initiate more group activities. A knowledge steward should be appointed to motivate management and staff, and to overcome possible barriers to management buy-in and a knowledge silo culture.

Furthermore, it was found that for small/medium-sized professional service firms, knowledge can be used as the enabler of competitive advantage but that the epistemologies within the firm should be considered when evaluating a KM strategy.},
 duplicado = {false},
 inserir = {false},
 title = {Managing organizational knowledge: developing a strategy for a professional services company},
 year = {2007}
}

@article{1877,
 abstract = {
Purpose
� Effective customer?specific knowledge transfer is the cornerstone of customer value creation in professional service organizations. In order to formulate a coherent service offering across different expertise areas, it is crucial to share customer?specific knowledge between professionals, business functions and units. The purpose of this study is to offer insights into the role of key account management (KAM) systems in facilitating this process.

Design/methodology/approach
� The work is based on an explorative case study in which the implementation of the KAM system in two consulting and training companies was investigated. Comparison of the two cases in terms of KAM design and success in knowledge transfer enabled conclusions to be drawn about the role of KAM as a knowledge carrier and a �linking pin� in a loosely coupled organization.

Findings
� Organizational fragmentation and insufficient communication channels among experts and subgroups of professional organizations cause problems in relation to knowledge transfer. This also makes it more difficult to combine expertise and to create innovative service concepts for customers. A KAM system, if managed effectively, provides a powerful tool for counteracting these problems. It functions as a linking pin in a loosely coupled organization, helping to maintain customer?specific knowledge transfer and continuity in customer relationships.

Originality/value
� Very little research has been conducted on customer?specific knowledge transfer in professional service organizations in spite of its central role in the creation of customer value. This study is unique in offering empirical evidence of the role of KAM systems in facilitating knowledge transfer. In the future, it would be interesting to study the role of different organizational conditions and practices, including organizational structures, the use of technological knowledge tools and cooperative working methods. The effectiveness of KAM systems in terms of financial performance and the creation of value for clients also deserve more research attention.},
 duplicado = {false},
 inserir = {false},
 title = {Customer knowledge transfer and key account management in professional service organizations},
 year = {2006}
}

@article{1878,
 abstract = {Abstract:
Empirical Software Engineering research has achieved considerable results in building our knowledge about selecting and applying appropriate empirical methods for technology evaluation. Empirical studies in general and empirical studies in industrial settings in particular have played an important role in successful transition of many Software Engineering technologies to industry, for example, defect detection techniques and automated test cases. However, conducting empirical research in industrial settings remains a challenging undertaking for a variety of reasons. There is no substantial literature reporting on the challenges and complexities involved in conducting empirical studies in an industry in general and in settings whose business models are built around global sourcing. This paper reports some of our experiences and lessons learned from conducting empirical research in industry. Some of the observed challenges include short time horizon for research, high expectations, limited research skills, and the `acceptable' research rigor. The paper discusses some of these issues with relevant examples and provides some strategies for overcoming these issues. We also stress that researchers and practitioners should share their experiences of conducting empirical research in order to help build a body of knowledge to guide the future efforts.},
 duplicado = {false},
 inserir = {false},
 title = {Conducting empirical studies in industry: Balancing rigor and relevance},
 year = {2013}
}

@article{1880,
 abstract = {Abstract
In the last 15 years, software architecture has emerged as an important software engineering field for managing the development and maintenance of large, software-intensive systems. Software architecture community has developed numerous methods, techniques, and tools to support the architecture process (analysis, design, and review). Historically, most advances in software architecture have been driven by talented people and industrial experience, but there is now a growing need to systematically gather empirical evidence about the advantages or otherwise of tools and methods rather than just rely on promotional anecdotes or rhetoric. The aim of this paper is to promote and facilitate the application of the empirical paradigm to software architecture. To this end, we describe the challenges and lessons learned when assessing software architecture research that used controlled experiments, replications, expert opinion, systematic literature reviews, observational studies, and surveys. Our research will support the emergence of a body of knowledge consisting of the more widely-accepted and well-formed software architecture theories.},
 duplicado = {false},
 inserir = {false},
 title = {Applying empirical software engineering to software architecture: challenges and lessons learned},
 year = {2010}
}

@article{1881,
 abstract = {Abstract
Concurrent ting is a challenging activity due to factors that are not present in sequential programs, such as communication, synchronization, and non-determinism, and that directly affect the testing process. When we consider multithreaded programs, new challenges for the testing activity are imposed. In the context of structural testing, an important problem raised is how to deal with the coverage of shared variables in order to establish the association between def-use of shared variables. This paper presents results related to the structural testing of multithreaded programs, including testing criteria for coverage testing, a supporting tool, called ValiPthread testing tool and results of an experimental study. This study was conducted to evaluate the cost, effectiveness, and strength of the testing criteria. Also, the study evaluates the contribution of these testing criteria to test specific aspects of multithreaded programs. The experimental results show evidence that the testing criteria present lower cost and higher effectiveness when revealing some kinds of defects, such as deadlock and critical region block. Also, compared to sequential testing criteria, the proposed criteria show that it is important to establish specific coverage testing for multithreaded programs.},
 duplicado = {false},
 inserir = {false},
 title = {Contributions for the structural testing of multithreaded programs: coverage criteria, testing tool, and experimental evaluation},
 year = {2018}
}

@article{1882,
 abstract = {Abstract:
The technical literature regarding Model-based Testing (MBT) has several techniques with different characteristics and goals available to be applied in software projects. Besides the lack of information regarding these techniques, they could be applied together in a software project aiming at improving the testing coverage. However, this decision needs to be carefully analyzed to avoid loss of resources in a software project. Based on this scenario, this paper proposes an approach with the purpose of supporting the unique or combined selection of MBT techniques for a given software project considering two aspects: the adequacy level between MBT techniques and the software project characteristics and impact of more than one MBT technique in some testing process variables. At the end, preliminary results of an experimental evaluation are presented.},
 duplicado = {false},
 inserir = {false},
 title = {Porantim: An approach to support the combination and selection of Model-based Testing techniques},
 year = {2009}
}

@article{1883,
 abstract = {Abstract:
Numerous software verification and validation (V&V) techniques and tools exist to analyse requirements, designs and implementations of software systems. These V&V technologies range from relatively lightweight ones, such as inspection and testing, to more heavyweight technologies based on formal methods and theorem proving. For complex systems, a significant part of the cost and effort for development and maintenance is associated with V&V activities, and this almost always involves selecting and applying a mix of V&V technologies. Unfortunately, little is known about the cost-effectiveness of individual technologies or how to derive the most cost-effective combination. As such, combinations for particular projects are typically selected in an ad-hoc way. In this paper, several issues related to the selection and evaluation of combinations of V&V technologies are explored, based on personal experiences with the V&V of concurrent Java components. A V&V method is presented that was systematically derived through an analysis of the possible failures that can occur in concurrent Java components. This method combines inspection, static analysis, and dynamic testing. In addition, empirical methods that use analysis of fault data and experiments to evaluate V&V combinations are presented. Finally, ideas are presented for an iterative method that can be used to assist with both the selection and evaluation of cost-effective V&V combinations in a given context.},
 duplicado = {false},
 inserir = {false},
 title = {Selecting V&V Technology Combinations: How to Pick a Winner?},
 year = {2003}
}

@article{1884,
 abstract = {Abstract:
Testing is a very essential activity for software development. It is the act of executing a software product in order to validate whether it behaves as intended and identify possible malfunctions. Studies have shown that testing makes up more than 50% of the development cost. Besides, much effort and emphasis have now been placed on tasks related to automation with the purpose of reducing cost and the participation of the human element in software testing activities. Testing is still a human-based activity. Therefore, efficient ways of testing software products for quality assurance will require a better and more comprehensive understanding of testers' feelings, perceptions and motivations, in this paper referred to as Tester Experience (TX). Thus, the better the tester's experience during the software testing process, the better the result. TX can be defined as a means of capturing how testers think and feel about their activities within the software testing environment, with the assumption that an improvement of the tester's experience has positive impact on quality assurance. This paper motivates the importance of TX, highlights related approaches from other domains, proposes a definition based on similar concepts in other domains, and proposes future research activities.},
 duplicado = {false},
 inserir = {false},
 title = {Tester Experience: Concept, Issues and Definition},
 year = {2017}
}

@article{1885,
 abstract = {High-Performance Computing (HPC) applications consist of concurrent programs with multi-process and/or multithreaded models with varying degrees of parallelism. Although their design patterns, models, and principles are similar to those of sequential ones, their non-deterministic behavior makes the testing activity more complex. In an attempt to solve such complexity, several techniques for concurrent software testing have been developed over the past years. However, the transference of knowledge between academy and industry remains a challenge, mainly due to the lack of a solid base of evidence with information that assists the decision-making process. This paper proposes the construction of a body of evidence for the concurrent programming field that supports the selection of an adequate testing technique for a software project. We propose a characterization schema which assists the decision-making support and is based on relevant information from the technical literature regarding available techniques, attributes, and concepts of concurrent programming that affect the testing process. The schema classified 109 studies that compose the preliminary body of evidence. A survey was conducted with specialists for the validation of the schema, regarding adequacy and relevance of the attributes defined. The results indicate the schema is effective and can support testing teams for concurrent applications.},
 duplicado = {false},
 inserir = {true},
 title = {How to test your concurrent software: an approach for the selection of testing techniques},
 year = {2017}
}

@article{1886,
 abstract = {Abstract:
The rapid development of verification and validation (V&V) has resulted in a multitude of V&V technologies, making V&V selection difficult for practitioners. Since most V&V technologies will be combined it is important to be aware of how they should be combined and the cost-effectiveness of these combinations. This paper presents a strategy for selecting and evaluating particular V&V combinations that focuses on maximising completeness and minimising effort. The strategy includes a systematic approach for applying empirical information regarding the costs and capabilities of V&V technologies.},
 duplicado = {false},
 inserir = {false},
 title = {An Iterative Empirical Strategy for the Systematic Selection of a Combination of Verification and Validation Technologies},
 year = {2007}
}

@article{1887,
 abstract = {Abstract:
This paper describes the Java Unit Testing Tool Competition that ran in the context of the Search Based Software Testing (SBST) workshop at ICST 2013. It describes the main objective of the benchmark, the Java classes that were selected, the data that was collected, the tools that were used for data collection, the protocol that was carried out to execute the benchmark and how the final benchmark score for each participating tool can be calculated.},
 duplicado = {false},
 inserir = {false},
 title = {Unit Testing Tool Competition},
 year = {2015}
}

@article{1888,
 abstract = {Abstract:
This paper presents a framework to instantiate software technologies selection approaches by using search techniques. The software technologies selection problem (STSP) is modeled as a Combinatorial Optimization problem aiming attending different real scenarios in Software Engineering. The proposed framework works as a top-level layer over generic optimization frameworks that implement a high number of metaheuristics proposed in the technical literature, such as JMetal and OPT4J. It aims supporting software engineers that are not able to use optimization frameworks during a software project due to short deadlines and limited resources or skills. The framework was evaluated in a case study of a complex real-world software engineering scenario. This scenario was modeled as the STSP and some experiments were executed with different metaheuristics using the proposed framework. The results indicate its feasibility as support to the selection of software technologies.},
 duplicado = {false},
 inserir = {false},
 title = {A Framework to Support the Selection of Software Technologies by Search-Based Strategy},
 year = {2014}
}

@article{1890,
 abstract = {Abstract

Defining organization-specific process standards by integrating, harmonizing, and standardizing heterogeneous and often implicit processes is an important task, especially for large development organizations. On the one hand, such a standard must be generic enough to cover all of the organization's development activities; on the other hand, it must be as detailed and precise as possible to support employees' daily work. Today, organizations typically maintain and advance a plethora of individual processes, each addressing specific problems. This requires enormous effort, which could be spent more efficiently. This article introduces an approach for developing a Software Process Line that, similar to a Software Product Line, promises to reduce the complexity and thus, the effort required for managing the processes of a software organization. We propose Scoping, Modeling, and Architecting the Software Process Line as major steps, and describe in detail the Scoping approach we recommend, based on an analysis of the potential products to be produced in the future, the projects expected in the future, and the respective process capabilities needed. In addition, the article sketches experience from determining the scope of space process standards for satellite software development. Finally, it discusses the approach, draws conclusions, and gives an outlook on future work. },
 duplicado = {false},
 inserir = {false},
 title = {Scoping software process lines},
 year = {2009}
}

@article{1891,
 abstract = {Abstract:
There exists a real need in industry to have guidelines on what testing techniques use for different testing objectives, and how usable (effective, efficient, satisfactory) these techniques are. Up to date, these guidelines do not exist. Such guidelines could be obtained by doing secondary studies on a body of evidence consisting of case studies evaluating and comparing testing techniques and tools. However, such a body of evidence is also lacking. In this paper, we will make a first step towards creating such body of evidence by defining a general methodological evaluation framework that can simplify the design of case studies for comparing software testing tools, and make the results more precise, reliable, and easy to compare. Using this framework, (1) software testing practitioners can more easily define case studies through an instantiation of the framework, (2) results can be better compared since they are all executed according to a similar design, (3) the gap in existing work on methodological evaluation frameworks will be narrowed, and (4) a body of evidence will be initiated. By means of validating the framework, we will present successful applications of this methodological framework to various case studies for evaluating testing tools in an industrial environment with real objects and real subjects.},
 duplicado = {false},
 inserir = {false},
 title = {A Methodological Framework for Evaluating Software Testing Techniques and Tools},
 year = {2012}
}

@article{1892,
 abstract = {Abstract:
This paper describes the third round of the Java Unit Testing Tool Competition. This edition of the contest evaluates no less than seven automated testing tools! And, like during the second round, test suites written by human testers are also used for comparison. This paper contains the full results of the evaluation.},
 duplicado = {false},
 inserir = {false},
 title = {Unit Testing Tool Competition -- Round Three},
 year = {2015}
}

@article{1893,
 abstract = {Abstract:
Different quality assurance techniques can be applied in the software development lifecycle to improve the quality of a product. Today, no holistic approach for planning and customizing inspection and testing techniques in a given context exists. In this paper, we present a framework which consists of the main aspects necessary for a systematic planning of quality assurance activities in different development steps. We present the core elements of the framework: influence and variation factors, characteristics of techniques and a role concept. The framework is a first step towards a systematic planning approach for quality assurance, which has to be refined in future research. Based on the elements, we recommend an initial process how to apply the framework. With this, a basis is developed to support quality managers planning quality assurance techniques in a more systematic way.},
 duplicado = {false},
 inserir = {false},
 title = {A Comprehensive Planning Framework for Selecting and Customizing Quality Assurance Techniques},
 year = {2007}
}

@article{1894,
 abstract = {Abstract:
The selection of software technologies represents a risk factor to a software project. Therefore, using tailored software technologies to support this task can contribute to reduce the risk of inadequate choices made by software engineers. This paper presents the results of an experimental study conducted to evaluate if three dependent variables (selection time, % of correct choices, and used information) can be significantly affected by two different approaches to support the selection of Model-Based Testing techniques. This study consists of an external replication of an experiment conducted to evaluate two approaches for the selection of testing techniques. Therefore, we present the adaptations performed in the planning and design of the experiment and the challenges observed in conducting a study replication. The results indicated that the time spent to select MBT techniques and the percentage of right selections can be affected by the approach used to select MBT techniques for different software project categories.},
 duplicado = {false},
 inserir = {false},
 title = {Evaluation of {model-based} testing techniques selection approaches: An external replication},
 year = {2009}
}

@article{1895,
 abstract = {Abstract:
The technical literature on model-based testing (MBT) offers us several techniques with different characteristics and goals. Contemporary software projects usually need to make use of different software testing techniques. However, a lack of empirical information regarding their scalability and effectiveness is observed. It makes their application difficult in real projects, increasing the technical difficulties to combine two or more MBT techniques for the same software project. In addition, current software testing selection approaches offer limited support for the combined selection of techniques. Therefore, this paper describes the conception and evaluation of an approach aimed at supporting the combined selection of MBT techniques for software projects. It consists of an evidence-based body of knowledge with 219 MBT techniques and their corresponding characteristics and a selection process that provides indicators on the level of adequacy (impact indicator) amongst MBT techniques and software projects characteristics. Results from the data analysis indicate it contributes to improve the effectiveness and efficiency of the selection process when compared to another selection approach available in the technical literature. Aiming at facilitating its use, a computerized infrastructure, evaluated into an industrial context and evolved to implement all the facilities needed to support such selection approach, is presented.},
 duplicado = {false},
 inserir = {false},
 title = {Supporting the Combined Selection of Model-Based Testing Techniques},
 year = {2014}
}

@article{1896,
 abstract = {Testing techniques have been widely used as a method to help software engineers in detecting defects in a software system in order to develop high-quality software system and achieve customer satisfaction. Different techniques reveal different quality aspects of a software system. This paper proposes a model-based methodology of the major accepted categories of testing techniques to evaluate many aspects like functional, structural, reliability, usability requirements and check their consistency. Evaluating all these aspects in a software project will help ensure the success of such software development project and will also assist software testers in error handling in order to achieve the desired quality for software customers.},
 duplicado = {false},
 inserir = {false},
 title = {A Generic Model-Based Methodology of Testing Techniques to Obtain High Quality Software},
 year = {2015}
}

@article{1897,
 abstract = {Abstract
Selecting software technologies for software projects represents a challenge to software engineers. It is known that software projects differ from each other by presenting different characteristics that can complicate the selection of such technologies. This is not different when considering model-based testing. There are many approaches with different characteristics described in the technical literature that can be used in software projects. However, there is no indication as to how they can fit a software project. Therefore, a strategy to select model-based testing approaches for software projects called Porantim is fully described in this paper. Porantim is based on a body of knowledge describing model-based testing approaches and their characterization attributes (identified by secondary and primary experimental studies), and a process to guide by adequacy and impact criteria regarding the use of this sort of software technology that can be used by software engineers to select model-based testing approaches for software projects.},
 duplicado = {false},
 inserir = {false},
 title = {Model-based testing approaches selection for software projects},
 year = {2009}
}

@article{1898,
 abstract = {Abstract:
The combination of testing techniques is considered an effective strategy to evaluate a software product. However, the selection of which techniques to combine in a software project has been an interesting challenge in the Software Engineering field. This paper presents a proposal extending an approach developed to support the combined selection of model-based testing (MBT) techniques, named Porantim, applying Multiobjective Combinatorial Optimization strategy by determining the smallest dominating set in a bipartite and weighted graph. Thus, a local search strategy algorithm is proposed generating solutions aiming at maximizing the coverage of software project characteristics and skills required by the testing team to use the techniques and minimizing the eventual effort to construct models used for test cases generation. A preliminary evaluation analyzes this new approach when compared to the Porantim's original version, and the results indicate improvements in the MBT techniques selection.},
 duplicado = {false},
 inserir = {false},
 title = {Porantim-Opt: Optimizing the Combined Selection of Model-Based Testing Techniques},
 year = {2011}
}

@article{1899,
 abstract = {Abstract:
Software testing is expensive for the industry, and always constrained by time and effort. Although there is a multitude of test techniques, there are currently no scientifically based guidelines for the selection of appropriate techniques of different domains and contexts. For large complex systems, some techniques are more efficient in finding failures than others and some are easier to apply than others are. From an industrial perspective, it is important to find the most effective and efficient test design technique that is possible to automate and apply. In this paper, we propose an experimental framework for comparison of test techniques with respect to efficiency, effectiveness and applicability. We also plan to evaluate ease of automation, which has not been addressed by previous studies. We highlight some of the problems of evaluating or comparing test techniques in an objective manner. We describe our planned process for this multi-phase experimental study. This includes presentation of some of the important measurements to be collected with the dual goals of analyzing the properties of the test technique, as well as validating our experimental framework},
 duplicado = {false},
 inserir = {false},
 title = {A Framework for Comparing Efficiency, Effectiveness and Applicability of Software Testing Techniques},
 year = {2006}
}

@article{1900,
 abstract = {Abstract
Experiments in computing share many characteristics with the traditional experimental method, but also present significant differences from a practical perspective, due to their aim at producing software artifacts and the central role played by human actors and organizations (e.g., programmers, project teams, software houses) involved in the software development process. By analyzing some of the most significant experiments in the subfield of software engineering, we aim at showing how the conceptual framework that supports experimental methodology in this context needs an extension in a socio-technical perspective},
 duplicado = {false},
 inserir = {false},
 title = {Rethinking Experiments in a Socio-Technical Perspective: The Case of Software Engineering},
 year = {2016}
}

@article{1901,
 abstract = {Abstract
Model-Based Testing (MBT) represents a feasible and interesting testing strategy where test cases are generated from formal models describing the software behavior/structure. The MBT field is continuously evolving, as it could be observed in the increasing number of MBT techniques published at the technical literature. However, there is still a gap between researches regarding MBT and its application in the software industry, mainly occasioned by the lack of information regarding the concepts, available techniques, and challenges in using this testing strategy in real software projects. This chapter presents information intended to support researchers and practitioners reducing this gap, consequently contributing to the transfer of this technology from the academia to the industry. It includes information regarding the concepts of MBT, characterization of 219 MBT available techniques, approaches supporting the selection of MBT techniques for software projects, risk factors that may influence the use of these techniques in the industry together with some mechanisms to mitigate their impact, and future perspectives regarding the MBT field.},
 duplicado = {false},
 inserir = {false},
 title = {A Picture from the Model-Based Testing Area: Concepts, Techniques, and Challenges},
 year = {2010}
}

@article{1902,
 abstract = {Abstract
Software testing techniques differ in the type of faults they are more prone to detect, and their performance varies depending on the features of the application being tested. Practitioners often use informally their knowledge about the software under test in order to combine testing techniques for maximizing the number of detected faults.

This work presents an approach to enable practitioners to select testing techniques according to the features of the software to test. A method to build a testing-related base of knowledge for tailoring the techniques selection process to the specific application(s) is proposed. The method grounds upon two basic steps: (i) constructing, on an empirical basis, models to characterize the software to test in terms of fault types it is more prone to contain; (ii) characterizing testing techniques with respect to fault types they are more prone to detect in the given context.
Using the created base of knowledge, engineers within an organization can define the mix of techniques so as to maximize the effectiveness of the testing process for their specific software.},
 duplicado = {false},
 inserir = {false},
 title = {Testing techniques selection based on ODC fault types and software metrics},
 year = {2013}
}

@article{1904,
 abstract = {Abstract:
Conducting verification and validation (V&V) of modeling and simulation (M&S) requires systematic and structured application of different V&V techniques throughout the M&S life cycle. Whether an existing technique is appropriate to a particular V&V activity depends not only on the characteristics of the technique but also on the situation where it will be applied. Although there already exit several guidance documents describing a variety of V&V techniques and their application potential, accessible findings or experiences on the effective selection of suitable V&V techniques for a given M&S context are still lacking. This paper presents: (1.) a characterization approach to developing a V&V techniques catalog that packages the available techniques together with the information about their application conditions; and (2.) a planning and tailoring strategy for project-specific selection of the appropriate V&V techniques from the established catalog according to the goals and characteristics of a simulation study.},
 duplicado = {false},
 inserir = {false},
 title = {Selecting verification and validation techniques for simulation projects: A planning and tailoring strategy},
 year = {2013}
}

@article{1905,
 abstract = {Abstract:
System testing based on a black box approach is a common industrial practice in information systems. Despite its widespread use, however, little guidance is available for testing engineers facing the problem of selecting the best test strategy. In previous work, we proposed to adopt functional models and related testing patterns according to the architectural style of the application under test. In this paper, we present an industrial study that applies this technique to system testing of repository based applications. We define a set of functional models abstracting different concerns of software applications: hierarchy of functions, business processes and states/transitions of application objects. The models are used to derive the functional test cases through the definition of test patterns. We applied this approach in an industrial context for over 5 years. In this paper, we analyze a data set of 37 test projects including about 22000 test cases and 1500 failures. We relate failures to the originating defect types. The study confirms that a system test strategy that uses multiple functional models according to the architectural style of the software application generates a better cost/benefit ratio than the use of just one model. The explanation is that - despite a small overlap - each model detects specific types of software defects. The results of the study can guide testing engineers in selecting the best system test strategy and significantly improve the efficiency of their work.},
 duplicado = {false},
 inserir = {false},
 title = {System Testing of Repository-Style Software: An Experience Report},
 year = {2016}
}

@article{1906,
 abstract = {Abstract
Knowledge and experience are important assets for software organizations. In today�s global software development trends, development teams are no longer located in single premise; they are spreading across national and geographic boundaries. As a software project progresses, more and more activities are involved which results with the accumulation of knowledge and experiences. Maintaining and reusing of past experiences are vital; and it is even more crucial for distributed teams. In order to sustain in today�s competitive advantages, organizations should prepare a well collaborative solution for managing software development knowledge and experiences to maximize sharing and future reuse. Numerous attempts have been invested by researchers to overcome the issues on knowledge management in software development; however, the emphasis on the actual experiences collected throughout the development phases is limited. Furthermore, there are not many solutions offering comprehensive collaborative solution for managing software development experiences. In this paper, we propose a model for managing software development experiences including its tacit and explicit knowledge based on experience factory approach. The model is adapted for cloud computing environment with the goal to provide efficient and effective collaborative solution for knowledge access, sharing and reuse by capitalizing the cloud�s resources and infrastructure. A systematic literature review has been conducted to investigate the current issues of knowledge management in software development and to analyze available approaches and solutions. The findings are quantitatively and qualitatively evaluated to support the model formulation. },
 duplicado = {false},
 inserir = {false},
 title = {Towards developing collaborative experience based factory model for software development process in cloud computing environment},
 year = {2015}
}

@article{1907,
 abstract = {Abstract:
This position paper argues that fault classification provides vital information for software analytics, and that machine learning techniques such as clustering can be applied to learn a project- (or organization-) specific fault taxonomy. Anecdotal evidence of this position is presented as well as possible areas of research for moving toward the posited goal.},
 duplicado = {false},
 inserir = {false},
 title = {Toward a learned project-specific fault taxonomy: application of software analytics},
 year = {2015}
}

@article{1908,
 abstract = {Abstract
Although the aim of empirical software engineering is to provide evidence for selecting the appropriate technology, it appears that there is a lack of recognition of this work in industry. Results from empirical research only rarely seem to find their way to company decision makers. If information relevant for software managers is provided in reports on experiments, such reports can be considered as a source of information for them when they are faced with making decisions about the selection of software engineering technologies. To bridge this communication gap between researchers and professionals, we propose characterizing the information needs of software managers in order to show empirical software engineering researchers which information is relevant for decision-making and thus enable them to make this information available. We empirically investigated decision makers� information needs to identify which information they need to judge the appropriateness and impact of a software technology. We empirically developed a model that characterizes these needs. To ensure that researchers provide relevant information when reporting results from experiments, we extended existing reporting guidelines accordingly. We performed an experiment to evaluate our model with regard to its effectiveness. Software managers who read an experiment report according to the proposed model judged the technology�s appropriateness significantly better than those reading a report about the same experiment that did not explicitly address their information needs. Our research shows that information regarding a technology, the context in which it is supposed to work, and most importantly, the impact of this technology on development costs and schedule as well as on product quality is crucial for decision makers.},
 duplicado = {false},
 inserir = {false},
 title = {Reporting experiments to satisfy professionals information needs},
 year = {2014}
}

@article{1909,
 abstract = {More and more software development organisations are paying attention to the improvement of the software testing process, because it is considered a key factor to ensure the quality of software products. However, the staff usually has problems developing testing activities because they do not have the appropriate competences to carry out these activities effectively; this results in low performance in organisations and increased costs of software products. A way to reduce this gap is developing a competence model that defines the roles who participate in the software testing activities as well as the general and technical competences required for them. Therefore this model could be applied to train staff in software testing activities and to recruit the appropriate profiles, which contribute to improving their performance. Considering that there is no published competence model specifically addressed to software testing, this study presents one that has been developed, analysing the literature and testing jobs and validated by experts in the software testing field using a survey as a validation method. So, as a result of this work, a competence model for software testing close to the software industry has been obtained.},
 duplicado = {false},
 inserir = {false},
 title = {Design of a competence model for testing teams},
 year = {2012}
}

@article{1910,
 abstract = {Software development is a data rich activity with many sophisticated metrics. Yet engineers often lack the tools and techniques necessary to leverage these potentially powerful information resources toward decision making. In this paper, we present the data and analysis needs of professional software engineers, which we identified among 110 developers and managers in a survey. We asked about their decision making process, their needs for artifacts and indicators, and scenarios in which they would use analytics.

The survey responses lead us to propose several guidelines for analytics tools in software development including: Engineers do not necessarily have much expertise in data analysis; thus tools should be easy to use, fast, and produce concise output. Engineers have diverse analysis needs and consider most indicators to be important; thus tools should at the same time support many different types of artifacts and many indicators. In addition, engineers want to drill down into data based on time, organizational structure, and system architecture.},
 duplicado = {false},
 inserir = {false},
 title = {Information needs for software development analytics},
 year = {2012}
}

@article{1911,
 abstract = {Abstract
In the development of many safety-critical systems, test cases are still created on the basis of experience rather than systematic methods. As a consequence, many redundant test cases are created and many aspects remain untested. One of the most important questions in testing dependable systems is: which are the right test techniques to obtain a test set that will detect critical errors in a complex system? In this paper, we provide an overview of the state-of-practice in designing test cases for dependable event-based systems regulated by the IEC 61508 and DO-178B standards. For example, the IEC 61508 standard stipulates modelbased testing and systematic test-case design and generation techniques such as transition-based testing and equivalence-class partitioning for software verification. However, it often remains unclear in which situation these techniques should be applied and what information is needed to select the right technique to obtain the best set of test cases. We propose an approach that selects appropriate test techniques by considering issues such as specification techniques, failure taxonomies and quality risks. We illustrate our findings with a case study for an interlocking system for Siemens transportation systems.},
 duplicado = {false},
 inserir = {false},
 title = {Testing of safety-critical systems - A structural approach to test case design},
 year = {2011}
}

@article{1912,
 abstract = {Abstract:
We propose a set of functional test patterns for testing a class of repository-style information systems. This class, which we call �workflow-based�, implements not only functions reading and writing from/to the database, but also workflows (business processes) that may be constrained by states of application objects. Patterns are based on models (functional abstractions) of the software product. The testing strategy combines testing of different models (hierarchy of functions, models of business processes and state/transition automata) using different testing techniques for each type of model. We discuss the patterns application to a large health care software product. We analyze the results (about 2700 test cases) and the effect of testing multiple patterns on the discovered failures and types of faults. We also describe the use of the method for testing about thirty software applications of an information system of a large retail company.},
 duplicado = {false},
 inserir = {false},
 title = {Functional abstractions for testing repository-style information systems},
 year = {2009}
}

@article{1913,
 abstract = {Abstract:
Practitioners report that experience plays an important role in effective software testing. We investigate the role of experience in a multiple case study about three successful projects conducted at Siemens Austria and document the state of practice in testing software systems. The studied projects were employed from the domains telecommunications, insurance and banking, as well as safety-critical railway systems. The study shows that test design is to a considerable extent based on experience in all three projects and that experience-based testing is an important supplementary approach to requirements-based testing. The study further analyzes the different sources of experience, the perceived value of experience for testing, and the measures taken to manage and evolve this experience.},
 duplicado = {false},
 inserir = {true},
 title = {The role of experience in software testing practice},
 year = {2008}
}

@article{1914,
 abstract = {Software engineering comprehends several disciplines devoted to prevent and remedy malfunctions and to warrant adequate behaviour. Testing, the subject of this paper, is a widespread validation approach in industry, but it is still largely ad hoc, expensive, and unpredictably effective. Indeed, software testing is a broad term encompassing a variety of activities along the development cycle and beyond, aimed at different goals. Hence, software testing research faces a collection of challenges. A consistent roadmap of the most relevant challenges to be addressed is here proposed. In it, the starting point is constituted by some important past achievements, while the destination consists of four identified goals to which research ultimately tends, but which remain as unreachable as dreams. The routes from the achievements to the dreams are paved by the outstanding research challenges, which are discussed in the paper along with interesting ongoing work.},
 duplicado = {false},
 inserir = {false},
 title = {Software testing research: Achievements, challenges, dreams},
 year = {2007}
}

