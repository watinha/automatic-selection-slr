@article{1165,
 abstract = {The expansion of the Wireless Sensor Networks (WSNs) increases the number of possible security vulnerabilities. This is the reason why it is important to provide methods and tools that facilities the design and the analysis of the WSNs. This thesis presents different techniques to be used during their development process. The first part of the thesis proposes a Wireless Sensor Network Analysis framework that estimates the WSN security. The second part of the thesis presents techniques to develop secure and efficient WSN firmware. These techniques are focused on the design of attack-aware firmware, the improvement of the security during the booting process and the increase in the performance of the WSN update process. The last part of the thesis presents a verification methodology that allows reusing the tests in different phases of the design process.},
 duplicado = {false},
 inserir = {false},
 title = {Sistemas embebidos en red seguros},
 year = {2017}
}

@article{1166,
 abstract = {Ontology patterns have been pointed out as a promising approach for ontology engineering. The goal of this paper is to clarify concepts and the terminology used in Ontology Engineering to talk about the notion of ontology patterns taking into account already well-established notions of patterns in Software Engineering.},
 duplicado = {false},
 inserir = {false},
 title = {Ontology patterns: clarifying concepts and terminology},
 year = {2013}
}

@article{1167,
 abstract = {Abstract. This paper presents the version 2.0 of SABiO - a Systematic Approach
for Building Ontologies. SABiO focus on the development of domain
ontologies, and propose also support and management processes, which are
strongly linked to the development process. SABiO distinguishes between reference
and operational ontologies, providing activities that apply to the development
of both types of domain ontologies.},
 duplicado = {false},
 inserir = {false},
 title = {SABiO: Systematic Approach for Building Ontologies},
 year = {2014}
}

@article{1169,
 abstract = {Abstract. Currently, most requirements documents are prepared using desktop
text editors. These documents are intended to be used by human readers. In this
paper, we discuss the use of semantic annotations in requirements documents,
in order to make information regarding links between requirements and other
software artifacts, such as other requirements, use cases, classes and test cases,
interpretable by computers. To do that, we extend a semantic document management
platform to the requirements domain, and explore the conceptualization
established by the Software Requirements Reference Ontology in order to
provide features to support some activities of the Requirement Engineering
Process, namely: prioritizing requirements, analyzing impacts from requirements
changes, tracing requirements through traceability matrices, and verifying
requirements using checklists.},
 duplicado = {false},
 inserir = {false},
 title = {Semantic Documentation in Requirements Engineering},
 year = {2014}
}

@article{1171,
 abstract = {Abstract
Software Engineering (SE) is a wide domain, where ontologies are useful instruments for dealing with Knowledge Management (KM) related problems. When SE ontologies are built and used in isolation, some problems remain, in particular those related to knowledge integration. The goal of this paper is to provide an integrated solution for better dealing with KM-related problems in SE by means of a Software Engineering Ontology Network (SEON). SEON is designed with mechanisms for easing the development and integration of SE domain ontologies. The current version of SEON includes core ontologies for software and software processes, as well as domain ontologies for the main technical software engineering subdomains, namely requirements, design, coding and testing. We discuss the development of SEON and some of its envisioned applications related to KM.},
 duplicado = {false},
 inserir = {false},
 title = {SEON: A Software Engineering Ontology Network},
 year = {2016}
}

@article{1172,
 abstract = {Abstract
Motivated by the assumption that Semantic Web technologies are not sufficiently leveraged in the Software Engineering discipline (SE) to provide support regarding the standardization of software development processes by means of international software standards, we investigate the existence of systematic literature reviews in this regard. We concluded that none of the available reviews is specifically focused on analysing international standard-based SE approaches, but on investigating SE approaches in a general way. In this paper, we present the details about all the stages in the conducting of a systematic literature review on the Semantic Web technologies-based support for the standardization of the SE discipline regarding software development processes; one of the major findings of the presented review is that nowadays there is a shortage of approaches providing support for the standardization of software development processes for small and very small software companies.},
 duplicado = {false},
 inserir = {false},
 title = {Towards Supporting International Standard-Based Software Engineering Approaches Using Semantic Web Technologies: A Systematic Literature Review},
 year = {2016}
}

@article{1178,
 abstract = {Abstract:
Despite all the efforts to reduce the cost of the testing phase in software development, it is still one of the most expensive phases. In order to continue to minimize those costs, in this paper, we propose a Domain-Specific Language (DSL), built on top of MetaEdit+ language workbench, to model performance testing for web applications. Our DSL, called Canopus, was developed in the context of a collaboration1 between our university and a Technology Development Laboratory (TDL) from an Information Technology (IT) company. We present, in this paper, the Canopus metamodels, its domain analysis, a process that integrates Canopus to Model-Based Performance Testing, and applied it to an industrial case study.},
 duplicado = {false},
 inserir = {false},
 title = {Canopus: A Domain-Specific Language for Modeling Performance Testing},
 year = {2016}
}

@article{1180,
 abstract = {Abstract
Testing of a software system is resource-consuming activity. One of the promising ways to improve the efficiency of the software testing process is to use ontologies for testing. This paper presents an approach to test case generation based on the use of an ontology and inference rules. The ontology represents requirements from a software requirements specification, and additional knowledge about components of the software system under development. The inference rules describe strategies for deriving test cases from the ontology. The inference rules are constructed based on the examination of the existing test documentation and acquisition of knowledge from experienced software testers. The inference rules are implemented in Prolog and applied to the ontology that is translated from OWL functional-style syntax to Prolog syntax. The first experiments with the implementation showed that it was possible to generate test cases with the same level of detail as the existing, manually produced, test cases.},
 duplicado = {false},
 inserir = {false},
 title = {Application of Inference Rules to a Software Requirements Ontology to Generate Software Test Cases},
 year = {2016}
}

@article{1181,
 abstract = {This master project presents a view of how Computers Science areas related to performance (Performance Evaluation, Software Performance Engineering and Software Performance Testing) can be related and also proposes a performance analysis methodology that contains concepts from all areas previously identified, so that it is more complete and can be understood by professionals of these three areas. To formalize this relationship, an ontology that shows how the correlation occurs between areas was built. And from the proposed methodology, it was possible to analyze the performance of ValiPar tool, in its parallel version, and it was concluded that the main bottleneck with respect to its scalability is its portion executed sequentially. Finally, it was observed that the proposed methodology has advantages compared to others, as a formalization in its analysis steps.},
 duplicado = {false},
 inserir = {false},
 title = {Unificando conceitos de avaliacao de desempenho, engenharia de desempenho e teste de software para a analise de sistemas computacionais},
 year = {2016}
}

@article{1182,
 abstract = {Abstract. There are several areas of computer science whose main point of
study is the performance of computer systems, whether for the construction of
software with performance as to estimate the performance of systems. Examples
of such areas are: Performance Evaluation, Software Performance Engineering,
Software Performance Testing and Experimental Studies. However, there is no
work in the literature that unifies or even lists the concepts and methodologies
they raised. In this context, this paper describes how these areas can be related,
and this study will provide the basis for the definition of an ontology},
 duplicado = {false},
 inserir = {false},
 title = {Relacionando Conceitos de areas de Estudo de Desempenho da Ciencia da Computacao},
 year = {2015}
}

@article{1183,
 abstract = {Abstract Performance is a fundamental quality of software
systems. The focus of performance testing is to reveal bottlenecks
or lack of scalability of a system or an environment. However,
usually the software development cycle does not include this effort
on the early development phases, which leads to a weak elicitation
process of performance requirements. One way to mitigate that
is to include performance requirements in the system models.
This can be achieved by using Model-Based Testing (MBT) since
it enables to aggregate testing information in the system model
since the early stages of the software development cycle. This
also allows to automate the generation of test artifacts, such as
test cases or test scripts, and improves communication among
different teams. In this paper, we present a set of requirements
for developing a Domain-Specific Language (DSL) for modeling
performance testing of Web applications. In addition, we present
our design decisions in creating a solution that meets the specific
needs of a partner company. We believe that these decisions help
in building a body of knowledge that can be reused in different
settings that share similar requirements.},
 duplicado = {false},
 inserir = {false},
 title = {A Domain-Specific Language for Modeling Performance Testing},
 year = {2016}
}

@article{1186,
 abstract = {Effective software performance testing is essential to the development and delivery of
quality software products. Many software testing investigations have reported software
performance testing improvements, but few have quantitatively validated measurable
software testing performance improvements across an aggregate of studies. This study
addressed that gap by conducting a meta-analysis to assess the relationship between
applying Design of Experiments (DOE) techniques in the software testing process and the
reported software performance testing improvements. Software performance testing
theories and DOE techniques composed the theoretical framework for this study.
Software testing studies (n = 96) were analyzed, where half had DOE techniques applied
and the other half did not. Five research hypotheses were tested, where findings were
measured in (a) the number of detected defects, (b) the rate of defect detection, (c) the
phase in which the defect was detected, (d) the total number of hours it took to complete
the testing, and (e) an overall hypothesis which included all measurements for all
findings. The data were analyzed by first computing standard difference in means effect
sizes, then through the Z test, the Q test, and the t test in statistical comparisons. Results
of the meta-analysis showed that applying DOE techniques in the software testing
process improved software performance testing (p < 05). These results have social
implications for the software testing industry and software testing professionals,
providing another empirically-validated testing methodology. Software organizations can
use this methodology to differentiate their software testing process, to create more quality
products, and to benefit the consumer and society in general. },
 duplicado = {false},
 inserir = {false},
 title = {The Effect of Applying Design of Experiments Techniques to Software Performance Testing},
 year = {2015}
}

@article{1190,
 abstract = {    Still today, the development of effective and high-quality software tests is an expensive and very labor intensive process. It demands a high amount of problem awareness, domain knowledge and concentration from human software testers. Therefore, any technology that can help reduce the manual effort involved in the software testing process -- while ensuring at least the same level of quality -- has the potential to significantly reduce software development and maintenance costs. In this dissertation, we present a new idea for achieving this by reusing the knowledge bound up in existing tests. Over the last two decades, software reuse and code recommendation has received a wide variety of attention in academia and industry, but the research conducted in this area to date has focused on the reuse of application code rather than on the reuse of tests. By switching this focus, this thesis paves the way for the automated extraction of test data and knowledge from previous software projects. In particular, it presents a recommendation approach for software tests that leverages lessons learned from traditional software reuse to make test case reuse suggestions to software engineers while they are working. In contrast to most existing testing-assistance tools, which provide ex post assistance to test developers in the form of coverage assessments and test quality evaluations, our approach offers an automated, proactive, non-intrusive test recommendation system for efficient software test development.},
 duplicado = {false},
 inserir = {true},
 title = {Reuse-Based Test Recommendation in Software Engineering},
 year = {2014}
}

@article{1191,
 abstract = {Abstract
Despite the fact that the test phase is described in the literature as one of the most relevant for quality assurance in software projects, this test phase is not usually developed, among others, with enough resources, time or suitable techniques.

To offer solutions which supply the test phase, with appropriate tools for the automation of tests generation, or even, for their self-execution, could become a suitable way to improve this phase and reduce the cost constraints in real projects.

This paper focuses on answering a concrete research question: is it possible to generate test cases from functional requirements described in an informal way? For this aim, it presents an overview of a set of relevant approaches that works in this field and offers a set of comparative analysis to determine which the state of the art is.},
 duplicado = {false},
 inserir = {false},
 title = {An overview on test generation from functional requirements},
 year = {2011}
}

@article{1192,
 abstract = {Abstract
Model-based testing (MBT) is about testing a software system using a model of its behaviour. To benefit fully from MBT, automation support is required. The goal of this systematic review is determining the current state of the art of prominent MBT tool support where we focus on tools that rely on state-based models. We automatically searched different source of information including digital libraries and mailing lists dedicated to the topic. Precisely defined criteria are used to compare selected tools and comprise support for test adequacy and coverage criteria, level of automation for various testing activities and support for the construction of test scaffolding. Simple adequacy criteria are supported but not advanced ones; data(-flow) criteria are seldom supported; support for creating test scaffolding varies a great deal. The results of this review should be of interest to a wide range of stakeholders: software companies interested in selecting the most appropriate MBT tool for their needs; organizations willing to invest into creating MBT tool support; researchers interested in setting research directions.},
 duplicado = {false},
 inserir = {false},
 title = {A systematic review of state-based test tools},
 year = {2015}
}

@article{1193,
 abstract = {Requirements Engineering and Software Testing are mature areas and have seen a lot of research. Nevertheless, their interactions have been sparsely explored beyond the concept of traceability. To fill this gap, we propose a definition of requirements engineering and software test (REST) alignment, a taxonomy that characterizes the methods linking the respective areas, and a process to assess alignment. The taxonomy can support researchers to identify new opportunities for investigation, as well as practitioners to compare alignment methods and evaluate alignment, or lack thereof. We constructed the REST taxonomy by analyzing alignment methods published in literature, iteratively validating the emerging dimensions. The resulting concept of an information dyad characterizes the exchange of information required for any alignment to take place. We demonstrate use of the taxonomy by applying it on five in-depth cases and illustrate angles of analysis on a set of thirteen alignment methods. In addition, we developed an assessment framework (REST-bench), applied it in an industrial assessment, and showed that it, with a low effort, can identify opportunities to improve REST alignment. Although we expect that the taxonomy can be further refined, we believe that the information dyad is a valid and useful construct to understand alignment.},
 duplicado = {false},
 inserir = {false},
 title = {A taxonomy for requirements engineering and software test alignment},
 year = {2014}
}

@article{1194,
 abstract = {Debugging failing test cases, particularly the search for failure causes, is often a laborious and time-consuming activity. With the help of spectrum-based fault localization developers are able to reduce the potentially large search space by detecting anomalies in tested program entities. However, such anomalies do not necessarily indicate defects and so developers still have to analyze numerous candidates one by one until they find the failure cause. This procedure is inefficient since it does not take into account how suspicious entities relate to each other, whether another developer is better qualified for debugging this failure, or how erroneous behavior comes to be.
We present test-driven fault navigation as an interconnected debugging guide that integrates spectrum-based anomalies and failure causes. By analyzing failure-reproducing test cases, we reveal suspicious system parts, developers most qualified for addressing localized faults, and erroneous behavior in the execution history. The Paths tool suite realizes our approach: PathMap supports a breadth first search for narrowing down failure causes and recommends developers for help; PathFinder is a lightweight back-in-time debugger that classifies failing test behavior for easily following infection chains back to defects. The evaluation of our approach illustrates the improvements for debugging test cases, the high accuracy of recommended developers, and the fast response times of our corresponding tool suite.},
 duplicado = {false},
 inserir = {false},
 title = {Test-driven Fault Navigation for Debugging Reproducible Failures},
 year = {2012}
}

@article{1195,
 abstract = {Abstract:
Context: In teaching about software engineering we currently make little use of any empirical knowledge. Aim: To examine the outcomes available from the use of Evidence-Based Software Engineering (EBSE) practices, so as to identify where these can provide support for, and inform, teaching activities. Method: We have examined all known secondary studies published up to the end of 2009, together with those published in major journals to mid-2011, and identified where these provide practical results that are relevant to student needs. Results: Starting with 145 candidate systematic literature reviews (SLRs), we were able to identify and classify potentially useful teaching material from 43 of them. Conclusions: EBSE can potentially lend authority to our teaching, although the coverage of key topics is uneven. Additionally, mapping studies can provide support for research-led teaching.},
 duplicado = {false},
 inserir = {false},
 title = {What scope is there for adopting evidence-informed teaching in SE?},
 year = {2012}
}

@article{1196,
 abstract = {Abstract:
Software testing is an important part of project development. Depending on system type and size, it is performed variously. Unit testing is one of the available approaches that is used to ensure that behavior of small software parts is consistent with requirements. It allows to improve software quality and decrease overall costs. Despite the fact that such an approach is commonly judged as a vital concept, it is not usual in control software. In this paper, the comprehensive approach to test the IEC 61131-3 software using unit tests is presented. It supports to create tests in two ways-either in textual and graphical IEC 61131-3 languages or in the CPTest+ dedicated test definition language. The latter is equipped with many advanced features, such as test fixtures and inclusions, parameterized and analog signal extensions, mock objects, as well as a few kinds of suites. The overall solution runs on the developer and testing station; hence, it does not have significant impact on performance of the control program and tests are more reliable and repeatable. To explain the concept, the simple running example is presented in this paper. The described solution has been introduced in the CPDev engineering environment for programming controllers.},
 duplicado = {false},
 inserir = {false},
 title = {POU-Oriented Unit Testing of IEC 61131-3 Control Software},
 year = {2015}
}

@article{1197,
 abstract = {Abstract
The paper presents a concept and implementation of Communication Performance Tests (CPT) for small distributed control systems. Requirements for the communication performance are specified using SysML notation. Test cases included in the specification are translated into a dedicated test definition language CPTest+. System implementation is then verified by executing the tests generated from the specification and analyzing results of test runs. The procedure is supported by specialized tools integrated with IEC 61131-3 development environment, including SysML model editor and CPTest testing environment.},
 duplicado = {false},
 inserir = {false},
 title = {Communication Performance Tests in Distributed Control Systems},
 year = {2013}
}

@article{1198,
 abstract = {In the last decade we have witnessed a growth in outsourcing and outshoring development. Following the promise of reducing costs and round-the-clock development, software organizations have grown from local to global enterprises. In the same decade, agile software development methodologies have emerged as a viable alternative to produce software. There is a myriad of agile processes and methodologies now available for any software development organization to choose from. These agile processes follow the values signed in the Agile Manifesto that preaches the exaltation of the individual programmer, high feedback, customer interaction and just enough planning and documentation. But how does global distribution affect these values? Can agile software development be implemented under the global software development context? This paper presents a systematic literature review aimed at identifying factors that affect the adoption of agile factors in global distributed teams. Our findings show that the literature is still in its initial case study publication stage. But most notably, we have found that only a few of the factors found are related to the agile values. Even though more research is clearly needed, this can be a signal that the factors affecting team distribution has more impact on software development than the values and practices preached by the agile processes.},
 duplicado = {false},
 inserir = {false},
 title = {FACTORS AFFECTING DISTRIBUTED AGILE PROJECTS: A SYSTEMATIC REVIEW},
 year = {2013}
}

@article{1199,
 abstract = {Abstract:
Over the past 25 years the Brazilian Symposium on Software Engineering (SBES) has evolved to become the most important event on software engineering in Brazil. Throughout these years, SBES has gathered a large body of studies in software testing. Aimed at providing an insightful understanding of what has already been published in such event, we synthesized its rich 25-year history of research on software testing. Using information drawn from this overview we attempted to highlight which types of study have been the most applied for conveying software testing efforts. We also devised a co-authorship network to obtain a bird's-eye view of which research groups and scholars have been the most prolific ones. Moreover, by performing a citation analysis of the selected studies we set out to ascertain the importance of SBES in a wider scenario. Finally, borne out by the information extracted from the studies, we shed some light on the state-of-the-art of software testing in Brazil and provide an outlook on its foreseeable future.},
 duplicado = {false},
 inserir = {false},
 title = {What a Long, Strange Trip It's Been: Past, Present, and Future Perspectives on Software Testing Research},
 year = {2011}
}

@article{1200,
 abstract = {Abstract:
There is a gap between software testing research and practice. One reason is the discrepancy between how testing research is reported and how testing challenges are perceived in industry. We propose the SERP-test taxonomy to structure information on testing interventions and practical testing challenges from a common perspective and thus bridge the communication gap. To develop the taxonomy we follow a systematic incremental approach. The SERP-test taxonomy may be used by both researchers and practitioners to classify and search for testing challenges or interventions. The SERP-test taxonomy also supports comparison of testing interventions by providing an instrument for assessing the distance between them and thus identify relevant points of comparisons.},
 duplicado = {false},
 inserir = {false},
 title = {Mapping software testing practice with software testing research SERP-test taxonomy},
 year = {2015}
}

@article{1201,
 abstract = {Abstract
Over the past 25 years the Brazilian Symposium on Software Engineering (SBES) has evolved to become the most important event on software engineering in Brazil. Throughout these years, SBES has gathered a large body of studies in software testing. Aimed at providing an insightful understanding of what has already been published in such event, we have synthesized its 25-year history of research on software testing. Using information drawn from this overview we highlighted which software testing topics have been the most extensively surveyed in SBES literature. We have also devised a co-authorship network to depict the most prolific research groups and researchers. Moreover, by performing a citation analysis of the selected studies we have tried to ascertain the importance of SBES in a wider scenario. Finally, using the information extracted from the studies, we have shed light on the state-of-the-art of software testing in Brazil and provided an outlook on its foreseeable future.},
 duplicado = {false},
 inserir = {false},
 title = {A scoping study on the 25 years of research into software testing in Brazil and an outlook on the future of the area},
 year = {2013}
}

@article{1202,
 abstract = {Abstract [en]
Background: In Agile Software Development (ASD) planning is valued more than the resulting plans. Planning and estimation are carried out at multiple levels in ASD. Agile plans and estimates are frequently updated to reflect the current situation. It supports shorter release cycles and flexibility to incorporate changing market and customer needs. Many empirical studies have been conducted to investigate effort estimation in ASD. However, the evidence on effort estimation in ASD has not been aggregated and organized.

Objective: This thesis has two main objectives: First, to identify and aggregate evidence, from both literature and industry, on effort estimation in ASD. Second, to support research and practice on effort estimation in ASD by organizing the identified knowledge.

Method: In this thesis we conducted a Systematic Literature Review (SLR), a systematic mapping study, a questionnaire based industrial survey and an interview based survey.

Results: The SLR and survey results showed that agile teams estimate effort, mostly during release and iteration planning, using techniques that are based on experts' subjective assessments. During effort estimation team related cost drivers, such as team members expertise, are considered important. The results also highlighted that implementation and testing are the only activities that are accounted for in effort estimates by most agile teams. Our mapping study identified that taxonomies in SE are mostly designed and presented in an ad-hoc manner. To fill this gap we updated an existing method to design taxonomies in a systematic way. The method is then used to design taxonomy on effort estimation in ASD using the evidence identified in our SLR and survey as input.

Conclusions: The proposed taxonomy is evaluated by characterizing effort estimation cases of selected agile projects reported in literature. The evaluation found that the reporting of the selected studies lacks information related to the context and predictors used during effort estimation in ASD. The taxonomy can be used in consistently reporting effort estimation studies in ASD to facilitate identification, aggregation and analysis of the evidence. The proposed taxonomy was also used to characterize the effort estimation activity of agile teams in three different software companies. The proposed taxonomy was found to be useful by interviewed agile practitioners in documenting important effort estimation related knowledge, which otherwise remain tacit in most cases.},
 duplicado = {false},
 inserir = {false},
 title = {Supporting Effort Estimation in Agile Software Development},
 year = {2015}
}

@article{1203,
 abstract = {Abstract
This paper presents the construction and evaluation of SERP-test, a taxonomy aimed to improve communication between researchers and practitioners in the area of software testing. SERP-test can be utilized for direct communication in industry academia collaborations. It may also facilitate indirect communication between practitioners adopting software engineering research and researchers who are striving for industry relevance. SERP-test was constructed through a systematic and goal-oriented approach which included literature reviews and interviews with practitioners and researchers. SERP-test was evaluated through an online survey and by utilizing it in an industry academia collaboration project. SERP-test comprises four facets along which both research contributions and practical challenges may be classified: Intervention, Scope, Effect target and Context constraints. This paper explains the available categories for each of these facets (i.e., their definitions and rationales) and presents examples of categorized entities. Several tasks may benefit from SERP-test, such as formulating research goals from a problem perspective, describing practical challenges in a researchable fashion, analyzing primary studies in a literature review, or identifying relevant points of comparison and generalization of research.},
 duplicado = {false},
 inserir = {false},
 title = {SERP-test: a taxonomy for supporting industry academia communication},
 year = {2017}
}

@article{1204,
 abstract = {Background

In Global Software Engineering (GSE), the need for a common terminology and knowledge classification has been identified to facilitate the sharing and combination of knowledge by GSE researchers and practitioners. A GSE taxonomy was recently proposed to address such a need, focusing on a core set of dimensions; however its dimensions do not represent an exhaustive list of relevant GSE factors. Therefore, this study extends the existing taxonomy, incorporating new GSE dimensions that were identified by means of two empirical studies conducted recently.

Methods

To address the research questions of this study, we used evidence found through a systematic literature review and a survey. Based on literature, new dimensions were added to the existing taxonomy.

Results

We identified seven dimensions to extend and incorporate into the recently proposed GSE taxonomy. The resulting extended taxonomy was later on validated by comparing it with the existing taxonomy on which the extension is built and one additional taxonomy. We also demonstrated the utility of the extended taxonomy using it to classify eight finished real GSE projects. The extended taxonomy was representative enough to classify the projects in a clear way.

Conclusions

The extended taxonomy can help both researchers and practitioners by providing dimensions that can enable the description of different GSE contexts in a more comprehensive way; this can facilitate the understanding, comparison and aggregation of GSE-related findings.},
 duplicado = {false},
 inserir = {false},
 title = {An extended global software engineering taxonomy},
 year = {2016}
}

@article{1205,
 abstract = {
Abstract
Context: Software Engineering (SE) is an evolving discipline with new subareas being continuously developed and added. To structure and better understand the SE body of knowledge, taxonomies have been proposed in all SE knowledge areas.
Objective: The objective of this paper is to characterize the state-of-the-art research on SE taxonomies.
Method: A systematic mapping study was conducted, based on 270 primary studies.
Results: An increasing number of SE taxonomies have been published since 2000 in a broad range of venues, including the top SE journals and conferences. The majority of taxonomies can be grouped into the following SWEBOK knowledge areas: construction (19.55%), design (19.55%), requirements (15.50%) and maintenance (11.81%). Illustration (45.76%) is the most frequently used approach for taxonomy validation. Hierarchy (53.14%) and faceted analysis (39.48%) are the most frequently used classification structures. Most taxonomies rely on qualitative procedures to classify subject matter instances, but in most cases (86.53%) these procedures are not described in sufficient detail. The majority of the taxonomies (97%) target unique subject matters and many taxonomy-papers are cited frequently. Most SE taxonomies are designed in an ad-hoc way. To address this issue, we have revised an existing method for developing taxonomies in a more systematic way.
Conclusion: There is a strong interest in taxonomies in SE, but few taxonomies are extended or revised. Taxonomy design decisions regarding the used classification structures, procedures and descriptive bases are usually not well described and motivated.},
 duplicado = {false},
 inserir = {false},
 title = {Taxonomies in software engineering: A Systematic mapping study and a revised taxonomy development method},
 year = {2017}
}

@article{1206,
 abstract = {Abstract [en]
The development of large, software-intensive systems is a complex undertaking that is generally tackled by a divide and conquer strategy. Organizations face thereby the challenge of coordinating the resources which enable the individual aspects of software development, commonly solved by adopting a particular process model. The alignment between requirements engineering (RE) and software testing (ST) activities is of particular interest as those two aspects are intrinsically connected: requirements are an expression of user/customer needs while testing increases the likelihood that those needs are actually satisfied.

The work in this thesis is driven by empirical problem identification, analysis and solution development towards two main objectives. The first is to develop an understanding of RE and ST alignment challenges and characteristics. Building this foundation is a necessary step that facilitates the second objective, the development of solutions relevant and scalable to industry practice that improve REST alignment.

The research methods employed to work towards these objectives are primarily empirical. Case study research is used to elicit data from practitioners while technical action research and field experiments are conducted to validate the developed  solutions in practice.

This thesis contains four main contributions: (1) An in-depth study on REST alignment challenges and practices encountered in industry. (2) A conceptual framework in the form of a taxonomy providing constructs that further our understanding of REST alignment. The taxonomy is operationalized in an assessment framework, REST-bench (3), that was designed to be lightweight and can be applied as a postmortem in closing development projects. (4) An extensive investigation into the potential of information retrieval techniques to improve test coverage, a common REST alignment challenge, resulting in a solution prototype, risk-based testing supported by topic models (RiTTM).

REST-bench has been validated in five cases and has shown to be efficient and effective in identifying improvement opportunities in the coordination of RE and ST. Most of the concepts operationalized from the REST taxonomy were found to be useful, validating the conceptual framework. RiTTM, on the other hand, was validated in a single case experiment where it has shown great potential, in particular by identifying test cases that were originally overlooked by expert test engineers, improving effectively test coverage.},
 duplicado = {false},
 inserir = {false},
 title = {Coordinating requirements engineering and software testing},
 year = {2015}
}

@article{1207,
 abstract = {Abstract:
To facilitate the sharing and combination of knowledge by Global Software Engineering (GSE) researchers and practitioners, the need for a common terminology and knowledge classification scheme has been identified, and as a consequence, a taxonomy and an extension were proposed. In addition, one systematic literature review and a survey on respectively the state of the art and practice of effort estimation in GSE were conducted, showing that despite its importance in practice, the GSE effort estimation literature is rare and reported in an ad-hoc way. Therefore, this paper proposes a specialized GSE taxonomy for effort estimation, which was built on the recently proposed general GSE taxonomy (including the extension) and was also based on the findings from two empirical studies and expert knowledge. The specialized taxonomy was validated using data from eight finished GSE projects. Our effort estimation taxonomy for GSE can help both researchers and practitioners by supporting the reporting of new GSE effort estimation studies, i.e. new studies are to be easier to identify, compare, aggregate and synthesize. Further, it can also help practitioners by providing them with an initial set of factors that can be considered when estimating effort for GSE projects.},
 duplicado = {false},
 inserir = {false},
 title = {A Specialized Global Software Engineering Taxonomy for Effort Estimation},
 year = {2016}
}

@article{1209,
 abstract = {Abstract:
Unit testing is the core fundamental to ensure code is in accordance with the design specifications. The coding and unit testing standard reflects the stability of project (not to mention the testing effort). Code stability is greatly influenced by the efforts of unit testing, which can be automated to reduce the human efforts. Inspite of several tools identified for unit testing, tools need to be able to identify the level dependencies or depth of program entity usage in software fragments. This factor greatly influences unit testing complexity. Higher the level of dependency, the greater the complexity of unit testing the code. This paper focuses on this factor which has been a trivial concern for developer over the years. This problem is neither limited nor restricted to a particular paradigm of programming language. Unit Testing becomes more effective when construct becomes more specific to be tested instead of checking the entire code being developed.},
 duplicado = {false},
 inserir = {false},
 title = {Level dependencies of individual entities in random unit testing of structured code},
 year = {2011}
}

@article{1210,
 abstract = {In Agile Software Development (ASD) effort estimation plays an important role during release and iteration planning. The state of the art and practice on effort estimation in ASD have been recently identified. However, this knowledge has not yet been organized. The aim of this study is twofold: (1) To organize the knowledge on effort estimation in ASD and (2) to use this organized knowledge to support practice and the future research on effort estimation in ASD. We applied a taxonomy design method to organize the identified knowledge as a taxonomy of effort estimation in ASD. The proposed taxonomy offers a faceted classification scheme to characterize estimation activities of agile projects. Our agile estimation taxonomy consists of four dimensions: estimation context, estimation technique, effort predictors and effort estimate. Each dimension in turn has several facets. We applied the taxonomy to characterize estimation activities of 10 agile projects identified from the literature to assess whether all important estimation-related aspects are reported. The results showed that studies do not report complete information related to estimation. The taxonomy was also used to characterize the estimation activities of four agile teams from three different software companies. The practitioners involved in the investigation found the taxonomy useful in characterizing and documenting the estimation sessions.},
 duplicado = {false},
 inserir = {false},
 title = {An Effort Estimation Taxonomy for Agile Software Development},
 year = {2017}
}

@article{1211,
 abstract = {Background: Recruitment and onboarding of software developers are essential
steps in software development undertakings. The need for adding
new people is often associated with large-scale long-living projects and
globally distributed projects. The formers are challenging because they
may contain large amounts of legacy (and often complex) code (legacy
projects). The latters are challenging, because the inability to find suffi-
cient resources in-house may lead to onboarding people at a distance, and
often in many distinct sites. While onboarding is of great importance for
companies, there is little research about the challenges and implications
associated with onboarding software developers and teams in large-scale
globally distributed projects with large amounts of legacy code. Furthermore,
no study has proposed any systematic approaches to support the
design of onboarding strategies and evaluation of onboarding results in
the aforementioned context.
Objective: The aim of this thesis is two-fold: i) identify the challenges and
implications associated with onboarding software developers and teams in
large-scale globally distributed legacy projects; and ii) propose solutions to
support the design of onboarding strategies and evaluation of onboarding
results in large-scale globally distributed legacy projects.
Method: In this thesis, we employed literature review, case study, and business
process modeling. The main case investigated in this thesis is the development
of a legacy telecommunication software product in Ericsson.
Results: The results show that the performance (productivity, autonomy,
and lead time) of new developers/teams onboarded in remote locations
in large-scale distributed legacy projects is much lower than the performance
of mature teams. This suggests that new teams have a considerable
performance gap to overcome. Furthermore, we learned that onboarding
problems can be amplified by the following challenges: complexity of
the product and technology stack, distance to the main source of product
knowledge, lack of team stability, training expectation misalignment, and
lack of formalism and control over onboarding strategies employed in different
sites of globally distributed projects. To help companies addressing
the challenges we identified in this thesis, we propose a process to support
the design of onboarding strategies and the evaluation of onboarding results.
Conclusions: The results show that scale, distribution and complex legacy
code may make onboarding more difficult and demand longer periods
of time for new developers and teams to achieve high performance. This means that onboarding in large-scale globally distributed legacy projects
must be planned well ahead and companies must be prepared to provide
extended periods of mentoring by expensive and scarce resources, such as
software architects. Failure to foresee and plan such resources may result
in effort estimates on one hand, and unavailability of mentors on another,
if not planned in advance. The process put forward herein can help companies
to deal with the aforementioned problems through more systematic,
effective and repeatable onboarding strategies.},
 duplicado = {false},
 inserir = {false},
 title = {Strategizing and Evaluating the Onboarding of Software Developers in Large-Scale Globally Distributed Legacy Projects},
 year = {2017}
}

@article{1213,
 abstract = {Abstract
Context-Aware Software Systems (CASS) use environmental information to provide better service to the systems actors to fulfill their goals. Testing of ubiquitous software systems can be challenging since it is unlikely that, while designing the test cases, the tester can identify all possible context variations. A quasi-Systematic Literature Review has been undertaken to characterize the methods usually used for testing CASS. The analysis and generation of knowledge in this work rely on classifying the extracted information. Established taxonomies of software testing and context-aware were used to characterize and interpret the findings. The results show that, although it is possible to observe the utilization of some software testing methods, few empirical studies are evaluating such methods when testing CASS. The selected technical literature conveys a lack of consensus on the understanding of context and CASS, and on the meaning of software testing. Furthermore, context variation in CASS has only been partially addressed by the identified approaches. They either rely on simulating context or in fixing the values of context variables during testing. We argue that the tests of context-aware software systems need to deal with the diversity of context instead of mitigating their effects.},
 duplicado = {false},
 inserir = {false},
 title = {Characterizing testing methods for context-aware software systems: Results from a quasi-systematic literature review},
 year = {2017}
}

@article{1215,
 abstract = {Abstract

The transition from a classical to an agile software development procedure needs a structured and strategic roll out to realize the expected benefits of the transition phase. Quality assurance as part of the software development has also to be designed to realize its targets of the agile transition for adequate project and program quality assurance. Besides effectivity the economic aspects have to be implemented also in a value-driven agile product quality assurance. This tension between effectivity and economy will be shown on the example of the Volkswagen group IT. The transition is mapped to the SPI Manifesto to demonstrate the currency in 2009 of the established values and principles for software process improvement.},
 duplicado = {false},
 inserir = {false},
 title = {Effectivity and economical aspects for agile quality assurance in large enterprises},
 year = {2016}
}

@article{1216,
 abstract = {ABSTRACT: Unit testing has been widely recognized as an important and valuable means of improving software reliability,
as it exposes bugs early in the software development life cycle. However, manual unit testing is often tedious and insufficient.
Testing tools can be used to enable economical use of resources by reducing manual effort. Recently the use of parameters in
unit testing has emerged as a very promising and effective methodology to allow the separation of two testing concerns or
tasks: the specification of external, black-box behavior (i.e., assertions or specifications) by developers and the generation
and selection of internal, white-box test inputs (i.e., high-code-covering test inputs) by tools. The Unit Testing Tool produced
in this research is based on a parameterized test method that takes parameters, calls the code under test, and states assertions.},
 duplicado = {false},
 inserir = {false},
 title = {Parametrized Unit Testing Tool for. Net Framework},
 year = {2012}
}

@article{1217,
 abstract = {Abstract: - Using the theoretical formalism of Li Wei et al (Phys. LettA 333 (2004)) and Afshin Moradi (J.
Electromagnetic Analysis & Applications (2010), we have studied the dispersion relation of TM-mode with and without
electron energy band effects. Without including electron energy band effects, the dispersion relation can be studied with
linearized hydrodynamic theory with Maxwell equations. This indicates that the TM-mode is very different from TE-mode.
Here, the dispersion relation does not approach to well-known dispersion relation of 2D electron-gas. Besides, it also
indicates that internal interaction forces play an important role on the dispersion relation of TM-mode. The dispersion
relation of TM-mode including electron energy band effects can be studied by means of the semi classical kinectic theory of
the electron dynamics. The effect of energy band structure is taken into account for surface plasmon oscillations in the zigzag
and armchair nanotube of metallic character. Our theoretical results also indicate that plasmon waves are not sensitive
to the types of metallic nanotube with same radius. Our theoretically evaluated results are in good agreement with those of
the other theoretical workers.},
 duplicado = {false},
 inserir = {false},
 title = {AN EVALUATION OF DISPERSION RELATION OF PROPAGATION OF ELECTROMAGNETIC WAVE WITH TM MODES IN CARBON NANOTUBE WITH AND WITHOUT ELECTRON ENERGY-BAND EFFECTS},
 year = {2015}
}

@article{1218,
 abstract = {Abstract:
In the present day, every operation of any organization are highly affected by Information Technology. Rapid innovations in technology influence companies to produce high quality software and to maintain the state of the art of the software in order to remain competitive in the industry. Customers rely on the functionality and accuracy of the software. This has placed a greater responsibility on IT companies. Release of poor quality software has long term negative effect where recovery may be difficult. Hence, it is important for IT companies to deliver high quality software that is reliable, delivered on time, lies within budget and provides an assurance that the software performs according to the specifications before being delivered to the customer. In order to fulfill the above challenge IT companies follow various strategies. This paper therefore gives a brief summary of the strategies followed by IT industry to deliver high quality software.},
 duplicado = {false},
 inserir = {false},
 title = {Defect dection strategies and implications: A researcher's perspective},
 year = {2016}
}

@article{1219,
 abstract = {Abstract:
The measurement of external software attributes and the analysis of how those attributes have evolved through the software's releases are challenging activities. This is particularly evident when we discuss the maintainability of Object-Oriented (OO) systems which, due to their specific characteristics, hide information that cannot be gathered through static analysis. As maintainability can be defined as the "speed and ease with which a program can be corrected or changed", we believe that test data are reflective of the changes performed during maintenance. Moreover, empirical observations allow to speculate about the relationships between maintainability and the behaviour of the software when executed by test cases (e.g., coverage values) and the test cases' characteristics (e.g., generation time). Our aim is to complement the state-of-the art by proposing a new approach for understanding and characterizing the maintainability of OO systems, which makes use of test data and of the information gathered from the tests' execution.},
 duplicado = {false},
 inserir = {false},
 title = {On the Evaluation of Software Maintainability Using Automatic Test Case Generation},
 year = {2014}
}

@article{1220,
 abstract = {
All the factors and their level of efficiency, which have a negative impact over the process, should be defined for the model of an efficient product development process. This doctorate thesis analyses the difficulties faced by the organizations developing product in the information technologies field during the product development lifecycle and creates the most appropriate performance models, uncovering the time and effort deviations coming from these difficulties. In this work, the process performance of product development is measured by four basic deviation values: time deviation quantity, time deviation rates, effort deviation quantity and effort deviation rates. When the planned period is removed from the completion period of the project, the deviation quantity is reached. When the deviation quantity is divided by the planned project period, the deviation rate is found. Similarly, the effort deviation quantity is the difference between the actual and the planned project efforts. The effort deviation rate shows at what rate development effort calculated in the beginning of the project deviates. All the factors causing time and effort deviations in the product development lifecycle are evaluated with their deviation rates and quantities in the total project and the elements having an impact on these factors are exposed. There are seven basic factor classes having a direct impact over the product development process and cause the deviations from the plan in the project. The requirements coming from the users and clients make a start to the product development process. The problems or misinterpretation or deficient interpretation of the requirements have a negative impact over the development process during the requirement analysis. These factor classes gathered under the title of the understandability of requirements define the difficulties and deviations caused by the product requirements. The change of the requirements in the product development lifecycle is an inevitable situation. The difficulties of the changing requirements were analyzed under the title of new requirements. In some projects, the environment, data, document and confirmation, which will be provided by the customer to go on the development process are needed. When the customer cannot provide this information on time, the process has difficulties. The situations rising from the clients are evaluated under the title of customer based difficulties. Another important facility of the product development process is the implementation of the analyzed requirements. The difficulties appeared during the product development (difficulties rising from the technical features, development and test environments and devices) were evaluated as general technical difficulties. The difficulties arising from the discontinuity of the human resources, the difficulties arising from the subcontractors and the project partners and the difficulties during the product supply were evaluated respectively under the name of human resources difficulties, subcontractor and project partner difficulties, and purchasing difficulties. During the product development process, there are also some factors having an indirect impact over the process. These factors are the factors whose effect on project performance cannot be obtained directly from project plans or project team. Factors such as reusing strategy, preliminary preparation studies, the state of taking a product or a system as a reference, competence levels of team members, competence levels of project manager, innovation level of the product, Research and Development level (R&D) of the project, state of product requirements being identified and customer participation are included in this category. The impacts of direct and indirect destructive factors over the project period and project effort were exposed with the works carried out on the 75 finished projects. Which factors have an impact over the delay of the projects and how much these delays prolong the project period (as quantity and rate) were analyzed. Similarly, the factors that could affect project effort and effort deviations are also studied. All the hypotheses about all the factors having a direct or indirect impact over the product development process were composed in this work. With respect to their appearance frequency and magnitude, the effective factors over the process were uncovered after testing the hypotheses. The hypotheses were analyzed with the methods of Pearson, Kendall�s tau_b, Kruskal-Wallis and Mann-Whitney U. When the appearance frequency of the destructive factors and their level of affecting the process were evaluated, it was seen that the understandability of requirements and the general technical difficulties have a great effect over the product development process. These factors are followed by new requirements, client based difficulties, and human resources. Even when this order can change within itself, the general perspective does not change in time deviation rates. The correlation analyses carried out supports these findings. The relations among the factors indirectly affecting the product development process, time deviation quantities, time deviation rates, effort deviation quantities and effort deviation rates are evaluated. It was revealed that there is a negative relationship between the management experience of the project manager and the time deviation values. This relationship is better at the groups having a perfect management experience. The similar results are reached when the domain expertise and technical skill of the project manager are compared with the time deviation values. Furthermore, there is a negative relationship between the competence levels of the project manager and effort deviation rates. On the other hand, there is no relationship between the domain expertise and management skill of the project manager and effort deviation quantity. It is seen that there is a weak and negative relationship between the competence levels of the project team and time and effort deviation quantities. Conversely, there is no relationship between the project and technical experience of the team and time deviation rates. Furthermore, no relationship was found between the competence levels of the team and effort deviation rates. The competence levels of the team members are contrasted with the deviation rates rising from the basic destructive factors. It was seen that there is a negative relation between the domain expertise and the understandability of requirements and new requirements. Also, there is a negative relation between the technical experience and general technical problems and purchasing difficulties. Time deviations are influenced by preliminary preparation studies, the use of existing products, and the state of taking a product or a system as a reference. On the other hand, effort deviations are only influenced by preliminary preparation studies. It was understood that there are negative relationships between three basic factor classes (the understandability of requirements, new requirements and customer problems) and level of defining the operational, functional and design requirements. On the other hand, it is seen that the general technical problems are only related to levels of defining the functional and design requirements. Furthermore, effort deviations are influenced by level of defining the operational, functional and design requirements. Although there is a difference among the R&D levels of the projects in terms of their quantities of time and effort deviation, there is no such difference in their rates of time and effort deviation. When the relation between the innovation levels of the product and time deviations of the project was analyzed, it was seen that there is a difference between the projects which have innovative features and which do not in terms of time deviation values. The similar results were found for the effort deviation quantities. On the other hand, it was revealed that there is no difference among the innovation levels of the products in terms of their effort deviation rates. After a few tests carried out, the impacts of the all factors over the project time values were found and they were visualized with the correlation diagrams. In the last two parts of the work, factor classes were uncovered with two different modelling methodologies, namely multiple regression analysis and decision tree techniques. Multiple regression analysis was used to analyze the relationship between the project time deviations and the basic factors having a direct impact over the product development process. A total of five regression models were composed in this study. The understandability of requirements, new requirements, general technical difficulties, purchasing difficulties and human resources difficulties were selected as independent variables for the overall project deviation quantities model. Time deviation quantities of these basic factor classes were weighted by the regression analysis procedure to obtain best prediction model for the deviation quantities. The generated regression equation can explain 89.5% of the variance in the dependent variable. The second regression model predicts the total time deviation rates based on the understandability of requirements, new requirements, general technical difficulties, purchasing difficulties and customer based difficulties. 90.5% of the variance in the time deviation rates can be explained by these five destructive factors. The state of taking a product or a system as a reference is used as dummy variable to estimate the overall project deviation rate in the third regression model. The deviations rates caused by new requirements, general technical problems and customer based problems are the independent variables of the model. 76.7% of the variance in the time deviation rates can be explained by these three destructive factors. The fourth regression model uses management experience of the project manager as a dummy variable. This model predicts the overall time deviation rates based on the understandability of requirements, new requirements, general technical difficulties and customer based problems. %91.6 of the variance in the time deviation rates can be explained by this regression equation. Furthermore, in the last regression model, the preliminary preparation studies are used as dummy variable to estimate the overall project deviation rate. The model uses new requirements, general technical difficulties, customer based difficulties and the domain experience of the team as independent variables. %79.8 of the variance in the time deviation rates can be explained by these three basic destructive factors and the domain experience of the team. In the last part of the work, the factor classes which influence the rates of the project time deviation were uncovered, with CHAID, which is one of the decision tree techniques. The decision trees were composed in three steps. Two models were created at the first step. These models accept the total project time deviation rates as dependent variable and basic factor deviations and indirect factors as independent variable. The first model assumes the factor of the understandability of requirements as the first decomposition point. This model estimates the total project time deviation rates according to the state of preliminary preparation studies, the innovation levels of the products, the general technical difficulties and deviations. If the requirements are not well understood, it is inevitable that the delay will be at high level. The state of preliminary preparations and the innovation level of the products come to the fore. In the second model, the first decomposition point is the problems coming from the new requirements. This model uncovers how the new requirements will affect the project performance if they are accepted. Five models were created at the second step. These models accept the basic factor deviations as dependent variable and the factor classes influencing the process indirectly as independent variable. The third and fourth models accept the time deviations originating from the understandability of requirements as the target variable. The third model is a very easy model. The state of functional requirements being identified is only one decomposition point in the model. The fourth model accepts the deviations rising from the understandability of requirements as the target variable and makes a start to decomposition with the preliminary preparation works carried out before the project. If systematic preliminary preparation studies have not been conducted, the deviation rates increase. Domain knowledge of the project manager and the state of taking a product as a reference come to the fore in this model. In the fifth model, the time deviation values coming from the general technical problems are accepted as the target variable. The accurate definition of the design requirements is the first decomposition point in the generated decision tree. In the sixth model, deviation states of the new coming requirement were chosen as the target variable. All elements having an impact over the deviations due to this factor were uncovered. In the seventh model, the delays resulting from the purchasing process were analyzed. The R&D level of the products determines the probability of delay. Finally, two models which accept the total project time deviation rates as dependent variable and the indirect factor classes as independent variable were created. These models analyze the time deviation rates in terms of the state of requirement to being defined and the competence levels of project manager. The model taking the competence levels of project manager as a reference shows that there are usually no significant delays at the groups having a perfect management experience. Almost all of the delays in this category are related to the innovation features of the product. The model taking the state of the requirements to be defined as a reference shows that there are not high rates of time deviation in the projects whose product requirements are defined completely and properly at the beginning of the project. The model shows how the other factors influence the process performance in case of the deficient product requirements. In summary, the destructive factors having an impact over the product development process were completed after a wide research and the hypotheses were written. Seventy-five pieces of project information were reached after the studies were done over the finished projects. The impact of these factors (their frequency and magnitude) was exposed by the statistical methods executed on the real project information acquired. The result of all the tests carried out with the correlation analyses was summarized in correlation diagrams. The impact of the defined destructive factors over the process performance were modelled with the multiple regression analysis and decision trees techniques, and the interaction between the factors, which affect the process directly and indirectly was shown with these models. The decision models and regression equations mentioned in this thesis are the best models among other models in terms of the results of estimation.},
 duplicado = {false},
 inserir = {false},
 title = {Modeling Destructive Factors In Product Development Process Of Project Based Companies In The Field Of Information Technologies And Determining Their Impacts},
 year = {2012}
}

@article{1221,
 abstract = {Testing techniques refer to different methods or ways of testing particular features of a computer
program, system or product. Presently there are so many different software testing techniques that we
can use. Whether we decide to automate or just execute tests manually, there is a selection of testing
techniques to choose from. We have to make sure that we select technique(s) that will help to ensure
the most efficient and effective testing of the system. The fundamental problem in software testing thus
throws an open question, as to what would be the techniques that we should adopt for an efficient and
effective testing. Thus, the selection of right testing techniques at the right time for right problem will
make the software testing efficient and effective. In this paper we discuss how should testing techniques
be compared with one another and why do we face a problem in making appropriate testing technique
selection.},
 duplicado = {false},
 inserir = {false},
 title = {Identifying some problems with selection of software testing techniques},
 year = {2010}
}

@article{1222,
 abstract = {Background: The quality of software is determined during its development. Software
products target specific software quality attributes, such as safety or performance.
Development teams use various techniques to investigate, evaluate and control potential
product quality problems. We call these Quality Attribute Techniques (QATs). An
example QAT for safety is the hazard analysis technique of Fault Tree Analysis, and for
performance is Stress Testing. Although QATs are widely used in practice, there is no
systematic approach to represent and integrate QATs for arbitrary quality attributes
within existing approaches to software process modelling and tailoring.
Aims: This research aims to provide systematic approaches for better selection and
integration of QATs into tailored software process models that target specific product
qualities.
Method: A framework to capture and present significant QAT information is derived
by 1) reviewing literature to identify important characteristics of QATs for selection and
process integration; and 2) using risk management as a general theory to understand
how QATs function to manage product quality problems during development. A
systematic selection method is then developed to support the choice of appropriate
QATs for any quality attribute, across the lifecycle. The selection method is based on
three perspectives: 1) risk management; 2) process integration; and 3) cost/benefit using
Analytic Hierarchy Process (AHP). A literature-based evaluation is used to validate and
improve the initial framework. An industry case study validates the feasibility and
effectiveness of applying the framework and selection method.
Results: The literature-based evaluation shows that the framework can accommodate a
variety of quality attributes and that the categorisation scheme provides a way to choose
QATs based on their impact in managing quality risks. The case study demonstrates that
the framework and selection method provide a more methodological and effective
approach to choose QATs for projects that target a specific quality attribute, compared
to the ad hoc selection performed by development teams.
Conclusions: The framework codifies a new risk-based understanding about how QATs
impact product qualities across the development lifecycle. The selection method can be
used to systematically choose the QATs for projects to target specific product qualities
throughout lifecycle.},
 duplicado = {false},
 inserir = {false},
 title = {Representation and Selection of Quality Attribute Techniques for Software Development Process},
 year = {2011}
}

@article{1223,
 abstract = {Abstract:
First failure data capture (FFDC) is a technology that is used to capture diagnostic information when errors occur. This information reduces the need to reproduce errors. FFDC involves all firmware components. The processes and techniques of FFDC are crucial to the success of an embedded system. With a good FFDC implementation, software engineers can collect diagnostic data quickly and fix defects without impacting customer's operation. One of the challengers with FFDC is collecting the useful data. There are cases where the volume of data collected is huge but only covers a short amount of time. In other cases, the collected data cover a long period time, but only gather a few variables or items. A new FFDC technique has been developed to capture useful data. This new FFDC technique captures very detail data for a short period of time and captures much less detail data for a long period of time. This paper first introduces three FFDC techniques. The three techniques are general dump, event trace and error log. Second, it analyzes the problems of current FFDC techniques. Third, it discusses a new FFDC technique. This new FFDC technique is low cost, efficient and good for data collection.},
 duplicado = {false},
 inserir = {false},
 title = {First failure data capture in embedded system},
 year = {2007}
}

@article{1224,
 abstract = {ABSTRACT
Software testing is vital and challenging activity in SDLC. The
complexity and size of the software is increasing multi-fold. A
multitude of techniques have been proposed for software
testing, but the key problem in software testing is the selection
of most effective technique. A great deal of research has been
carried out to evaluate the effectiveness and efficiency of
various testing techniques. However, up to now no study was
able to present a universally acceptable solution. As no testing
technique provides a single, comprehensive solution; the
selection must be done according to a given state. We present
here a decision support approach for selecting the most
suitable testing technique on the basis of number of identified
factors that influence the selection of appropriate techniques.
Selection of testing techniques based on factors identified in
this paper can improve the effectiveness of testing process
significantly.},
 duplicado = {false},
 inserir = {false},
 title = {Testing Techniques Selection: A Systematic Approach},
 year = {2011}
}

@article{1225,
 abstract = {Abstract:
This paper introduces twelve defect detection techniques and describes a non-controlled experiment related to defect detection techniques to address the uncertainty of how to test an embedded software and find defects effectively. In this non-controlled experiment, three common testing techniques were applied to a large scale embedded system. This study is intended to evaluate different defect detection techniques that are actually used by software engineers using empirical software engineering method. The objective of empirical software engineering is to improve the software development processes and quality. This could be done by evaluating, comparing and controlling defect detection methods. This study is also intended to find a best method to reduce defects and increase the defect detection rate in a large scale embedded system, since defect detection is considered as one of the most costly development process in software development cycle},
 duplicado = {false},
 inserir = {false},
 title = {Optimize defect detection techniques through empirical software engineering method},
 year = {2005}
}

@article{1226,
 abstract = {Abstract
The quality of software is achieved during its development. Development teams use various techniques to investigate, evaluate and control potential quality problems in their systems. These Quality Attribute Techniques' target specific product qualities such as safety or security. This paper proposes a framework to capture important characteristics of these techniques. The framework is intended to support process tailoring, by facilitating the selection of techniques for inclusion into process models that target specific product qualities. We use risk management as a theory to accommodate techniques for many product qualities and lifecycle phases. Safety techniques have motivated the framework, and safety and performance techniques have been used to evaluate the framework. The evaluation demonstrates the ability of quality risk management to cover the development lifecycle and to accommodate two different product qualities. We identify advantages and limitations of the framework, and discuss future research on the framework.},
 duplicado = {false},
 inserir = {false},
 title = {Quality Attribute Techniques Framework},
 year = {2009}
}

@article{1227,
 abstract = {Various techniques are used to investigate, evaluate, and control product quality risks throughout software development process. These "Quality Attribute Techniques" are used during all stages of the software development life cycle to ensure that acceptable levels of product qualities such as safety and performance are in place. In this paper, we propose a method to select from among the alternatives of these techniques. This method is based on Risk Management theory and the Analytic Hierarchy Process (AHP) approach. We apply our method to an example of real-world safety system presented in the literature. We identify advantages and limitations of the method, and discuss future research.},
 duplicado = {false},
 inserir = {false},
 title = {Systematic selection of quality attribute techniques},
 year = {2010}
}

@article{1228,
 abstract = {Abstract
Testing of dependable event-based systems is very
important to ensure that all requirements (including nonfunctional
requirements such as reliability, availability,
safety and security) are met, and the relevant standards are
observed. In this paper we provide an overview of the state
of the practice in testing dependable event-based systems
and identify the challenges that have to be addressed in the
future. We illustrate our findings by a case study for a
transportation system. The most important topics for
research and improvement are: (1) formal modeling
techniques for domain experts, (2) smart monkey testing
techniques for reliability testing (3) mutation analysis to
reveal the defect-detection potential of test suites, and a (4)
recommending tool for the selection of test-case design
methods. },
 duplicado = {false},
 inserir = {false},
 title = {Model-based Testing and Verification of Dependable Systems},
 year = {2009}
}

@article{1230,
 abstract = {Abstract. Accomplishing software verification and validation (V&V) activities
is not a simple task. It involves a great number of techniques to choose and
there is no sufficient organized information to support the selection regarding
the V&V technique to be used. This paper describes an ongoing research work
concerned with the definition of an approach to plan verification and validation
processes. Its objective is to define how V&V activities can be supported
throughout software development processes accomplished by a Software
Engineering Environment (SEE). Besides V&V knowledge integrated into the
SEE, this approach will also organize some practical recommendations
generated by experimental studies regarding V&V techniques to support their
use.},
 duplicado = {false},
 inserir = {true},
 title = {Integrating Verification and Validation Techniques Knowledge into Software Engineering Environments},
 year = {2004}
}

@article{1231,
 abstract = {Abstract
The aim is to compare different testing techniques and what efforts it takes to find genuine
failures on particular level of test. These levels are e.g. unit/component level, i.e.
code/designer level testing, as well as on other test levels, such as integrated and system test
level. Test cases are often complementary to each other, and explore different aspects of the
program being tested. Writing efficient test cases is a necessity, since testing all aspects of a
complex system is not feasible. For large complex systems, the share amount of code, and the
probability of typical failures at different level implies that there are typical test case
approaches that should be more efficient than others. Yet, there is not much research
comparing testing techniques other than comparing two particular techniques, and often the
evaluation is done on very small code samples. The purpose of this research is first to
understand, classify and explore test techniques and their efficiency to find failures, which
includes doing a thorough literature study among what has been evaluated and how at earlier
stages. Secondly, the analysis of this will result in an exploration of the existing
classifications, and its benefits and downfalls. Thirdly the work will focus on trying to select
some testing technique, and understand and explore how to actually evaluate the testing
technique in a proper way by doing experiments, and finally to expand this experiment to
more testing techniques and a larger scale of code, which will include experiments on
efficiency. This efficiency exploration will lead the research into areas to explore where
automated techniques is interesting result of study.},
 duplicado = {false},
 inserir = {false},
 title = {Software Testing Techniques},
 year = {2003}
}

@article{1232,
 abstract = {ABSTRACT
We have a great number of software testing
techniques at our disposal for testing a software product.
Although the utilization of these techniques is growing,
we do have a very inadequate knowledge about their
relative quantitative and qualitative statistics. The choice
of a software testing technique in software testing
influences both process and product quality. So it is
imperative for us to find a testing technique which is
effective as well as efficient. However it is not sufficient
if testing techniques are only compared on fault
detecting ability. They should also be evaluated to check
which among them enhances reliability most. To
establish a useful theory for testing, we need to evaluate
existing and novel testing techniques not only for
effectiveness and efficiency but also for their ability of
enhancing software reliability. },
 duplicado = {false},
 inserir = {false},
 title = {Improving Software Consistency by Evaluating Efficiency of Software Testing Techniques},
 year = {2012}
}

@article{1233,
 abstract = {ABSTRACT
Testing techniques refer to different methods or ways
of testing particular features of a computer program,
system or product. Presently there are so many
different software testing techniques that we can use.
Whether we decide to automate or just execute tests
manually, there is a selection of testing techniques to
choose from. We have to make sure that we select
technique(s) that will help to ensure the most efficient
and effective testing of the system. The fundamental
problem in software testing thus throws an open
question, as to what would be the techniques that we
should adopt for an efficient and effective testing.
Thus, the selection of right testing techniques at the
right time for right problem will make the software
testing efficient and effective. In this paper we discuss
how should testing techniques be compared with one
another and why do we face a problem in making
appropriate testing technique selection.},
 duplicado = {false},
 inserir = {false},
 title = {Problems in Making Appropriate Testing Technique Selection},
 year = {2012}
}

@article{1235,
 abstract = {Verification and validation are important parts of
the software development process. The software
verification and validation (V&V) activities ensure that
the artifacts conform to the specifications and that the
final product is what was initially requested from the
customers. However, the accomplishing of V&V
activities is not simple. For many software
development projects, half of the planed schedule is
spent on software verification and validation activities.
Additionally, in most cases the software developers
make use of technologies for which we have not
enough evidence to confirm their suitability, limits,
qualities, costs, and inherent risks.
There is a need to help developers accomplishing
V&V activities. There are two ways to support the
developer. One way is defining what developers have
to do, that is, the sequence of V&V activities to be
executed in the software development. Software
processes have this purpose describing V&V activities,
pre-conditions, sub activities, responsibilities and work
products. So, the first goal is to help in V&V process
definition. However, the definition of what have to be
done does not guarantee that the software developer
knows how to do. The second way is to support the
execution of V&V activities defined in the processes,
associating activities with knowledge necessary to
execute it.
When we consider the software V&V processes,
we should take into account international standards,
like ISO/IEC 12207, and maturity models, like CMMI
(www.sei.cmu.edu/cmmi). Despite the existence of this
standard and CMMI, the definition of the software
processes is an activity requiring experience and it
needs software engineering knowledge. Besides, ISO
12207 and CMMI describe V&V activities and
practices using different denominations and detail
levels, which make hard their combined use for the
software process definition.
When considering the support to the execution of
V&V activities, knowledge and experience can be very
useful. But instead of being based in mere intuition,
software development decision should be based on
factual knowledge. Experimentation is an important
way to produce this knowledge. A body of
experimental knowledge can indicate which are the
available technologies to choose, their strengths and
limitations, and under which conditions such
technologies work best.
However there are some difficulties to support
V&V activities with experimental knowledge.
Regarding V&V knowledge, there are several works,
including experimental studies, exploring the state of
practice of the V&V processes and techniques in
software development organizations [1]. All these
works search a better understanding of V&V activities.
However, they do not define direct ways to support
these activities. So, there is no enough grouped
information about V&V methods and techniques that
could allow software developers to properly decide
which techniques they should use in a given context.
Some examples concerned with experimental
software engineering knowledge can be represented by
CeBASE (http://www.cebase.org), and ESERNET
(http://www.esernet.org) repository, both containing
experimental results about the software engineering
technologies effectiveness in organizational contexts.
Despite the fact that these knowledge bases have
information applicable to V&V activities, such
knowledge is not described in a direct way to provide
practical guidelines of its use and also, the knowledge
is not integrated into the software development
process, making its use workload higher.
Considering these scenarios, this work proposes an
approach that intends to directly integrate V&V
knowledge into a software engineering environment
(SEE). Software processes definition and planning,
and, the choice of the suitable V&V techniques to a
specific software product should be supported by the
knowledge management (KM) of a body of practical
recommendations built through systematic reviews [2]
of experimental studies},
 duplicado = {false},
 inserir = {true},
 title = {Supporting Software Verification and Validation with Knowledg},
 year = {2004}
}

@article{1236,
 abstract = {Abstract:
One major problem for integrating study results into a common body of knowledge is the heterogeneity of reporting styles: (1) it is difficult to locate relevant information and (2) important information is often missing. Reporting guidelines are expected to support a systematic, standardized presentation of empirical research, thus improving reporting in order to support readers in (1) finding the information they are looking for, (2) understanding how an experiment is conducted, and (3) assessing the validity of its results. The objective of this paper is to survey the most prominent published proposals for reporting guidelines, and to derive a unified standard that which can serve as a starting point for further discussion. We provide detailed guidance on the expected content of the sections and subsections for reporting a specific type of empirical studies, i.e., controlled experiments. Before the guidelines can be evaluated, feedback from the research community is required. For this purpose, we propose to adapt guideline development processes from other disciplines.},
 duplicado = {false},
 inserir = {false},
 title = {Reporting guidelines for controlled experiments in software engineering},
 year = {2005}
}

@article{1237,
 abstract = {Background: One major problem for integrating study results into a common body of knowledge is the heterogeneity of reporting styles: (1) It is difficult to locate relevant information and (2) important information is often missing.

Objective: A guideline for reporting results from controlled experiments is expected to support a systematic, standardized presentation of empirical research, thus improving reporting in order to support readers in (1) finding the information they are looking for, (2) understanding how an experiment is conducted, and (3) assessing the validity of its results.

Method: The guideline for reporting is based on (1) a survey of the most prominent published proposals for reporting guidelines in software engineering and (2) an iterative development incorporating feedback from members of the research community.

Result: This chapter presents the unification of a set of guidelines for reporting experiments in software engineering.

Limitation: The guideline has not been evaluated broadly yet.

Conclusion: The resulting guideline provides detailed guidance on the expected content of the sections and subsections for reporting a specific type of empirical study, i.e., experiments (controlled experiments and quasi-experiments).},
 duplicado = {false},
 inserir = {false},
 title = {Reporting Experiments in Software Engineering},
 year = {2008}
}

@article{1238,
 abstract = {Abstract
One of the major problems within the software testing area is how to get a suitable set of cases to test a software system. This set should assure maximum effectiveness with the least possible number of test cases. There are now numerous testing techniques available for generating test cases. However, many are never used, and just a few are used over and over again. Testers have little (if any) information about the available techniques, their usefulness and, generally, how suited they are to the project at hand upon, which to base their decision on which testing techniques to use. This paper presents the results of developing and evaluating an artefact (specifically, a characterisation schema) to assist with testing technique selection. When instantiated for a variety of techniques, the schema provides developers with a catalogue containing enough information for them to select the best suited techniques for a given project. This assures that the decisions they make are based on objective knowledge of the techniques rather than perceptions, suppositions and assumptions.},
 duplicado = {false},
 inserir = {false},
 title = {A Characterisation Schema for Software Testing Techniques},
 year = {2005}
}

@article{1242,
 abstract = {Understanding the effects of software engineering techniques and processes under varying conditions can be seen as a major prerequisite towards predictable project planning and guaranteeing software quality. Evidence regarding the effects of techniques and processes for specific contexts can be gained by empirical studies. Due to the fact that software development is a human-based and context-oriented activity the effects vary from project environment to project environment. As a consequence, the studies need to be performed in specific environments and the results are typically only valid for these local environments. Potential users of the evidence gained in such studies (e.g., project planners who need to select techniques and processes for a project) are confronted with difficulties such as finding and understanding the relevant results and assessing whether and how they can be applied to their own situation. Thereby, effective transfer and use of empirical findings is hindered. Our thesis is that effective dissemination and exploitation of empirical evidence into industry requires aggregation, integration, and adequate stakeholder-oriented presentation of the results. This position paper sketches major problems and challenges and proposes research issues towards solving the problem.},
 duplicado = {false},
 inserir = {false},
 title = {Accumulation and presentation of empirical evidence: problems and challenges},
 year = {2005}
}

@article{1245,
 abstract = {In this thesis we explore how knowledge management is performed in open source projects. Open source projects are often perceived as informal, even unmanaged. Still, they appear to manage knowledge acquisition and sharing sufficiently well to successfully develop software in such a distributed environment as the Internet. This thesis aims to explore that apparent contradiction, and thus complement the currently limited research in this field. The thesis consists of a literature study of knowledge management theory and open source development, resulting in the analysis of open source practices from a knowledge management perspective. Currently the field of knowledge management maintains several, partially opposing doctrines. Apart from the business aspect, two main schools of thought are present. The commodity school approaches knowledge as a universal truth, an object that can be separated from the knower. The community school emphasises knowledge as something internal to the human mind, but which can be shared as experiences between people. In the analysis presented, we have applied an analysis method which considers both the commodity and the community perspectives. The analysis is based on previous research studies of open source, and open source practices, and is furthered by a cursory case study using examples from a selected set of open source projects. Our conclusions are that knowledge management indeed is present in open source projects, and that it is supported by an ecology like interaction of project practices.},
 duplicado = {false},
 inserir = {false},
 title = {Identifying and Analyzing Knowledge Management Aspects of Practices in Open Source Software Development},
 year = {2004}
}

@article{1247,
 abstract = {Abstract
Software testing has often to be done under severe pressure due to limited resources and a challenging time schedule facing the demand to assure the fulfillment of the software requirements. In addition, testing should unveil those software defects that harm the mission-critical functions of the software. Risk-based testing uses risk (re-)assessments to steer all phases of the test process to optimize testing efforts and limit risks of the software-based system. Due to its importance and high practical relevance, several risk-based testing approaches were proposed in academia and industry. This paper presents a taxonomy of risk-based testing providing a framework to understand, categorize, assess, and compare risk-based testing approaches to support their selection and tailoring for specific purposes. The taxonomy is aligned with the consideration of risks in all phases of the test process and consists of the top-level classes risk drivers, risk assessment, and risk-based test process. The taxonomy of risk-based testing has been developed by analyzing the work presented in available publications on risk-based testing. Afterwards, it has been applied to the work on risk-based testing presented in this special section of the International Journal on Software Tools for Technology Transfer.},
 duplicado = {false},
 inserir = {false},
 title = {A taxonomy of risk-based testing},
 year = {2014}
}

@article{1248,
 abstract = {Abstract
In many development projects, testing has to be conducted under severe pressure due to limited resources and a challenging time schedule. Risk-based testing, which utilizes identified risks of the system for testing purposes, has a high potential to improve testing as it helps to optimize the allocation of resources and provides decision support for management. But for many organizations, the integration of a risk-based approach into established testing activities is a challenging task, and there are several options to do so. In this article, we analyze how risk is defined, assessed, and applied to support and improve testing activities in projects, products, and processes. We investigate these questions empirically by a multiple case study of currently applied risk-based testing activities in industry. The case study is based on three cases from different backgrounds, i.e., a test project in context of the extension of a large Web-based information system, product testing of a measurement and diagnostic equipment for the electrical power industry, as well as a test process of a system integrator of telecommunication solutions. By analyzing and comparing these different industrial cases, we draw conclusions on the state of risk-based testing and discuss possible improvements.},
 duplicado = {false},
 inserir = {false},
 title = {A multiple case study on risk-based testing in industry},
 year = {2014}
}

@article{1249,
 abstract = {Abstract:
Context: The use of defect taxonomies and their assignment to requirements can improve system-level testing of requirements. Experiences from industrial applications indicate that the type of the underlying top-level defect categories constitutes the main influence factor for defining defect taxonomies. Objective: The objective addressed in this paper is to investigate the influence of the type of top-level defect categories on the quality of the created defect taxonomy, on the quality of the assignment of requirements to defect categories as well as the quality of designed test cases. Method: We conducted a controlled student experiment to determine the influence of two different types of top-level defect categories, i.e., Generic and web application specific, on the quality of the created defect taxonomy, their assignment to requirements and the derived test cases. Results: The results indicate an influence of the type of the top-level defect categories on the quality of the derived defect taxonomy but no influence on the quality of the assignment of requirements to defect categories and the quality of the designed test cases. Contribution: The main contributions of this paper is the empirical investigation of the role of the type of top-level defect taxonomies themselves for testing requirements and the consequences of the results for industrial application.},
 duplicado = {false},
 inserir = {false},
 title = {On the Role of Defect Taxonomy Types for Testing Requirements: Results of a Controlled Experiment},
 year = {2014}
}

@article{1250,
 abstract = {Abstract
Context

It is a difficult and challenging task to fully automatize model-based testing because this demands complete and unambiguous system models as input. Therefore, in practice, test cases, especially on the system level, are still derived manually from behavioral models like UML activity diagrams or state machines. But this kind of manual test case derivation is error-prone and knowing these errors makes it possible to provide guidelines to reduce them.

Objective

The objective of the study presented in this paper therefore is to examine which errors are possible and actually made when manually deriving test cases from UML activity diagrams or state machines and whether there are differences between these diagram types.

Method

We investigate the errors made when deriving test cases manually in a controlled student experiment. The experiment was performed and internally replicated with overall 84 participants divided into three groups at two institutions.

Results

As a result of our experiment, we provide a taxonomy of errors made and their frequencies. In addition, our experiment provides evidence that activity diagrams have a higher perceived comprehensibility but also a higher error-proneness than state machines with regard to manual test case derivation. This information helps to develop guidelines for manual test case derivation from UML activity diagrams and state machines.

Conclusion

Most errors observed were due to missing test steps, conditions or results, or content was written into the wrong field. As activity diagrams have a higher perceived comprehensibility, but also more error-prone than state machines, both diagram types are useful for manual test case derivation. Their application depends on the context and should be complemented with clear rules on how to derive test cases.},
 duplicado = {false},
 inserir = {false},
 title = {Manual test case derivation from UML activity diagrams and state machines: A controlled experiment},
 year = {2015}
}

@article{1251,
 abstract = {Abstract:
Quality of requirements is of great importance for the software development lifecycle as it influences all steps of software development. To ensure various quality attributes, suitable requirements validation techniques such as reviews or testing are essential. In this paper, we show how defect taxonomies can improve requirements reviews and testing. We point out how defect taxonomies can be seamlessly integrated into the requirements engineering process and discuss requirements validation with defect taxonomies as well as its benefits and the lessons learned with reference to industrial projects of a public health insurance institution where this approach has been successfully applied.},
 duplicado = {false},
 inserir = {false},
 title = {Using defect taxonomies for requirements validation in industrial projects},
 year = {2013}
}

@article{1252,
 abstract = {Abstract:
Systematic defect management based on bug-tracking systems such as Bugzilla is well established and has been successfully used in many software organizations. Defect management weights the failures observed during test execution according to their severity and forms the basis for effective defect taxonomies. In practice, most defect taxonomies are used only for the a posteriori allocation of testing resources to prioritize failures for debugging. Thus, these taxonomies' full potential to control and improve all the steps of testing has remained unexploited. This is especially the case for testing a system's user requirements. System-level defect taxonomies can improve the design of requirements-based tests, the tracing of defects to requirements, the quality assessment of requirements, and the control of the relevant defect management. So, we developed requirements-based testing with defect taxonomies (RTDT). This approach is aligned with the standard test process and uses defect taxonomies to support all phases of testing requirements. To illustrate this approach and its benefits, we use an example project (which we call Project A) from a public health insurance institution.},
 duplicado = {false},
 inserir = {false},
 title = {Using Defect Taxonomies for Testing Requirements},
 year = {2014}
}

@article{1253,
 abstract = {Bug classification is a well-established practice which supports important activities such as enhancing verification and validation (V&V) efficiency and effectiveness. The state of the practice is manual and hence classification errors occur. This paper investigates the sensitivity of the value of bug classification (specifically, failure type classification) to its error rate; i.e., the degree to which misclassified historic bugs decrease the V&V effectiveness (i.e., the ability to find bugs of a failure type of interest). Results from the analysis of an industrial database of more than 3,000 bugs show that the impact of classification error rate on V&V effectiveness significantly varies with failure type. Specifically, there are failure types for which a 5% classification error can decrease the ability to find them by 66%. Conversely, there are failure types for which the V&V effectiveness is robust to very high error rates. These results show the utility of future research aimed at: 1) providing better tool support for decreasing human errors in classifying the failure type of bugs, 2) providing more robust approaches for the selection of V&V techniques, and 3) including robustness as an important criterion when evaluating technologies.},
 duplicado = {false},
 inserir = {false},
 title = {On failure classification: the impact of getting it wrong},
 year = {2014}
}

@article{1254,
 abstract = {Abstract
In this paper we present the Social Weaver platform that enables end users to weave snippets of social software features into the workflows of existing enterprise applications. We discuss the underlying vision from a technological viewpoint, i.e., an end-user development viewpoint, and an organizational viewpoint which is about a certain ubiquitous understanding of enterprise application integration. We present the system's requirements, architecture and realization. The concrete platform is based on the standard web technology stack, which makes sense because the web is the current natural host for enterprise applications, at least for new ones. However, the approach presented in this article is technological-independent with the concrete platform as a concrete instance proving the approach as doable. Conceptually, the realized platform is a key to analyze the current situation and possible future of today's enterprise application landscapes which oscillate between emerging social software metaphors and an ever increasing degree of process automation found in today's organizations.},
 duplicado = {false},
 inserir = {false},
 title = {Weaving Social Software Features into Enterprise Resource Planning Systems},
 year = {2014}
}

@article{1255,
 abstract = {Abstract:
The level of joint industry-academia collaborations in SE is, unfortunately, very low, compared to the amount of activities in each of the two communities. In their ongoing efforts to find the root-causes of this issue, the authors compare the focus areas of industry and academia in software testing, as represented by the titles of talks (papers) from a set of selected conferences in each of the two communities. Our comparisons show that the two groups are talking about quite different things. For example, when practitioners talk about test automation, they mostly refer to automating the test execution phase. However, academic works� on automated approaches are mostly focused on test-case generation and test oracles. We also suggest what we can do to improve things, for example, researchers are advised to use the principles of Action Research in their research, especially when collaborating with industry, to ensure that the research problems are chosen from the actual needs of the industry, and that the research outputs will actually be used and benefit their industrial collaborators.},
 duplicado = {false},
 inserir = {false},
 title = {Living in two different worlds: A comparison of industry and academic focus areas in software testing},
 year = {2017}
}

@article{1257,
 abstract = {Abstract. Modeling Computer-interpretable Clinical Guidelines (CIGs) entails
the need to involve a heterogeneous group of professionals in the tasks required,
mainly knowledge engineers and physicians, but possibly others (e.g. nurses, patient
associations). The workflow process followed by the team is diverse, depending
on who raise an issue, demand a requirement, introduce expert criteria
deviating from the guideline, solve a bug, detect a duplicate, etc. Follow-up of
this process is one of the challenges, as responsibilities are fuzzy and traditional
communication via skype meetings or e-mail sometimes is not efficient enough.
In this position paper we describe a preliminary label taxonomy to support classification
of modeling issues and to facilitate collaborative formalization of CIGs.
The taxonomy was designed during a project where a PROForma CIG model for
hyponatremia diagnosis and classification was formalized. Beyond tracking and
organizing issues, the use of this taxonomy might facilitate analysis of modeling
events and time efforts.},
 duplicado = {false},
 inserir = {false},
 title = {A Label Taxonomy to Support Collaborative Formalization of Computer-interpretable Guidelines and Classification of Modeling Issues},
 year = {2017}
}

@article{1258,
 abstract = {Abstract:
Although white-box regression test prioritization has been well-studied, the more recently introduced black-box prioritization approaches have neither been compared against each other nor against more well-established white-box techniques. We present a comprehensive experimental comparison of several test prioritization techniques, including well-established white-box strategies and more recently introduced black-box approaches. We found that Combinatorial Interaction Testing and diversity-based techniques (Input Model Diversity and Input Test Set Diameter) perform best among the black-box approaches. Perhaps surprisingly, we found little difference between black-box and white-box performance (at most 4% fault detection rate difference). We also found the overlap between black- and white-box faults to be high: the first 10% of the prioritized test suites already agree on at least 60% of the faults found. These are positive findings for practicing regression testers who may not have source code available, thereby making white-box techniques inapplicable. We also found evidence that both black-box and white-box prioritization remain robust over multiple system releases.},
 duplicado = {false},
 inserir = {false},
 title = {Comparing White-Box and Black-Box Test Prioritization},
 year = {2016}
}

@article{1259,
 abstract = {Abstract:
During software maintenance, testing is a crucial activity to ensure the quality of code as it evolves over time. With the increasing size and complexity of software, adequate software testing has become increasingly important. Code coverage is an important metric to gauge the effectiveness of test cases and the adequacy of testing. However, what is the coverage level exhibited by large-scale open-source projects? What is the correlation between software metrics and the code coverage of the software? In this study, we investigate the state-of-the-practice of testing by measuring code coverage in open-source software projects. We examine over 300 large open-source projects written in Java, to measure the code coverage of their associated test cases. We analyse correlations between code coverage and relevant software metrics such as lines of code, cyclomatic complexity, and number of developers. Our results show that the coverage level decreases with the increase in size and complexity of the software, whereas the number of developers has an insignificant correlation with the code coverage. However, considering individual files, coverage increases with the size and complexity, whereas the number of developers has no correlation with the code coverage. Our results highlight the strengths and weaknesses of testing in open-source projects and make recommendations for future research.},
 duplicado = {false},
 inserir = {false},
 title = {An Empirical Study on the Adequacy of Testing in Open Source Projects},
 year = {2014}
}

@article{1260,
 abstract = {Abstract
The impact of software bugs on today's system failures is of primary concern. Many bugs are detected and removed during testing, while others do not show up easily at development time and manifest themselves only as operational failures. Besides the importance of understanding the bug features from the programmer perspective (i.e., what is wrong in the code), a key role in counteracting bugs is played by the chain that from the bug activation leads to failure.

This article investigates the characteristics of the bug manifestation process. Through an extensive empirical study, a set of failure-exposing conditions is first identified as bug manifestation characteristics; 666 bug reports from two applications are then analyzed with respect to these characteristics under several perspectives. Findings highlight: (i) the main occurrence patterns of bug triggering conditions in the selected case studies and the role played by the workload, the application and the environment where it runs; (ii) how such conditions evolve over time; (iii) how they relate to bug exposure and fixing difficulty; (iv) how they impact the user. Results provide a fine-grain characterization of bug manifestation that is expected to increase the perceived importance of this dimension in testing, debugging, and fault tolerance strategies.},
 duplicado = {false},
 inserir = {false},
 title = {How do bugs surface? A comprehensive study on the characteristics of software bugs manifestation},
 year = {2016}
}

@article{1261,
 abstract = {Abstract:
Testing software to assess or improve reliability presents several practical challenges. Conventional operational testing is a fundamental strategy that simulates the real usage of the system in order to expose failures with the highest occurrence probability. However, practitioners find it unsuitable for assessing/achieving very high reliability levels; also, they do not see the adoption of a real usage profile estimate as a sensible idea, being it a source of non-quantifiable uncertainty. Oppositely, debug testing aims to expose as many failures as possible, but regardless of their impact on runtime reliability. These strategies are used either to assess or to improve reliability, but cannot improve and assess reliability in the same testing session. This article proposes Reliability Assessment and Improvement (RELAI) testing, a new technique thought to improve the delivered reliability by an adaptive testing scheme, while providing, at the same time, a continuous assessment of reliability attained through testing and fault removal. The technique also quantifies the impact of a partial knowledge of the operational profile. RELAI is positively evaluated on four software applications compared, in separate experiments, with techniques conceived either for reliability improvement or for reliability assessment, demonstrating substantial improvements in both cases.},
 duplicado = {false},
 inserir = {false},
 title = {RELAI Testing: A Technique to Assess and Improve Software Reliability},
 year = {2015}
}

@article{1262,
 abstract = {Debugging is an indispensable yet frustrating activity in software development and maintenance. Thus, numerous techniques have been proposed to aid this task. Despite the demonstrated effectiveness and future potential of these techniques, many of them have the unrealistic single-fault failure assumption. To alleviate this problem, we propose a technique that can be used to distinguish failing tests that executed a single fault from those that executed multiple faults in this paper. The technique suitably combines information from (i) a set of fault localization ranked lists, each produced for a certain failing test and (ii) the distance between a failing test and the passing test that most resembles it to achieve this goal. An experiment on 5 real-life medium-sized programs with 18, 920 multiple-fault versions, which are shipped with number of faults ranging from 2 to 8, has been conducted to evaluate the technique. The results indicate that the performance of the technique in terms of evaluation measures precision, recall, and F-measure is promising. In addition, for the identified failing tests that executed a single fault, the technique can also properly cluster them.},
 duplicado = {false},
 inserir = {false},
 title = {Does the failing test execute a single or multiple faults?: an approach to classifying failing tests},
 year = {2015}
}

@article{1263,
 abstract = {Abstract:
Assessing reliability of software programs during validation is a challenging task for engineers. The assessment is not only required to be unbiased, but it needs to provide tight variance (hence, tight confidence interval) with as few test cases as possible. Statistical sampling is a theoretically sound approach for reliability testing, but it is often impractical in its current form, because of too many test cases required to achieve desired confidence levels, especially when the software has few residual faults inside. We claim that the potential of statistical sampling methods is largely underestimated. This paper presents an adaptive sampling-based testing (AST) strategy for reliability assessment. A two-stage conceptual framework is defined, where adaptiveness is included to uncover residual faults earlier, while various sampling-based techniques are proposed to improve the efficiency (in terms of variance-test cases tradeoff) by better exploiting the information available to tester. An empirical study is conducted to assess the AST performance and compare the proposed sampling techniques to each other on real programs.},
 duplicado = {false},
 inserir = {false},
 title = {On Adaptive Sampling-Based Testing for Software Reliability Assessment},
 year = {2016}
}

@article{1264,
 abstract = {Abstract
Poor software quality leads to lost profits and even loss of life. U.S. organizations lose
billions of dollars annually because of poor software quality. The purpose of this multiple
case study was to explore the strategies that quality assurance (QA) leaders in small
software development organizations used for successful software quality assurance
(SQA) processes. A case study provided the best research design to allow for the
exploration of organizational and managerial processes. The target population group was
the QA leaders of 3 small software development organizations who successfully
implemented SQA processes, located in Saint John, New Brunswick, Canada. The
conceptual framework that grounded this study was total quality management (TQM)
established by Deming in 1980. Face-to-face semistructured interviews with 2 QA
leaders from each organization and documentation including process and training
materials provided all the data for analysis. NVivo software aided a qualitative analysis
of all collected data using a process of disassembling the data into common codes,
reassembling the data into themes, interpreting the meaning, and concluding the data. The
resulting major themes were Agile practices, documentation, testing, and lost profits.
The results were in contrast to the main themes discovered in the literature review,
although there was some overlap. The implications for positive social change include the
potential to provide QA leaders with the strategies to improve SQA processes, thereby
allowing for improved profits, contributing to the organizations� longevity in business,
and strengthening the local economy.},
 duplicado = {false},
 inserir = {false},
 title = {Exploring Organizations' Software Quality Assurance Strategies},
 year = {2016}
}

@article{1265,
 abstract = {We propose a light-weight client-server model of communication between existing implementations of different program analyses. The communication is on-line and anonymous which means that all analyses simultaneously analyse the same program and an analysis does not know what other analyses participate in the communication. The anonymity and model's strong emphasis on independence of analyses allow to preserve almost everything in existing implementations. An analysis only has to add an implementation of a proposed communication protocol, determine places in its code where information from others would help, and then check whether there is no communication scenario, which would corrupt its result. We demonstrate functionality and effectiveness of the proposed communication model in a detailed case study with three analyses: two abstract interpreters and the classic symbolic execution. Results of the evaluation on SV-COMP benchmarks show impressive improvements in computed invariants and increased counts of successfully analysed benchmarks.},
 duplicado = {false},
 inserir = {false},
 title = {Anonymous On-line Communication Between Program Analyses},
 year = {1504}
}

@article{1267,
 abstract = {Abstract:
Model-based testing is of high practical relevance and many model-based testing approaches have been developed during the last years. But the key question under which conditions model-based testing pays off and a related decision support procedure for its application has not been sufficiently addressed. In this paper we develop a generic decision support procedure whether to apply model-based testing in a project or not. The decision support procedure compares estimated costs and benefits of model-based testing throughout all phases of the test process and is derived on the basis of a case study performed at the European Space Agency.},
 duplicado = {false},
 inserir = {false},
 title = {Estimating the Cost and Benefit of Model-Based Testing: A Decision Support Procedure for the Application of Model-Based Testing in Industry},
 year = {2015}
}

@article{1268,
 abstract = {Abstract: Software engineering is an applied research area preferably conducted
jointly by academia and industry to enable transfer of knowledge in both directions
and at the end improvement software engineering in industry. In this paper we
present how the required mutual knowledge transfer via empirical evaluation was
performed to improve testing with defect taxonomies in industry.},
 duplicado = {false},
 inserir = {true},
 title = {Mutual knowledge transfer between industry and academia to improve testing with defect taxonomies},
 year = {2015}
}

@article{1269,
 abstract = {Abstract
In software system development, testing can take considerable time and resources, and there are numerous examples in the literature of how to improve the testing process. In particular, methods for selection and prioritization of test cases can play a critical role in efficient use of testing resources. This paper focuses on the problem of selection and ordering of integration-level test cases. Integration testing is performed to evaluate the correctness of several units in composition. Further, for reasons of both effectiveness and safety, many embedded systems are still tested manually. To this end, we propose a process, supported by an online decision support system, for ordering and selection of test cases based on the test result of previously executed test cases. To analyze the economic efficiency of such a system, a customized return on investment (ROI) metric tailored for system integration testing is introduced. Using data collected from the development process of a large-scale safety-critical embedded system, we perform Monte Carlo simulations to evaluate the expected ROI of three variants of the proposed new process. The results show that our proposed decision support system is beneficial in terms of ROI at system integration testing and thus qualifies as an important element in improving the integration testing process.},
 duplicado = {false},
 inserir = {false},
 title = {Cost-Benefit Analysis of Using Dependency Knowledge at Integration Testing},
 year = {2016}
}

@article{1270,
 abstract = {Abstract: In this paper we summarize requirements-based testing with defect
taxonomies which seamlessly integrates defect taxonomies into the standard test
process to improve the effectiveness and the efficiency of testing requirements.},
 duplicado = {false},
 inserir = {false},
 title = {Requirements-based testing with defect taxonomies},
 year = {2015}
}

@article{1271,
 abstract = {The quality of the software can be measured by its return on investment. Factors which may affect the return on investment (ROI) is the tangible factors (such as the cost) dan intangible factors (such as the impact of software to the users or stakeholder). The factor of the software itself are assessed through reviewing, testing, process audit, and performance of software. This paper discusses the consideration of return on investment (ROI) assessment criteria derived from the software and its users. These criteria indicate that the approach may support a rational consideration of all relevant criteria when evaluating software, and shows examples of actual return on investment models. Conducted an analysis of the assessment criteria that affect the return on investment if these criteria have a disproportionate effort that resulted in a return on investment of a software decreased.},
 duplicado = {false},
 inserir = {false},
 title = {RETURN ON INVESMENT (ROI) OF SOFTWARE PRODUCT: A SYSTEMATIC LITERATURE REVIEW},
 year = {2015}
}

@article{1272,
 abstract = {In this paper, a literature review and classification scheme for selected software engineering researches is presented. The
study shows that an increasing volume of software engineering researches have been conducted in diverse range of areas. The
articles are classified and results of these are presented based on classification scheme that consist of five main categories:
software development process, software management, software engineering techniques, software re-engineering, and
software applications. Analyses of the selected researches are carried out and gaps in the research are identified. A
comprehensive list of references is presented. This review is intended to provide impetus in research and help simulate
further interest.},
 duplicado = {false},
 inserir = {false},
 title = {A Literature Review and Classification of Selected Software Engineering Researches},
 year = {2012}
}

