Analyzing web applications: An empirical study
Abstract
Due to the increased usage of JavaScript in web applications and the speed at which web technologies and browsers are evolving, web applications are be- coming ever more complex. Our hypothesis is that these applications contain severe errors, take unnecessary performance penalties, and violate accessibility standards. This study analyzes such errors and tries to quantify the need for a tool that can help developers make web applications with less errors. The research is conducted by first showing how much of the DOM is modified after the initial page load. This could indicate that static analysis does not suffice anymore. After that we quantify the amount of faults in the web application. The research is done on 3,422 sites randomly selected from the internet. They were automatically analyzed using a crawler. We conclude that the use of static analysis tools to prevent these faults does not suffice anymore. The errors and accessibility standard violations happen in dynamically generated DOM, which are not detectable by static analysis. The performance penalties are only visible through dynamic analysis. We propose to develop a random testing tool based on a crawler that checks for these errors. Our main contributions are the design of such a tool, the large dataset that we have gathered during this research and the quantification of both the level of dynamism of modern web applications and the fault-proneness of these applications due to this dynamism.